@article{antonelliMedicalSegmentationDecathlon2022,
  title = {The {{Medical Segmentation Decathlon}}},
  author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Golia and Rhode, Kawal and {Tobon-Gomez}, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbel{\'a}ez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Ildoo and {Maier-Hein}, Klaus and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2022},
  month = jul,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4128},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30695-9},
  urldate = {2023-09-28},
  abstract = {Abstract             International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD)\textemdash a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/7ARBRK4N/Antonelli et al_2022_The Medical Segmentation Decathlon.pdf}
}

@article{bastarrikaRelationshipCoronaryArtery2010,
  title = {Relationship {{Between Coronary Artery Disease}} and {{Epicardial Adipose Tissue Quantification}} at {{Cardiac CT}}},
  author = {Bastarrika, Gorka and Broncano, Jordi and Schoepf, U. Joseph and Schwarz, Florian and Lee, Yeong Shyan and Abro, Joseph A. and Costello, Philip and Zwerner, Peter L.},
  year = {2010},
  month = jun,
  journal = {Academic Radiology},
  volume = {17},
  number = {6},
  pages = {727--734},
  issn = {10766332},
  doi = {10.1016/j.acra.2010.01.015},
  urldate = {2023-09-27},
  langid = {english}
}

@article{bencevicRecentProgressEpicardial2022,
  title = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}: {{A Systematic Review}}},
  shorttitle = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}},
  author = {Ben{\v c}evi{\'c}, Marin and Gali{\'c}, Irena and Habijan, Marija and Pi{\v z}urica, Aleksandra},
  year = {2022},
  month = may,
  journal = {Applied Sciences},
  volume = {12},
  number = {10},
  pages = {5217},
  issn = {2076-3417},
  doi = {10.3390/app12105217},
  urldate = {2023-09-29},
  abstract = {Epicardial and pericardial adipose tissues (EAT and PAT), which are located around the heart, have been linked to coronary atherosclerosis, cardiomyopathy, coronary artery disease, and other cardiovascular diseases. Additionally, the volume and thickness of EAT are good predictors of CVD risk levels. Manual quantification of these tissues is a tedious and error-prone process. This paper presents a comprehensive and critical overview of research on the epicardial and pericardial adipose tissue segmentation and quantification methods, evaluates their effectiveness in terms of segmentation time and accuracy, provides a critical comparison of the methods, and presents ongoing and future challenges in the field. Described methods are classified into pericardial adipose tissue segmentation, direct epicardial adipose tissue segmentation, and epicardial adipose tissue segmentation via pericardium delineation. A comprehensive categorization of the underlying methods is conducted with insights into their evolution from traditional image processing methods to recent deep learning-based methods. The paper also provides an overview of the research on the clinical significance of epicardial and pericardial adipose tissues as well as the terminology and definitions used in the medical literature.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/IZLV7MLV/Benčević et al_2022_Recent Progress in Epicardial and Pericardial Adipose Tissue Segmentation and.pdf}
}

@article{calabreseUniversityCaliforniaSan2022,
  title = {The {{University}} of {{California San Francisco Preoperative Diffuse Glioma MRI}} ({{UCSF-PDGM}}) {{Dataset}}},
  author = {Calabrese, Evan and {Villanueva-Meyer}, Javier E. and Rudie, Jeffrey D. and Rauschecker, Andreas M. and Baid, Ujjwal and Bakas, Spyridon and Cha, Soonmee and Mongan, John T. and Hess, Christopher P.},
  year = {2022},
  month = nov,
  journal = {Radiology: Artificial Intelligence},
  volume = {4},
  number = {6},
  eprint = {2109.00356},
  primaryclass = {cs, eess},
  pages = {e220058},
  issn = {2638-6100},
  doi = {10.1148/ryai.220058},
  urldate = {2023-10-05},
  abstract = {Here we present the University of California San Francisco Preoperative Diffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500 subjects with histopathologically-proven diffuse gliomas who were imaged with a standardized 3 Tesla preoperative brain tumor MRI protocol featuring predominantly 3D imaging, as well as advanced diffusion and perfusion imaging techniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation status for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor methylation status for World Health Organization (WHO) grade III and IV gliomas. The UCSF-PDGM has been made publicly available in the hopes that researchers around the world will use these data to continue to push the boundaries of AI applications for diffuse gliomas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/DKRMCWRA/Calabrese et al_2022_The University of California San Francisco Preoperative Diffuse Glioma MRI.pdf;/Users/marinbenc/Zotero/storage/UADUEREK/2109.html}
}

@misc{choHowMuchData2016,
  title = {How Much Data Is Needed to Train a Medical Image Deep Learning System to Achieve Necessary High Accuracy?},
  author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06348},
  eprint = {1511.06348},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marinbenc/Zotero/storage/RWKU3KYG/Cho et al_2016_How much data is needed to train a medical image deep learning system to.pdf;/Users/marinbenc/Zotero/storage/VUF28Q6H/1511.html}
}

@misc{codellaSkinLesionAnalysis2019c,
  title = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018: {{A Challenge Hosted}} by the {{International Skin Imaging Collaboration}} ({{ISIC}})},
  shorttitle = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018},
  author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  year = {2019},
  month = mar,
  number = {arXiv:1902.03368},
  eprint = {1902.03368},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10\% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/4XNGJ29P/Codella et al_2019_Skin Lesion Analysis Toward Melanoma Detection 2018.pdf;/Users/marinbenc/Zotero/storage/DXME4599/1902.html}
}

@article{devunooruDeepLearningNeural2021,
  title = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis: A Recent Review and Taxonomy},
  shorttitle = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis},
  author = {Devunooru, Sindhu and Alsadoon, Abeer and Chandana, P. W. C. and Beg, Azam},
  year = {2021},
  month = jan,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {1},
  pages = {455--483},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-020-01998-w},
  urldate = {2023-09-27},
  langid = {english}
}

@article{edlundLIVECellLargescaleDataset2021,
  title = {{{LIVECell}}\textemdash{{A}} Large-Scale Dataset for Label-Free Live Cell Segmentation},
  author = {Edlund, Christoffer and Jackson, Timothy R. and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  year = {2021},
  month = sep,
  journal = {Nature Methods},
  volume = {18},
  number = {9},
  pages = {1038--1045},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01249-6},
  urldate = {2023-09-28},
  abstract = {Abstract             Light microscopy combined with well-established protocols of two-dimensional cell culture facilitates high-throughput quantitative imaging to study biological phenomena. Accurate segmentation of individual cells in images enables exploration of complex biological questions, but can require sophisticated imaging processing pipelines in cases of low contrast and high object density. Deep learning-based methods are considered state-of-the-art for image segmentation but typically require vast amounts of annotated data, for which there is no suitable resource available in the field of label-free cellular imaging. Here, we present LIVECell, a large, high-quality, manually annotated and expert-validated dataset of phase-contrast images, consisting of over 1.6 million cells from a diverse set of cell morphologies and culture densities. To further demonstrate its use, we train convolutional neural network-based models using LIVECell and evaluate model segmentation accuracy with a proposed a suite of benchmarks.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/TUQV6DIR/Edlund et al_2021_LIVECell—A large-scale dataset for label-free live cell segmentation.pdf}
}

@book{fosbinder2011essentials,
  title = {Essentials of Radiologic Science},
  author = {Fosbinder, R. and Orth, D.},
  year = {2011},
  publisher = {{Wolters Kluwer Health/Lippincott Williams \& Wilkins}},
  isbn = {978-0-7817-7554-0},
  lccn = {2010030972}
}

@misc{heMaskRCNN2018,
  title = {Mask {{R-CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2018},
  month = jan,
  number = {arXiv:1703.06870},
  eprint = {1703.06870},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-03},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/78ZMVALZ/He et al_2018_Mask R-CNN.pdf;/Users/marinbenc/Zotero/storage/RBRIFKSN/1703.html}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/ELVXRVGZ/Irvin et al_2019_CheXpert.pdf;/Users/marinbenc/Zotero/storage/PRSVMGSL/1901.html}
}

@article{jhaInstanceSegmentationWhole2021a,
  title = {Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment},
  shorttitle = {Instance Segmentation for Whole Slide Imaging},
  author = {Jha, Aadarsh and Yang, Haichun and Deng, Ruining and Kapp, Meghan E. and Fogo, Agnes B. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {Journal of Medical Imaging},
  volume = {8},
  number = {01},
  issn = {2329-4302},
  doi = {10.1117/1.JMI.8.1.014001},
  urldate = {2023-10-02},
  file = {/Users/marinbenc/Zotero/storage/33U6NMXI/Jha et al_2021_Instance segmentation for whole slide imaging.pdf}
}

@incollection{kamalianComputedTomographyImaging2016,
  title = {Computed Tomography Imaging and Angiography \textendash{} Principles},
  booktitle = {Handbook of {{Clinical Neurology}}},
  author = {Kamalian, Shervin and Lev, Michael H. and Gupta, Rajiv},
  year = {2016},
  volume = {135},
  pages = {3--20},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53485-9.00001-5},
  urldate = {2023-09-29},
  isbn = {978-0-444-53485-9},
  langid = {english}
}

@article{Kass1988,
  title = {Snakes: {{Active}} Contour Models},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf00133570}
}

@article{lotanMedicalImagingPrivacy2020,
  title = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}: {{Myth}}, {{Fallacy}}, and the {{Future}}},
  shorttitle = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}},
  author = {Lotan, Eyal and Tschider, Charlotte and Sodickson, Daniel K. and Caplan, Arthur L. and Bruno, Mary and Zhang, Ben and Lui, Yvonne W.},
  year = {2020},
  month = sep,
  journal = {Journal of the American College of Radiology},
  volume = {17},
  number = {9},
  pages = {1159--1162},
  issn = {15461440},
  doi = {10.1016/j.jacr.2020.04.007},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BFEQKKMA/Lotan et al_2020_Medical Imaging and Privacy in the Era of Artificial Intelligence.pdf}
}

@article{lutnickIntegratedIterativeAnnotation2019,
  title = {An Integrated Iterative Annotation Technique for Easing Neural Network Training in Medical Image Analysis},
  author = {Lutnick, Brendon and Ginley, Brandon and Govind, Darshana and McGarry, Sean D. and LaViolette, Peter S. and Yacoub, Rabi and Jain, Sanjay and Tomaszewski, John E. and Jen, Kuang-Yu and Sarder, Pinaki},
  year = {2019},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {2},
  pages = {112--119},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0018-3},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RU4JUGUR/Lutnick et al_2019_An integrated iterative annotation technique for easing neural network training.pdf}
}

@article{mahabadiCardiacComputedTomographyderived2017,
  title = {Cardiac Computed Tomography-Derived Epicardial Fat Volume and Attenuation Independently Distinguish Patients with and without Myocardial Infarction},
  author = {Mahabadi, Amir Abbas and Balcer, Bastian and Dykun, Iryna and Forsting, Michael and Schlosser, Thomas and Heusch, Gerd and Rassaf, Tienush},
  editor = {Merx, Marc W.},
  year = {2017},
  month = aug,
  journal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0183514},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183514},
  urldate = {2023-09-29},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XEEL69CF/Mahabadi et al_2017_Cardiac computed tomography-derived epicardial fat volume and attenuation.pdf}
}

@misc{matterport_maskrcnn_2017,
  title = {Mask {{R-CNN}} for Object Detection and Instance Segmentation on {{Keras}} and {{TensorFlow}}},
  author = {Abdulla, Waleed},
  year = {2017},
  publisher = {{Github}}
}

@misc{nguyen2020vindrcxr,
  title = {{{VinDr-CXR}}: {{An}} Open Dataset of Chest {{X-rays}} with Radiologist's Annotations},
  author = {Nguyen, Ha Q. and Lam, Khanh and Le, Linh T. and Pham, Hieu H. and Tran, Dat Q. and Nguyen, Dung B. and Le, Dung D. and Pham, Chi M. and Tong, Hang T. T. and Dinh, Diep H. and Do, Cuong D. and Doan, Luu T. and Nguyen, Cuong N. and Nguyen, Binh T. and Nguyen, Que V. and Hoang, Au D. and Phan, Hien N. and Nguyen, Anh T. and Ho, Phuong H. and Ngo, Dat T. and Nguyen, Nghia T. and Nguyen, Nhan T. and Dao, Minh and Vu, Van},
  year = {2020},
  eprint = {2012.15029},
  primaryclass = {eess.IV},
  archiveprefix = {arxiv}
}

@article{radlAVTMulticenterAortic2022a,
  title = {{{AVT}}: {{Multicenter}} Aortic Vessel Tree {{CTA}} Dataset Collection with Ground Truth Segmentation Masks},
  shorttitle = {{{AVT}}},
  author = {Radl, Lukas and Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Egger, Jan},
  year = {2022},
  month = feb,
  journal = {Data in Brief},
  volume = {40},
  pages = {107801},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.107801},
  urldate = {2023-10-05},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/KI8HGT9Y/Radl et al_2022_AVT.pdf}
}

@article{regionGrowing,
  title = {Seeded Region Growing},
  author = {Adams, R. and Bischof, L.},
  year = {1994},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {6},
  pages = {641--647},
  doi = {10.1109/34.295913}
}

@incollection{Rohlfing2005,
  title = {Quo Vadis, Atlas-Based Segmentation?},
  booktitle = {Handbook of Biomedical Image Analysis},
  author = {Rohlfing, Torsten and Brandt, Robert and Menzel, Randolf and Russakoff, Daniel B. and Maurer, Calvin R.},
  year = {2005},
  pages = {435--486},
  publisher = {{Springer US}},
  doi = {10.1007/0-306-48608-3_11}
}

@article{rotembergPatientcentricDatasetImages2021,
  title = {A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author = {Rotemberg, Veronica and Kurtansky, Nicholas and {Betz-Stablein}, Brigid and Caffery, Liam and Chousakos, Emmanouil and Codella, Noel and Combalia, Marc and Dusza, Stephen and Guitera, Pascale and Gutman, David and Halpern, Allan and Helba, Brian and Kittler, Harald and Kose, Kivanc and Langer, Steve and Lioprys, Konstantinos and Malvehy, Josep and Musthaq, Shenara and Nanda, Jabpani and Reiter, Ofer and Shih, George and Stratigos, Alexander and Tschandl, Philipp and Weber, Jochen and Soyer, H. Peter},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {34},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00815-z},
  urldate = {2023-10-02},
  abstract = {Abstract             Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients (20.8\% with at least one melanoma, 79.2\% with zero melanomas) from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 (1.8\%) histopathologically confirmed melanomas compared with benign melanoma mimickers.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/P5YJS86H/Rotemberg et al_2021_A patient-centric dataset of images and metadata for identifying melanomas.pdf}
}

@article{selleAnalysisVasculatureLiver2002,
  title = {Analysis of Vasculature for Liver Surgical Planning},
  author = {Selle, D. and Preim, B. and Schenk, A. and Peitgen, H.-O.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {21},
  number = {11},
  pages = {1344--1357},
  issn = {0278-0062},
  doi = {10.1109/TMI.2002.801166},
  urldate = {2023-09-27},
  langid = {english}
}

@article{sinclairAtlasISTNJointSegmentation2022a,
  title = {Atlas-{{ISTN}}: {{Joint}} Segmentation, Registration and Atlas Construction with Image-and-Spatial Transformer Networks},
  shorttitle = {Atlas-{{ISTN}}},
  author = {Sinclair, Matthew and Schuh, Andreas and Hahn, Karl and Petersen, Kersten and Bai, Ying and Batten, James and Schaap, Michiel and Glocker, Ben},
  year = {2022},
  month = may,
  journal = {Medical Image Analysis},
  volume = {78},
  pages = {102383},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102383},
  urldate = {2023-10-03},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/Q3S597NR/Sinclair et al_2022_Atlas-ISTN.pdf}
}

@inproceedings{sobhaniniaFetalUltrasoundImage2019,
  title = {Fetal {{Ultrasound Image Segmentation}} for {{Measuring Biometric Parameters Using Multi-Task Deep Learning}}},
  booktitle = {2019 41st {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Sobhaninia, Zahra and Rafiei, Shima and Emami, Ali and Karimi, Nader and Najarian, Kayvan and Samavi, Shadrokh and Reza Soroushmehr, S. M.},
  year = {2019},
  month = jul,
  pages = {6545--6548},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856981},
  urldate = {2023-09-27},
  isbn = {978-1-5386-1311-5},
  file = {/Users/marinbenc/Zotero/storage/IAH26LKM/Sobhaninia et al_2019_Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters Using.pdf}
}
