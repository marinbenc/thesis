@article{antonelliMedicalSegmentationDecathlon2022,
  title = {The {{Medical Segmentation Decathlon}}},
  author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Golia and Rhode, Kawal and {Tobon-Gomez}, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbel{\'a}ez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Ildoo and {Maier-Hein}, Klaus and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2022},
  month = jul,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4128},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30695-9},
  urldate = {2023-09-28},
  abstract = {Abstract             International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD){\textemdash}a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/7ARBRK4N/Antonelli et al_2022_The Medical Segmentation Decathlon.pdf}
}

@inproceedings{attnAllYouNeed,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{azadBiDirectionalConvLSTMUNet2019,
  title = {Bi-{{Directional ConvLSTM U-Net}} with {{Densley Connected Convolutions}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Azad, Reza and {Asadi-Aghbolaghi}, Maryam and Fathy, Mahmood and Escalera, Sergio},
  year = {2019},
  month = oct,
  pages = {406--415},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCVW.2019.00052},
  urldate = {2021-08-23},
  isbn = {978-1-72815-023-9},
  file = {/Users/marinbenc/Zotero/storage/WF8FX5RR/Azad et al. - 2019 - Bi-Directional ConvLSTM U-Net with Densley Connect.pdf}
}

@article{bastarrikaRelationshipCoronaryArtery2010,
  title = {Relationship {{Between Coronary Artery Disease}} and {{Epicardial Adipose Tissue Quantification}} at {{Cardiac CT}}},
  author = {Bastarrika, Gorka and Broncano, Jordi and Schoepf, U. Joseph and Schwarz, Florian and Lee, Yeong Shyan and Abro, Joseph A. and Costello, Philip and Zwerner, Peter L.},
  year = {2010},
  month = jun,
  journal = {Academic Radiology},
  volume = {17},
  number = {6},
  pages = {727--734},
  issn = {10766332},
  doi = {10.1016/j.acra.2010.01.015},
  urldate = {2023-09-27},
  langid = {english}
}

@article{bencevicRecentProgressEpicardial2022,
  title = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}: {{A Systematic Review}}},
  shorttitle = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}},
  author = {Ben{\v c}evi{\'c}, Marin and Gali{\'c}, Irena and Habijan, Marija and Pi{\v z}urica, Aleksandra},
  year = {2022},
  month = may,
  journal = {Applied Sciences},
  volume = {12},
  number = {10},
  pages = {5217},
  issn = {2076-3417},
  doi = {10.3390/app12105217},
  urldate = {2023-09-29},
  abstract = {Epicardial and pericardial adipose tissues (EAT and PAT), which are located around the heart, have been linked to coronary atherosclerosis, cardiomyopathy, coronary artery disease, and other cardiovascular diseases. Additionally, the volume and thickness of EAT are good predictors of CVD risk levels. Manual quantification of these tissues is a tedious and error-prone process. This paper presents a comprehensive and critical overview of research on the epicardial and pericardial adipose tissue segmentation and quantification methods, evaluates their effectiveness in terms of segmentation time and accuracy, provides a critical comparison of the methods, and presents ongoing and future challenges in the field. Described methods are classified into pericardial adipose tissue segmentation, direct epicardial adipose tissue segmentation, and epicardial adipose tissue segmentation via pericardium delineation. A comprehensive categorization of the underlying methods is conducted with insights into their evolution from traditional image processing methods to recent deep learning-based methods. The paper also provides an overview of the research on the clinical significance of epicardial and pericardial adipose tissues as well as the terminology and definitions used in the medical literature.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/IZLV7MLV/Benčević et al_2022_Recent Progress in Epicardial and Pericardial Adipose Tissue Segmentation and.pdf}
}

@inproceedings{bermudez-chaconDomainadaptiveTwostreamUNet2018,
  title = {A Domain-Adaptive Two-Stream {{U-Net}} for Electron Microscopy Image Segmentation},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {{Bermudez-Chacon}, Roger and {Marquez-Neila}, Pablo and Salzmann, Mathieu and Fua, Pascal},
  year = {2018},
  month = apr,
  pages = {400--404},
  publisher = {{IEEE}},
  address = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363602},
  urldate = {2023-11-28},
  isbn = {978-1-5386-3636-7},
  file = {/Users/marinbenc/Zotero/storage/XQK4IGIP/Bermudez-Chacon et al_2018_A domain-adaptive two-stream U-Net for electron microscopy image segmentation.pdf}
}

@article{bernalWMDOVAMapsAccurate2015,
  title = {{{WM-DOVA}} Maps for Accurate Polyp Highlighting in Colonoscopy: {{Validation}} vs. Saliency Maps from Physicians},
  shorttitle = {{{WM-DOVA}} Maps for Accurate Polyp Highlighting in Colonoscopy},
  author = {Bernal, Jorge and S{\'a}nchez, F. Javier and {Fern{\'a}ndez-Esparrach}, Gloria and Gil, Debora and Rodr{\'i}guez, Cristina and Vilari{\~n}o, Fernando},
  year = {2015},
  month = jul,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {43},
  pages = {99--111},
  issn = {08956111},
  doi = {10.1016/j.compmedimag.2015.02.007},
  urldate = {2021-04-13},
  langid = {english}
}

@article{bilicLiverTumorSegmentation2019,
  title = {The {{Liver Tumor Segmentation Benchmark}} ({{LiTS}})},
  author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"u}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`a}, Jana and Lowengrub, John and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"u}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"o}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and {van Ginneken}, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and {Ben-Cohen}, Avi and Klang, Eyal and Amitai, Marianne M. and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and {Lev-Cohain}, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.04056 [cs]},
  eprint = {1901.04056},
  primaryclass = {cs},
  urldate = {2021-04-13},
  abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/K3CADVKU/Bilic et al. - 2019 - The Liver Tumor Segmentation Benchmark (LiTS).pdf;/Users/marinbenc/Zotero/storage/BWEWN65C/1901.html}
}

@article{calabreseUniversityCaliforniaSan2022,
  title = {The {{University}} of {{California San Francisco Preoperative Diffuse Glioma MRI}} ({{UCSF-PDGM}}) {{Dataset}}},
  author = {Calabrese, Evan and {Villanueva-Meyer}, Javier E. and Rudie, Jeffrey D. and Rauschecker, Andreas M. and Baid, Ujjwal and Bakas, Spyridon and Cha, Soonmee and Mongan, John T. and Hess, Christopher P.},
  year = {2022},
  month = nov,
  journal = {Radiology: Artificial Intelligence},
  volume = {4},
  number = {6},
  eprint = {2109.00356},
  primaryclass = {cs, eess},
  pages = {e220058},
  issn = {2638-6100},
  doi = {10.1148/ryai.220058},
  urldate = {2023-10-05},
  abstract = {Here we present the University of California San Francisco Preoperative Diffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500 subjects with histopathologically-proven diffuse gliomas who were imaged with a standardized 3 Tesla preoperative brain tumor MRI protocol featuring predominantly 3D imaging, as well as advanced diffusion and perfusion imaging techniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation status for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor methylation status for World Health Organization (WHO) grade III and IV gliomas. The UCSF-PDGM has been made publicly available in the hopes that researchers around the world will use these data to continue to push the boundaries of AI applications for diffuse gliomas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/DKRMCWRA/Calabrese et al_2022_The University of California San Francisco Preoperative Diffuse Glioma MRI.pdf;/Users/marinbenc/Zotero/storage/UADUEREK/2109.html}
}

@misc{carr2021shuffle,
  title = {Shuffle to {{Learn}}: {{Self-supervised}} Learning from Permutations via Differentiable Ranking},
  author = {Carr, Andrew N and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Zeghidour, Neil},
  year = {2021}
}

@inproceedings{chaoHarDNetLowMemory2019,
  title = {{{HarDNet}}: {{A Low Memory Traffic Network}}},
  shorttitle = {{{HarDNet}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Chao, Ping and Kao, Chao-Yang and Ruan, Yushan and Huang, Chien-Hsiang and Lin, Youn-Long},
  year = {2019},
  month = oct,
  pages = {3551--3560},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00365},
  urldate = {2021-08-23},
  isbn = {978-1-72814-803-8},
  file = {/Users/marinbenc/Zotero/storage/9EV3N48V/Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf}
}

@misc{chen2017rethinking,
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  eprint = {1706.05587},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@article{chen2022vitadapter,
  title = {Vision Transformer Adapter for Dense Predictions},
  author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.08534},
  eprint = {2205.08534},
  archiveprefix = {arxiv}
}

@incollection{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2018},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11211},
  pages = {833--851},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_49},
  urldate = {2021-08-23},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  file = {/Users/marinbenc/Zotero/storage/7WE8QGJL/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.05709 [cs, stat]},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  urldate = {2022-02-10},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/9LN634BB/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/Users/marinbenc/Zotero/storage/JSY5MWDX/2002.html}
}

@misc{choHowMuchData2016,
  title = {How Much Data Is Needed to Train a Medical Image Deep Learning System to Achieve Necessary High Accuracy?},
  author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06348},
  eprint = {1511.06348},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marinbenc/Zotero/storage/RWKU3KYG/Cho et al_2016_How much data is needed to train a medical image deep learning system to.pdf;/Users/marinbenc/Zotero/storage/VUF28Q6H/1511.html}
}

@inproceedings{codellaSkinLesionAnalysis2018,
  title = {Skin Lesion Analysis toward Melanoma Detection: {{A}} Challenge at the 2017 {{International}} Symposium on Biomedical Imaging ({{ISBI}}), Hosted by the International Skin Imaging Collaboration ({{ISIC}})},
  shorttitle = {Skin Lesion Analysis toward Melanoma Detection},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Codella, Noel C. F. and Gutman, David and Celebi, M. Emre and Helba, Brian and Marchetti, Michael A. and Dusza, Stephen W. and Kalloo, Aadi and Liopyris, Konstantinos and Mishra, Nabin and Kittler, Harald and Halpern, Allan},
  year = {2018},
  month = apr,
  pages = {168--172},
  publisher = {{IEEE}},
  address = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363547},
  urldate = {2021-08-23},
  isbn = {978-1-5386-3636-7},
  file = {/Users/marinbenc/Zotero/storage/YWYZW6YY/Codella et al. - 2018 - Skin lesion analysis toward melanoma detection A .pdf}
}

@misc{codellaSkinLesionAnalysis2019c,
  title = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018: {{A Challenge Hosted}} by the {{International Skin Imaging Collaboration}} ({{ISIC}})},
  shorttitle = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018},
  author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  year = {2019},
  month = mar,
  number = {arXiv:1902.03368},
  eprint = {1902.03368},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10\% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/4XNGJ29P/Codella et al_2019_Skin Lesion Analysis Toward Melanoma Detection 2018.pdf;/Users/marinbenc/Zotero/storage/DXME4599/1902.html}
}

@article{commandeurDeepLearningQuantification2018a,
  title = {Deep {{Learning}} for {{Quantification}} of {{Epicardial}} and {{Thoracic Adipose Tissue From Non-Contrast CT}}},
  author = {Commandeur, Frederic and Goeller, Markus and Betancur, Julian and Cadet, Sebastien and Doris, Mhairi and Chen, Xi and Berman, Daniel S. and Slomka, Piotr J. and Tamarappoo, Balaji K. and Dey, Damini},
  year = {2018},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {37},
  number = {8},
  pages = {1835--1846},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2018.2804799},
  urldate = {2021-08-24},
  file = {/Users/marinbenc/Zotero/storage/2IG73BTZ/Commandeur et al. - 2018 - Deep Learning for Quantification of Epicardial and.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009a,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {{IEEE}},
  address = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2023-12-01},
  isbn = {978-1-4244-3992-8}
}

@article{devunooruDeepLearningNeural2021,
  title = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis: A Recent Review and Taxonomy},
  shorttitle = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis},
  author = {Devunooru, Sindhu and Alsadoon, Abeer and Chandana, P. W. C. and Beg, Azam},
  year = {2021},
  month = jan,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {1},
  pages = {455--483},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-020-01998-w},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{dosovitskiy2021an,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  booktitle = {International Conference on Learning Representations},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021}
}

@misc{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  number = {arXiv:1603.07285},
  eprint = {1603.07285},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-19},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/H4LV3GKY/Dumoulin_Visin_2018_A guide to convolution arithmetic for deep learning.pdf;/Users/marinbenc/Zotero/storage/E2J26Z3H/1603.html}
}

@article{edlundLIVECellLargescaleDataset2021,
  title = {{{LIVECell}}{\textemdash}{{A}} Large-Scale Dataset for Label-Free Live Cell Segmentation},
  author = {Edlund, Christoffer and Jackson, Timothy R. and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  year = {2021},
  month = sep,
  journal = {Nature Methods},
  volume = {18},
  number = {9},
  pages = {1038--1045},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01249-6},
  urldate = {2023-09-28},
  abstract = {Abstract             Light microscopy combined with well-established protocols of two-dimensional cell culture facilitates high-throughput quantitative imaging to study biological phenomena. Accurate segmentation of individual cells in images enables exploration of complex biological questions, but can require sophisticated imaging processing pipelines in cases of low contrast and high object density. Deep learning-based methods are considered state-of-the-art for image segmentation but typically require vast amounts of annotated data, for which there is no suitable resource available in the field of label-free cellular imaging. Here, we present LIVECell, a large, high-quality, manually annotated and expert-validated dataset of phase-contrast images, consisting of over 1.6 million cells from a diverse set of cell morphologies and culture densities. To further demonstrate its use, we train convolutional neural network-based models using LIVECell and evaluate model segmentation accuracy with a proposed a suite of benchmarks.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/TUQV6DIR/Edlund et al_2021_LIVECell—A large-scale dataset for label-free live cell segmentation.pdf}
}

@article{estevesPolarTransformerNetworks2018a,
  title = {Polar {{Transformer Networks}}},
  author = {Esteves, Carlos and {Allen-Blanchette}, Christine and Zhou, Xiaowei and Daniilidis, Kostas},
  year = {2018},
  month = feb,
  journal = {arXiv:1709.01889 [cs]},
  eprint = {1709.01889},
  primaryclass = {cs},
  urldate = {2021-04-01},
  abstract = {Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/X7Q3L4HS/Esteves et al. - 2018 - Polar Transformer Networks.pdf;/Users/marinbenc/Zotero/storage/66HS8UMH/1709.html}
}

@incollection{fangSelectiveFeatureAggregation2019,
  title = {Selective {{Feature Aggregation Network}} with {{Area-Boundary Constraints}} for {{Polyp Segmentation}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} {\textendash} {{MICCAI}} 2019},
  author = {Fang, Yuqi and Chen, Cheng and Yuan, Yixuan and Tong, Kai-yu},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  volume = {11764},
  pages = {302--310},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32239-7_34},
  urldate = {2021-04-19},
  isbn = {978-3-030-32238-0 978-3-030-32239-7},
  langid = {english}
}

@incollection{fanPraNetParallelReverse2020,
  title = {{{PraNet}}: {{Parallel Reverse Attention Network}} for {{Polyp Segmentation}}},
  shorttitle = {{{PraNet}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} {\textendash} {{MICCAI}} 2020},
  author = {Fan, Deng-Ping and Ji, Ge-Peng and Zhou, Tao and Chen, Geng and Fu, Huazhu and Shen, Jianbing and Shao, Ling},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  volume = {12266},
  pages = {263--273},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-59725-2_26},
  urldate = {2021-08-23},
  isbn = {978-3-030-59724-5 978-3-030-59725-2},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BWSUWGXP/Fan et al. - 2020 - PraNet Parallel Reverse Attention Network for Pol.pdf}
}

@inproceedings{fasterRCNN,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {91--99},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\textsubscript{r}cnn.}
}

@book{fosbinder2011essentials,
  title = {Essentials of Radiologic Science},
  author = {Fosbinder, R. and Orth, D.},
  year = {2011},
  publisher = {{Wolters Kluwer Health/Lippincott Williams \& Wilkins}},
  isbn = {978-0-7817-7554-0},
  lccn = {2010030972}
}

@inproceedings{ganinDA2015,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  series = {{{ICML}}'15},
  pages = {1180--1189},
  publisher = {{JMLR.org}},
  address = {{Lille, France}},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back propagation.Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016},
  keywords = {Machine learning}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-01},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/D9ICM9PZ/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/Users/marinbenc/Zotero/storage/DLDPDG3R/1406.html}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  urldate = {2023-10-21},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  file = {/Users/marinbenc/Zotero/storage/MENBPI9N/Hastie et al_2009_The Elements of Statistical Learning.pdf}
}

@article{he2019moco,
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2019},
  journal = {arXiv preprint arXiv:1911.05722},
  eprint = {1911.05722},
  archiveprefix = {arxiv}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2021-08-23},
  isbn = {978-1-4673-8851-1},
  file = {/Users/marinbenc/Zotero/storage/4QM6QX46/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heMaskRCNN2017b,
  title = {Mask {{R-CNN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = {2017},
  month = oct,
  pages = {2980--2988},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.322},
  urldate = {2023-10-09},
  isbn = {978-1-5386-1032-9}
}

@article{ho2020denoising,
  title = {Denoising Diffusion Probabilistic Models},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  journal = {arXiv preprint arxiv:2006.11239},
  eprint = {2006.11239},
  archiveprefix = {arxiv}
}

@article{huangHarDNetMSEGSimpleEncoderDecoder2021,
  title = {{{HarDNet-MSEG}}: {{A Simple Encoder-Decoder Polyp Segmentation Neural Network}} That {{Achieves}} over 0.9 {{Mean Dice}} and 86 {{FPS}}},
  shorttitle = {{{HarDNet-MSEG}}},
  author = {Huang, Chien-Hsiang and Wu, Hung-Yu and Lin, Youn-Long},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.07172 [cs]},
  eprint = {2101.07172},
  primaryclass = {cs},
  urldate = {2021-04-19},
  abstract = {We propose a new convolution neural network called HarDNet-MSEG for polyp segmentation. It achieves SOTA in both accuracy and inference speed on five popular datasets. For Kvasir-SEG, HarDNet-MSEG delivers 0.904 mean Dice running at 86.7 FPS on a GeForce RTX 2080 Ti GPU. It consists of a backbone and a decoder. The backbone is a low memory traffic CNN called HarDNet68, which has been successfully applied to various CV tasks including image classification, object detection, multi-object tracking and semantic segmentation, etc. The decoder part is inspired by the Cascaded Partial Decoder, known for fast and accurate salient object detection. We have evaluated HarDNet-MSEG using those five popular datasets. The code and all experiment details are available at Github. https://github.com/james128333/HarDNet-MSEG},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/YRDJXWHI/Huang et al. - 2021 - HarDNet-MSEG A Simple Encoder-Decoder Polyp Segme.pdf;/Users/marinbenc/Zotero/storage/GUI4TD7L/2101.html}
}

@inproceedings{huSqueezeandExcitationNetworks2018,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00745},
  urldate = {2021-08-23},
  isbn = {978-1-5386-6420-9},
  file = {/Users/marinbenc/Zotero/storage/3NCD5N6N/Hu et al. - 2018 - Squeeze-and-Excitation Networks.pdf}
}

@article{ibtehazMultiResUNetRethinkingUNet2020,
  title = {{{MultiResUNet}} : {{Rethinking}} the {{U-Net}} Architecture for Multimodal Biomedical Image Segmentation},
  shorttitle = {{{MultiResUNet}}},
  author = {Ibtehaz, Nabil and Rahman, M. Sohel},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {74--87},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.08.025},
  urldate = {2021-08-23},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/SVH4JXCR/Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net architecture f.pdf}
}

@article{info11020125,
  title = {Albumentations: {{Fast}} and Flexible Image Augmentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  journal = {Information-an International Interdisciplinary Journal},
  volume = {11},
  number = {2},
  issn = {2078-2489},
  doi = {10.3390/info11020125},
  article-number = {125}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/ELVXRVGZ/Irvin et al_2019_CheXpert.pdf;/Users/marinbenc/Zotero/storage/PRSVMGSL/1901.html}
}

@article{isenseeNnUNetSelfconfiguringMethod2021,
  title = {{{nnU-Net}}: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation},
  shorttitle = {{{nnU-Net}}},
  author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and {Maier-Hein}, Klaus H.},
  year = {2021},
  month = feb,
  journal = {Nature Methods},
  volume = {18},
  number = {2},
  pages = {203--211},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-020-01008-z},
  urldate = {2023-10-06},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/H5L3AVAQ/Isensee et al_2021_nnU-Net.pdf}
}

@article{jaderbergSpatialTransformerNetworks2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  journal = {arXiv:1506.02025 [cs]},
  eprint = {1506.02025},
  primaryclass = {cs},
  urldate = {2021-04-26},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/T69B79T5/Jaderberg et al. - 2016 - Spatial Transformer Networks.pdf;/Users/marinbenc/Zotero/storage/63BFW68K/1506.html}
}

@inproceedings{jhaDoubleUNetDeepConvolutional2020,
  title = {{{DoubleU-Net}}: {{A Deep Convolutional Neural Network}} for {{Medical Image Segmentation}}},
  shorttitle = {{{DoubleU-Net}}},
  booktitle = {2020 {{IEEE}} 33rd {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Jha, Debesh and Riegler, Michael A. and Johansen, Dag and Halvorsen, Pal and Johansen, Havard D.},
  year = {2020},
  month = jul,
  pages = {558--564},
  publisher = {{IEEE}},
  address = {{Rochester, MN, USA}},
  doi = {10.1109/CBMS49503.2020.00111},
  urldate = {2021-08-23},
  isbn = {978-1-72819-429-5},
  file = {/Users/marinbenc/Zotero/storage/3JKSV2PH/Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf}
}

@article{jhaInstanceSegmentationWhole2021a,
  title = {Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment},
  shorttitle = {Instance Segmentation for Whole Slide Imaging},
  author = {Jha, Aadarsh and Yang, Haichun and Deng, Ruining and Kapp, Meghan E. and Fogo, Agnes B. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {Journal of Medical Imaging},
  volume = {8},
  number = {01},
  issn = {2329-4302},
  doi = {10.1117/1.JMI.8.1.014001},
  urldate = {2023-10-02},
  file = {/Users/marinbenc/Zotero/storage/33U6NMXI/Jha et al_2021_Instance segmentation for whole slide imaging.pdf}
}

@incollection{kamalianComputedTomographyImaging2016,
  title = {Computed Tomography Imaging and Angiography {\textendash} Principles},
  booktitle = {Handbook of {{Clinical Neurology}}},
  author = {Kamalian, Shervin and Lev, Michael H. and Gupta, Rajiv},
  year = {2016},
  volume = {135},
  pages = {3--20},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53485-9.00001-5},
  urldate = {2023-09-29},
  isbn = {978-0-444-53485-9},
  langid = {english}
}

@article{Kass1988,
  title = {Snakes: {{Active}} Contour Models},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf00133570}
}

@article{khaderDenoisingDiffusionProbabilistic2023,
  title = {Denoising Diffusion Probabilistic Models for {{3D}} Medical Image Generation},
  author = {Khader, Firas and {M{\"u}ller-Franzes}, Gustav and Tayebi Arasteh, Soroosh and Han, Tianyu and Haarburger, Christoph and {Schulze-Hagen}, Maximilian and Schad, Philipp and Engelhardt, Sandy and Bae{\ss}ler, Bettina and Foersch, Sebastian and Stegmaier, Johannes and Kuhl, Christiane and Nebelung, Sven and Kather, Jakob Nikolas and Truhn, Daniel},
  year = {2023},
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {7303},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-34341-2},
  urldate = {2023-12-01},
  abstract = {Abstract             Recent advances in computer vision have shown promising results in image generation. Diffusion probabilistic models have generated realistic images from textual input, as demonstrated by DALL-E 2, Imagen, and Stable Diffusion. However, their use in medicine, where imaging data typically comprises three-dimensional volumes, has not been systematically evaluated. Synthetic images may play a crucial role in privacy-preserving artificial intelligence and can also be used to augment small datasets. We show that diffusion probabilistic models can synthesize high-quality medical data for magnetic resonance imaging (MRI) and computed tomography (CT). For quantitative evaluation, two radiologists rated the quality of the synthesized images regarding "realistic image appearance", "anatomical correctness", and "consistency between slices". Furthermore, we demonstrate that synthetic images can be used in self-supervised pre-training and improve the performance of breast segmentation models when data is scarce (Dice scores, 0.91 [without synthetic data], 0.95 [with synthetic data]).},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/2LD9NW58/Khader et al_2023_Denoising diffusion probabilistic models for 3D medical image generation.pdf}
}

@article{khanAutoCellSegRobustAutomatic2018,
  title = {{{AutoCellSeg}}: Robust Automatic Colony Forming Unit ({{CFU}})/Cell Analysis Using Adaptive Image Segmentation and Easy-to-Use Post-Editing Techniques},
  shorttitle = {{{AutoCellSeg}}},
  author = {Khan, Arif Ul Maula and Torelli, Angelo and Wolf, Ivo and Gretz, Norbert},
  year = {2018},
  month = may,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {7302},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24916-9},
  urldate = {2023-10-16},
  abstract = {Abstract             In biological assays, automated cell/colony segmentation and counting is imperative owing to huge image sets. Problems occurring due to drifting image acquisition conditions, background noise and high variation in colony features in experiments demand a user-friendly, adaptive and robust image processing/analysis method. We present AutoCellSeg (based on MATLAB) that implements a supervised automatic and robust image segmentation method. AutoCellSeg utilizes multi-thresholding aided by a feedback-based watershed algorithm taking segmentation plausibility criteria into account. It is usable in different operation modes and intuitively enables the user to select object features interactively for supervised image segmentation method. It allows the user to correct results with a graphical interface. This publicly available tool outperforms tools like OpenCFU and CellProfiler in terms of accuracy and provides many additional useful features for end-users.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/3UFPAMEA/Khan et al_2018_AutoCellSeg.pdf}
}

@article{kimCNNBasedUGS2018,
  title = {{{CNN}}-based {{UGS}} Method Using {{Cartesian}}-to-polar Coordinate Transformation},
  author = {Kim, B.-S. and Sun, J.-Y. and Kim, S.-W. and Kang, M.-C. and Ko, S.-J.},
  year = {2018},
  month = nov,
  journal = {Electronics Letters},
  volume = {54},
  number = {23},
  pages = {1321--1322},
  issn = {0013-5194, 1350-911X},
  doi = {10.1049/el.2018.5051},
  urldate = {2021-04-01},
  langid = {english}
}

@article{kimCyCNNRotationInvariant2020a,
  title = {{{CyCNN}}: {{A Rotation Invariant CNN}} Using {{Polar Mapping}} and {{Cylindrical Convolution Layers}}},
  shorttitle = {{{CyCNN}}},
  author = {Kim, Jinpyo and Jung, Wooekun and Kim, Hyungmo and Lee, Jaejin},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.10588 [cs, eess]},
  eprint = {2007.10588},
  primaryclass = {cs, eess},
  urldate = {2021-04-08},
  abstract = {Deep Convolutional Neural Networks (CNNs) are empirically known to be invariant to moderate translation but not to rotation in image classification. This paper proposes a deep CNN model, called CyCNN, which exploits polar mapping of input images to convert rotation to translation. To deal with the cylindrical property of the polar coordinates, we replace convolution layers in conventional CNNs to cylindrical convolutional (CyConv) layers. A CyConv layer exploits the cylindrically sliding windows (CSW) mechanism that vertically extends the input-image receptive fields of boundary units in a convolutional layer. We evaluate CyCNN and conventional CNN models for classification tasks on rotated MNIST, CIFAR-10, and SVHN datasets. We show that if there is no data augmentation during training, CyCNN significantly improves classification accuracies when compared to conventional CNN models. Our implementation of CyCNN is publicly available on https://github.com/mcrl/CyCNN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/CTW38JJ9/Kim et al. - 2020 - CyCNN A Rotation Invariant CNN using Polar Mappin.pdf;/Users/marinbenc/Zotero/storage/MQWMRPVX/2007.html}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  urldate = {2023-10-05}
}

@inproceedings{liu2021Swin,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021}
}

@article{liuDDNetCartesianpolarDualdomain2019a,
  title = {{{DDNet}}: {{Cartesian-polar Dual-domain Network}} for the {{Joint Optic Disc}} and {{Cup Segmentation}}},
  shorttitle = {{{DDNet}}},
  author = {Liu, Qing and Hong, Xiaopeng and Ke, Wei and Chen, Zailiang and Zou, Beiji},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.08773 [cs]},
  eprint = {1904.08773},
  primaryclass = {cs},
  urldate = {2021-04-01},
  abstract = {Existing joint optic disc and cup segmentation approaches are developed either in Cartesian or polar coordinate system. However, due to the subtle optic cup, the contextual information exploited from the single domain even by the prevailing CNNs is still insufficient. In this paper, we propose a novel segmentation approach, named Cartesian-polar dual-domain network (DDNet), which for the first time considers the complementary of the Cartesian domain and the polar domain. We propose a two-branch of domain feature encoder and learn translation equivariant representations on rectilinear grid from Cartesian domain and rotation equivariant representations on polar grid from polar domain parallelly. To fuse the features on two different grids, we propose a dual-domain fusion module. This module builds the correspondence between two grids by the differentiable polar transform layer and learns the feature importance across two domains in element-wise to enhance the expressive capability. Finally, the decoder aggregates the fused features from low-level to high-level and makes dense predictions. We validate the state-of-the-art segmentation performances of our DDNet on the public dataset ORIGA. According to the segmentation masks, we estimate the commonly used clinical measure for glaucoma, i.e., the vertical cup-to-disc ratio. The low cup-to-disc ratio estimation error demonstrates the potential application in glaucoma screening.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/W36E24YP/Liu et al. - 2019 - DDNet Cartesian-polar Dual-domain Network for the.pdf;/Users/marinbenc/Zotero/storage/3V8FQANE/1904.html}
}

@article{liuSelfsupervisedLearningMore2021,
  title = {Self-Supervised {{Learning}} Is {{More Robust}} to {{Dataset Imbalance}}},
  author = {Liu, Hong and HaoChen, Jeff Z. and Gaidon, Adrien and Ma, Tengyu},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05025 [cs, stat]},
  eprint = {2110.05025},
  primaryclass = {cs, stat},
  urldate = {2022-02-16},
  abstract = {Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find out via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments and theoretical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/64JUHWXH/Liu et al. - 2021 - Self-supervised Learning is More Robust to Dataset.pdf;/Users/marinbenc/Zotero/storage/4KVWATH5/2110.html}
}

@misc{liuUnsupervisedDeepDomain2018,
  title = {Unsupervised {{Deep Domain Adaptation}} for {{Pedestrian Detection}}},
  author = {Liu, Lihang and Lin, Weiyao and Wu, Lisheng and Yu, Yong and Yang, Michael Ying},
  year = {2018},
  month = feb,
  number = {arXiv:1802.03269},
  eprint = {1802.03269},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {This paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes. First, we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain. Meanwhile, we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples. Second, based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise. More specifically, we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer, and add the unsupervised regularizer to further improve the domain adaptation accuracy. In experiments for pedestrian detection, the proposed method boosts the recall value by nearly 30\% while the precision stays almost the same. Furthermore, we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/P2YU3XCR/Liu et al_2018_Unsupervised Deep Domain Adaptation for Pedestrian Detection.pdf;/Users/marinbenc/Zotero/storage/VY7U6WWI/1802.html}
}

@inproceedings{long2015fully,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440}
}

@article{lotanMedicalImagingPrivacy2020,
  title = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}: {{Myth}}, {{Fallacy}}, and the {{Future}}},
  shorttitle = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}},
  author = {Lotan, Eyal and Tschider, Charlotte and Sodickson, Daniel K. and Caplan, Arthur L. and Bruno, Mary and Zhang, Ben and Lui, Yvonne W.},
  year = {2020},
  month = sep,
  journal = {Journal of the American College of Radiology},
  volume = {17},
  number = {9},
  pages = {1159--1162},
  issn = {15461440},
  doi = {10.1016/j.jacr.2020.04.007},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BFEQKKMA/Lotan et al_2020_Medical Imaging and Privacy in the Era of Artificial Intelligence.pdf}
}

@article{lutnickIntegratedIterativeAnnotation2019,
  title = {An Integrated Iterative Annotation Technique for Easing Neural Network Training in Medical Image Analysis},
  author = {Lutnick, Brendon and Ginley, Brandon and Govind, Darshana and McGarry, Sean D. and LaViolette, Peter S. and Yacoub, Rabi and Jain, Sanjay and Tomaszewski, John E. and Jen, Kuang-Yu and Sarder, Pinaki},
  year = {2019},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {2},
  pages = {112--119},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0018-3},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RU4JUGUR/Lutnick et al_2019_An integrated iterative annotation technique for easing neural network training.pdf}
}

@article{mahabadiCardiacComputedTomographyderived2017,
  title = {Cardiac Computed Tomography-Derived Epicardial Fat Volume and Attenuation Independently Distinguish Patients with and without Myocardial Infarction},
  author = {Mahabadi, Amir Abbas and Balcer, Bastian and Dykun, Iryna and Forsting, Michael and Schlosser, Thomas and Heusch, Gerd and Rassaf, Tienush},
  editor = {Merx, Marc W.},
  year = {2017},
  month = aug,
  journal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0183514},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183514},
  urldate = {2023-09-29},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XEEL69CF/Mahabadi et al_2017_Cardiac computed tomography-derived epicardial fat volume and attenuation.pdf}
}

@article{manet,
  title = {{{MA-Net}}: {{A}} Multi-Scale Attention Network for Liver and Tumor Segmentation},
  author = {Fan, Tongle and Wang, Guanglei and Li, Yan and Wang, Hongrui},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {179656--179665},
  doi = {10.1109/ACCESS.2020.3025372}
}

@misc{monaiconsortiumMONAIMedicalOpen2023,
  title = {{{MONAI}}: {{Medical Open Network}} for {{AI}}},
  shorttitle = {{{MONAI}}},
  author = {{MONAI Consortium}},
  year = {2023},
  month = oct,
  doi = {10.5281/ZENODO.4323058},
  urldate = {2023-11-28},
  abstract = {AI Toolkit for Healthcare Imaging},
  copyright = {Apache License 2.0, Open Access},
  howpublished = {Zenodo}
}

@misc{myronenkoAutomated3DSegmentation2023,
  title = {Automated {{3D Segmentation}} of {{Kidneys}} and {{Tumors}} in {{MICCAI KiTS}} 2023 {{Challenge}}},
  author = {Myronenko, Andriy and Yang, Dong and He, Yufan and Xu, Daguang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.04110},
  eprint = {2310.04110},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform for researchers to compare their solutions to segmentation from 3D CT. In this work, we describe our submission to the challenge using automated segmentation of Auto3DSeg available in MONAI. Our solution achieves the average dice of 0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023 challenge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/NM2RZWIK/Myronenko et al_2023_Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge.pdf;/Users/marinbenc/Zotero/storage/2K9HW7IB/2310.html}
}

@incollection{newellStackedHourglassNetworks2016,
  title = {Stacked {{Hourglass Networks}} for {{Human Pose Estimation}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9912},
  pages = {483--499},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46484-8_29},
  urldate = {2021-08-23},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/ZXRR7XDV/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf}
}

@misc{nguyen2020vindrcxr,
  title = {{{VinDr-CXR}}: {{An}} Open Dataset of Chest {{X-rays}} with Radiologist's Annotations},
  author = {Nguyen, Ha Q. and Lam, Khanh and Le, Linh T. and Pham, Hieu H. and Tran, Dat Q. and Nguyen, Dung B. and Le, Dung D. and Pham, Chi M. and Tong, Hang T. T. and Dinh, Diep H. and Do, Cuong D. and Doan, Luu T. and Nguyen, Cuong N. and Nguyen, Binh T. and Nguyen, Que V. and Hoang, Au D. and Phan, Hien N. and Nguyen, Anh T. and Ho, Phuong H. and Ngo, Dat T. and Nguyen, Nghia T. and Nguyen, Nhan T. and Dao, Minh and Vu, Van},
  year = {2020},
  eprint = {2012.15029},
  primaryclass = {eess.IV},
  archiveprefix = {arxiv}
}

@incollection{norooziUnsupervisedLearningVisual2016,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2022-02-16},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XSBR7VU9/Noroozi and Favaro - 2016 - Unsupervised Learning of Visual Representations by.pdf}
}

@incollection{norooziUnsupervisedLearningVisual2016a,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2023-11-30},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/JHCT2EEB/Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  year = {2000},
  journal = {Dr. Dobb's Journal of Software Tools},
  citeulike-article-id = {2236121},
  posted-at = {2008-01-15 19:21:54},
  priority = {4},
  keywords = {bibtex-import}
}

@article{radlAVTMulticenterAortic2022a,
  title = {{{AVT}}: {{Multicenter}} Aortic Vessel Tree {{CTA}} Dataset Collection with Ground Truth Segmentation Masks},
  shorttitle = {{{AVT}}},
  author = {Radl, Lukas and Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Egger, Jan},
  year = {2022},
  month = feb,
  journal = {Data in Brief},
  volume = {40},
  pages = {107801},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.107801},
  urldate = {2023-10-05},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/KI8HGT9Y/Radl et al_2022_AVT.pdf}
}

@article{regionGrowing,
  title = {Seeded Region Growing},
  author = {Adams, R. and Bischof, L.},
  year = {1994},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {6},
  pages = {641--647},
  doi = {10.1109/34.295913}
}

@article{rodriguesNovelApproachAutomated2016,
  title = {A Novel Approach for the Automated Segmentation and Volume Quantification of Cardiac Fats on Computed Tomography},
  author = {Rodrigues, {\'E}.O. and Morais, F.F.C. and Morais, N.A.O.S. and Conci, L.S. and Neto, L.V. and Conci, A.},
  year = {2016},
  month = jan,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {123},
  pages = {109--128},
  issn = {01692607},
  doi = {10.1016/j.cmpb.2015.09.017},
  urldate = {2021-04-13},
  langid = {english}
}

@incollection{Rohlfing2005,
  title = {Quo Vadis, Atlas-Based Segmentation?},
  booktitle = {Handbook of Biomedical Image Analysis},
  author = {Rohlfing, Torsten and Brandt, Robert and Menzel, Randolf and Russakoff, Daniel B. and Maurer, Calvin R.},
  year = {2005},
  pages = {435--486},
  publisher = {{Springer US}},
  doi = {10.1007/0-306-48608-3_11}
}

@misc{rombach2021highresolution,
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2021},
  eprint = {2112.10752},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} {\textendash} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  urldate = {2021-08-23},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/PR8AHFUU/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015d,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-06},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/IYQVGL78/Ronneberger et al_2015_U-Net.pdf;/Users/marinbenc/Zotero/storage/6UK6QXVC/1505.html}
}

@article{rotembergPatientcentricDatasetImages2021,
  title = {A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author = {Rotemberg, Veronica and Kurtansky, Nicholas and {Betz-Stablein}, Brigid and Caffery, Liam and Chousakos, Emmanouil and Codella, Noel and Combalia, Marc and Dusza, Stephen and Guitera, Pascale and Gutman, David and Halpern, Allan and Helba, Brian and Kittler, Harald and Kose, Kivanc and Langer, Steve and Lioprys, Konstantinos and Malvehy, Josep and Musthaq, Shenara and Nanda, Jabpani and Reiter, Ofer and Shih, George and Stratigos, Alexander and Tschandl, Philipp and Weber, Jochen and Soyer, H. Peter},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {34},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00815-z},
  urldate = {2023-10-02},
  abstract = {Abstract             Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients (20.8\% with at least one melanoma, 79.2\% with zero melanomas) from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 (1.8\%) histopathologically confirmed melanomas compared with benign melanoma mimickers.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/P5YJS86H/Rotemberg et al_2021_A patient-centric dataset of images and metadata for identifying melanomas.pdf}
}

@inproceedings{salehinejadImageAugmentationUsing2018,
  title = {Image {{Augmentation Using Radial Transform}} for {{Training Deep Neural Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Salehinejad, Hojjat and Valaee, Shahrokh and Dowdell, Tim and Barfett, Joseph},
  year = {2018},
  month = apr,
  pages = {3016--3020},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8462241},
  urldate = {2021-08-23},
  isbn = {978-1-5386-4658-8}
}

@article{selleAnalysisVasculatureLiver2002,
  title = {Analysis of Vasculature for Liver Surgical Planning},
  author = {Selle, D. and Preim, B. and Schenk, A. and Peitgen, H.-O.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {21},
  number = {11},
  pages = {1344--1357},
  issn = {0278-0062},
  doi = {10.1109/TMI.2002.801166},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{SETR,
  title = {Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers},
  booktitle = {{{CVPR}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H.S. and Zhang, Li},
  year = {2021}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  month = may,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781107298019},
  urldate = {2023-12-01},
  abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
  isbn = {978-1-107-05713-5 978-1-107-29801-9}
}

@incollection{shinMedicalImageSynthesis2018,
  title = {Medical {{Image Synthesis}} for {{Data Augmentation}} and {{Anonymization Using Generative Adversarial Networks}}},
  booktitle = {Simulation and {{Synthesis}} in {{Medical Imaging}}},
  author = {Shin, Hoo-Chang and Tenenholtz, Neil A. and Rogers, Jameson K. and Schwarz, Christopher G. and Senjem, Matthew L. and Gunter, Jeffrey L. and Andriole, Katherine P. and Michalski, Mark},
  editor = {Gooya, Ali and Goksel, Orcun and Oguz, Ipek and Burgos, Ninon},
  year = {2018},
  volume = {11037},
  pages = {1--11},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00536-8_1},
  urldate = {2023-12-01},
  isbn = {978-3-030-00535-1 978-3-030-00536-8},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/WDXVQG8D/Shin et al_2018_Medical Image Synthesis for Data Augmentation and Anonymization Using.pdf}
}

@article{sinclairAtlasISTNJointSegmentation2022a,
  title = {Atlas-{{ISTN}}: {{Joint}} Segmentation, Registration and Atlas Construction with Image-and-Spatial Transformer Networks},
  shorttitle = {Atlas-{{ISTN}}},
  author = {Sinclair, Matthew and Schuh, Andreas and Hahn, Karl and Petersen, Kersten and Bai, Ying and Batten, James and Schaap, Michiel and Glocker, Ben},
  year = {2022},
  month = may,
  journal = {Medical Image Analysis},
  volume = {78},
  pages = {102383},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102383},
  urldate = {2023-10-03},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/Q3S597NR/Sinclair et al_2022_Atlas-ISTN.pdf}
}

@inproceedings{sobhaniniaFetalUltrasoundImage2019,
  title = {Fetal {{Ultrasound Image Segmentation}} for {{Measuring Biometric Parameters Using Multi-Task Deep Learning}}},
  booktitle = {2019 41st {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Sobhaninia, Zahra and Rafiei, Shima and Emami, Ali and Karimi, Nader and Najarian, Kayvan and Samavi, Shadrokh and Reza Soroushmehr, S. M.},
  year = {2019},
  month = jul,
  pages = {6545--6548},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856981},
  urldate = {2023-09-27},
  isbn = {978-1-5386-1311-5},
  file = {/Users/marinbenc/Zotero/storage/IAH26LKM/Sobhaninia et al_2019_Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters Using.pdf}
}

@article{tomarFANetFeedbackAttention2021a,
  title = {{{FANet}}: {{A Feedback Attention Network}} for {{Improved Biomedical Image Segmentation}}},
  shorttitle = {{{FANet}}},
  author = {Tomar, Nikhil Kumar and Jha, Debesh and Riegler, Michael A. and Johansen, H{\aa}vard D. and Johansen, Dag and Rittscher, Jens and Halvorsen, P{\aa}l and Ali, Sharib},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.17235 [cs, eess]},
  eprint = {2103.17235},
  primaryclass = {cs, eess},
  urldate = {2021-04-19},
  abstract = {With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/GV642GF8/Tomar et al. - 2021 - FANet A Feedback Attention Network for Improved B.pdf;/Users/marinbenc/Zotero/storage/QL674DQ4/2103.html}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995347},
  urldate = {2023-11-28},
  isbn = {978-1-4577-0394-2},
  file = {/Users/marinbenc/Zotero/storage/PYHB5657/Torralba_Efros_2011_Unbiased look at dataset bias.pdf}
}

@article{tschandlHAM10000DatasetLarge2018,
  title = {The {{HAM10000}} Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions},
  author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
  year = {2018},
  month = dec,
  journal = {Scientific Data},
  volume = {5},
  number = {1},
  pages = {180161},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.161},
  urldate = {2021-04-13},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/84M39I28/Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf}
}

@article{valanarasuKiUNetOvercompleteConvolutional2020a,
  title = {{{KiU-Net}}: {{Overcomplete Convolutional Architectures}} for {{Biomedical Image}} and {{Volumetric Segmentation}}},
  shorttitle = {{{KiU-Net}}},
  author = {Valanarasu, Jeya Maria Jose and Sindagi, Vishwanath A. and Hacihaliloglu, Ilker and Patel, Vishal M.},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.01663 [cs, eess]},
  eprint = {2010.01663},
  primaryclass = {cs, eess},
  urldate = {2021-04-22},
  abstract = {Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation- KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: https://github.com/jeya-maria-jose/KiU-Net-pytorch},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/L3XWJEEK/Valanarasu et al. - 2020 - KiU-Net Overcomplete Convolutional Architectures .pdf;/Users/marinbenc/Zotero/storage/Z4MWI9FD/2010.html}
}

@article{wasserthalTotalSegmentatorRobustSegmentation2023,
  title = {{{TotalSegmentator}}: {{Robust Segmentation}} of 104 {{Anatomic Structures}} in {{CT Images}}},
  shorttitle = {{{TotalSegmentator}}},
  author = {Wasserthal, Jakob and Breit, Hanns-Christian and Meyer, Manfred T. and Pradella, Maurice and Hinck, Daniel and Sauter, Alexander W. and Heye, Tobias and Boll, Daniel T. and Cyriac, Joshy and Yang, Shan and Bach, Michael and Segeroth, Martin},
  year = {2023},
  month = sep,
  journal = {Radiology: Artificial Intelligence},
  volume = {5},
  number = {5},
  pages = {e230024},
  issn = {2638-6100},
  doi = {10.1148/ryai.230024},
  urldate = {2023-11-27},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RMTDDIKB/Wasserthal et al_2023_TotalSegmentator.pdf}
}

@article{zhangAutomaticEpicardialFat2020a,
  title = {Automatic {{Epicardial Fat Segmentation}} and {{Quantification}} of {{CT Scans Using Dual U-Nets With}} a {{Morphological Processing Layer}}},
  author = {Zhang, Qi and Zhou, Jianhang and Zhang, Bob and Jia, Weijia and Wu, Enhua},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {128032--128041},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3008190},
  urldate = {2021-04-22},
  file = {/Users/marinbenc/Zotero/storage/JV42BARX/Zhang et al. - 2020 - Automatic Epicardial Fat Segmentation and Quantifi.pdf}
}

@inproceedings{zhao2017pspnet,
  title = {Pyramid Scene Parsing Network},
  booktitle = {{{CVPR}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017}
}

@article{zhou2019unetplusplus,
  title = {{{UNet}}++: {{Redesigning}} Skip Connections to Exploit Multiscale Features in Image Segmentation},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2019},
  journal = {IEEE Transactions on Medical Imaging},
  publisher = {{IEEE}}
}

@incollection{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and {Syeda-Mahmood}, Tanveer and Martel, Anne and {Maier-Hein}, Lena and Tavares, Jo{\~a}o Manuel R.S. and Bradley, Andrew and Papa, Jo{\~a}o Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
  year = {2018},
  volume = {11045},
  pages = {3--11},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00889-5_1},
  urldate = {2021-08-23},
  isbn = {978-3-030-00888-8 978-3-030-00889-5},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/FG69V4EC/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf}
}
