@article{antonelliMedicalSegmentationDecathlon2022,
  title = {The {{Medical Segmentation Decathlon}}},
  author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Golia and Rhode, Kawal and {Tobon-Gomez}, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbel{\'a}ez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Ildoo and {Maier-Hein}, Klaus and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2022},
  month = jul,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4128},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30695-9},
  urldate = {2023-09-28},
  abstract = {Abstract             International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD){\textemdash}a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/7ARBRK4N/Antonelli et al_2022_The Medical Segmentation Decathlon.pdf}
}

@inproceedings{attnAllYouNeed,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@article{bastarrikaRelationshipCoronaryArtery2010,
  title = {Relationship {{Between Coronary Artery Disease}} and {{Epicardial Adipose Tissue Quantification}} at {{Cardiac CT}}},
  author = {Bastarrika, Gorka and Broncano, Jordi and Schoepf, U. Joseph and Schwarz, Florian and Lee, Yeong Shyan and Abro, Joseph A. and Costello, Philip and Zwerner, Peter L.},
  year = {2010},
  month = jun,
  journal = {Academic Radiology},
  volume = {17},
  number = {6},
  pages = {727--734},
  issn = {10766332},
  doi = {10.1016/j.acra.2010.01.015},
  urldate = {2023-09-27},
  langid = {english}
}

@article{bencevicRecentProgressEpicardial2022,
  title = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}: {{A Systematic Review}}},
  shorttitle = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}},
  author = {Ben{\v c}evi{\'c}, Marin and Gali{\'c}, Irena and Habijan, Marija and Pi{\v z}urica, Aleksandra},
  year = {2022},
  month = may,
  journal = {Applied Sciences},
  volume = {12},
  number = {10},
  pages = {5217},
  issn = {2076-3417},
  doi = {10.3390/app12105217},
  urldate = {2023-09-29},
  abstract = {Epicardial and pericardial adipose tissues (EAT and PAT), which are located around the heart, have been linked to coronary atherosclerosis, cardiomyopathy, coronary artery disease, and other cardiovascular diseases. Additionally, the volume and thickness of EAT are good predictors of CVD risk levels. Manual quantification of these tissues is a tedious and error-prone process. This paper presents a comprehensive and critical overview of research on the epicardial and pericardial adipose tissue segmentation and quantification methods, evaluates their effectiveness in terms of segmentation time and accuracy, provides a critical comparison of the methods, and presents ongoing and future challenges in the field. Described methods are classified into pericardial adipose tissue segmentation, direct epicardial adipose tissue segmentation, and epicardial adipose tissue segmentation via pericardium delineation. A comprehensive categorization of the underlying methods is conducted with insights into their evolution from traditional image processing methods to recent deep learning-based methods. The paper also provides an overview of the research on the clinical significance of epicardial and pericardial adipose tissues as well as the terminology and definitions used in the medical literature.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/IZLV7MLV/Benčević et al_2022_Recent Progress in Epicardial and Pericardial Adipose Tissue Segmentation and.pdf}
}

@inproceedings{bermudez-chaconDomainadaptiveTwostreamUNet2018,
  title = {A Domain-Adaptive Two-Stream {{U-Net}} for Electron Microscopy Image Segmentation},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {{Bermudez-Chacon}, Roger and {Marquez-Neila}, Pablo and Salzmann, Mathieu and Fua, Pascal},
  year = {2018},
  month = apr,
  pages = {400--404},
  publisher = {{IEEE}},
  address = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363602},
  urldate = {2023-11-28},
  isbn = {978-1-5386-3636-7},
  file = {/Users/marinbenc/Zotero/storage/XQK4IGIP/Bermudez-Chacon et al_2018_A domain-adaptive two-stream U-Net for electron microscopy image segmentation.pdf}
}

@article{calabreseUniversityCaliforniaSan2022,
  title = {The {{University}} of {{California San Francisco Preoperative Diffuse Glioma MRI}} ({{UCSF-PDGM}}) {{Dataset}}},
  author = {Calabrese, Evan and {Villanueva-Meyer}, Javier E. and Rudie, Jeffrey D. and Rauschecker, Andreas M. and Baid, Ujjwal and Bakas, Spyridon and Cha, Soonmee and Mongan, John T. and Hess, Christopher P.},
  year = {2022},
  month = nov,
  journal = {Radiology: Artificial Intelligence},
  volume = {4},
  number = {6},
  eprint = {2109.00356},
  primaryclass = {cs, eess},
  pages = {e220058},
  issn = {2638-6100},
  doi = {10.1148/ryai.220058},
  urldate = {2023-10-05},
  abstract = {Here we present the University of California San Francisco Preoperative Diffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500 subjects with histopathologically-proven diffuse gliomas who were imaged with a standardized 3 Tesla preoperative brain tumor MRI protocol featuring predominantly 3D imaging, as well as advanced diffusion and perfusion imaging techniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation status for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor methylation status for World Health Organization (WHO) grade III and IV gliomas. The UCSF-PDGM has been made publicly available in the hopes that researchers around the world will use these data to continue to push the boundaries of AI applications for diffuse gliomas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/DKRMCWRA/Calabrese et al_2022_The University of California San Francisco Preoperative Diffuse Glioma MRI.pdf;/Users/marinbenc/Zotero/storage/UADUEREK/2109.html}
}

@misc{chen2017rethinking,
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  eprint = {1706.05587},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@article{chen2022vitadapter,
  title = {Vision Transformer Adapter for Dense Predictions},
  author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.08534},
  eprint = {2205.08534},
  archiveprefix = {arxiv}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.05709 [cs, stat]},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  urldate = {2022-02-10},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/9LN634BB/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/Users/marinbenc/Zotero/storage/JSY5MWDX/2002.html}
}

@misc{choHowMuchData2016,
  title = {How Much Data Is Needed to Train a Medical Image Deep Learning System to Achieve Necessary High Accuracy?},
  author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06348},
  eprint = {1511.06348},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marinbenc/Zotero/storage/RWKU3KYG/Cho et al_2016_How much data is needed to train a medical image deep learning system to.pdf;/Users/marinbenc/Zotero/storage/VUF28Q6H/1511.html}
}

@misc{codellaSkinLesionAnalysis2019c,
  title = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018: {{A Challenge Hosted}} by the {{International Skin Imaging Collaboration}} ({{ISIC}})},
  shorttitle = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018},
  author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  year = {2019},
  month = mar,
  number = {arXiv:1902.03368},
  eprint = {1902.03368},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10\% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/4XNGJ29P/Codella et al_2019_Skin Lesion Analysis Toward Melanoma Detection 2018.pdf;/Users/marinbenc/Zotero/storage/DXME4599/1902.html}
}

@article{devunooruDeepLearningNeural2021,
  title = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis: A Recent Review and Taxonomy},
  shorttitle = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis},
  author = {Devunooru, Sindhu and Alsadoon, Abeer and Chandana, P. W. C. and Beg, Azam},
  year = {2021},
  month = jan,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {1},
  pages = {455--483},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-020-01998-w},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{dosovitskiy2021an,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  booktitle = {International Conference on Learning Representations},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021}
}

@misc{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  number = {arXiv:1603.07285},
  eprint = {1603.07285},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-19},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/H4LV3GKY/Dumoulin_Visin_2018_A guide to convolution arithmetic for deep learning.pdf;/Users/marinbenc/Zotero/storage/E2J26Z3H/1603.html}
}

@article{edlundLIVECellLargescaleDataset2021,
  title = {{{LIVECell}}{\textemdash}{{A}} Large-Scale Dataset for Label-Free Live Cell Segmentation},
  author = {Edlund, Christoffer and Jackson, Timothy R. and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  year = {2021},
  month = sep,
  journal = {Nature Methods},
  volume = {18},
  number = {9},
  pages = {1038--1045},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01249-6},
  urldate = {2023-09-28},
  abstract = {Abstract             Light microscopy combined with well-established protocols of two-dimensional cell culture facilitates high-throughput quantitative imaging to study biological phenomena. Accurate segmentation of individual cells in images enables exploration of complex biological questions, but can require sophisticated imaging processing pipelines in cases of low contrast and high object density. Deep learning-based methods are considered state-of-the-art for image segmentation but typically require vast amounts of annotated data, for which there is no suitable resource available in the field of label-free cellular imaging. Here, we present LIVECell, a large, high-quality, manually annotated and expert-validated dataset of phase-contrast images, consisting of over 1.6 million cells from a diverse set of cell morphologies and culture densities. To further demonstrate its use, we train convolutional neural network-based models using LIVECell and evaluate model segmentation accuracy with a proposed a suite of benchmarks.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/TUQV6DIR/Edlund et al_2021_LIVECell—A large-scale dataset for label-free live cell segmentation.pdf}
}

@inproceedings{fasterRCNN,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {91--99},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\textsubscript{r}cnn.}
}

@book{fosbinder2011essentials,
  title = {Essentials of Radiologic Science},
  author = {Fosbinder, R. and Orth, D.},
  year = {2011},
  publisher = {{Wolters Kluwer Health/Lippincott Williams \& Wilkins}},
  isbn = {978-0-7817-7554-0},
  lccn = {2010030972}
}

@inproceedings{ganinDA2015,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  series = {{{ICML}}'15},
  pages = {1180--1189},
  publisher = {{JMLR.org}},
  address = {{Lille, France}},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back propagation.Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  urldate = {2023-10-21},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  file = {/Users/marinbenc/Zotero/storage/MENBPI9N/Hastie et al_2009_The Elements of Statistical Learning.pdf}
}

@inproceedings{heMaskRCNN2017b,
  title = {Mask {{R-CNN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = {2017},
  month = oct,
  pages = {2980--2988},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.322},
  urldate = {2023-10-09},
  isbn = {978-1-5386-1032-9}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/ELVXRVGZ/Irvin et al_2019_CheXpert.pdf;/Users/marinbenc/Zotero/storage/PRSVMGSL/1901.html}
}

@article{isenseeNnUNetSelfconfiguringMethod2021,
  title = {{{nnU-Net}}: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation},
  shorttitle = {{{nnU-Net}}},
  author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and {Maier-Hein}, Klaus H.},
  year = {2021},
  month = feb,
  journal = {Nature Methods},
  volume = {18},
  number = {2},
  pages = {203--211},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-020-01008-z},
  urldate = {2023-10-06},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/H5L3AVAQ/Isensee et al_2021_nnU-Net.pdf}
}

@article{jhaInstanceSegmentationWhole2021a,
  title = {Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment},
  shorttitle = {Instance Segmentation for Whole Slide Imaging},
  author = {Jha, Aadarsh and Yang, Haichun and Deng, Ruining and Kapp, Meghan E. and Fogo, Agnes B. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {Journal of Medical Imaging},
  volume = {8},
  number = {01},
  issn = {2329-4302},
  doi = {10.1117/1.JMI.8.1.014001},
  urldate = {2023-10-02},
  file = {/Users/marinbenc/Zotero/storage/33U6NMXI/Jha et al_2021_Instance segmentation for whole slide imaging.pdf}
}

@incollection{kamalianComputedTomographyImaging2016,
  title = {Computed Tomography Imaging and Angiography {\textendash} Principles},
  booktitle = {Handbook of {{Clinical Neurology}}},
  author = {Kamalian, Shervin and Lev, Michael H. and Gupta, Rajiv},
  year = {2016},
  volume = {135},
  pages = {3--20},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53485-9.00001-5},
  urldate = {2023-09-29},
  isbn = {978-0-444-53485-9},
  langid = {english}
}

@article{Kass1988,
  title = {Snakes: {{Active}} Contour Models},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf00133570}
}

@article{khanAutoCellSegRobustAutomatic2018,
  title = {{{AutoCellSeg}}: Robust Automatic Colony Forming Unit ({{CFU}})/Cell Analysis Using Adaptive Image Segmentation and Easy-to-Use Post-Editing Techniques},
  shorttitle = {{{AutoCellSeg}}},
  author = {Khan, Arif Ul Maula and Torelli, Angelo and Wolf, Ivo and Gretz, Norbert},
  year = {2018},
  month = may,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {7302},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24916-9},
  urldate = {2023-10-16},
  abstract = {Abstract             In biological assays, automated cell/colony segmentation and counting is imperative owing to huge image sets. Problems occurring due to drifting image acquisition conditions, background noise and high variation in colony features in experiments demand a user-friendly, adaptive and robust image processing/analysis method. We present AutoCellSeg (based on MATLAB) that implements a supervised automatic and robust image segmentation method. AutoCellSeg utilizes multi-thresholding aided by a feedback-based watershed algorithm taking segmentation plausibility criteria into account. It is usable in different operation modes and intuitively enables the user to select object features interactively for supervised image segmentation method. It allows the user to correct results with a graphical interface. This publicly available tool outperforms tools like OpenCFU and CellProfiler in terms of accuracy and provides many additional useful features for end-users.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/3UFPAMEA/Khan et al_2018_AutoCellSeg.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  urldate = {2023-10-05}
}

@inproceedings{liu2021Swin,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021}
}

@article{liuSelfsupervisedLearningMore2021,
  title = {Self-Supervised {{Learning}} Is {{More Robust}} to {{Dataset Imbalance}}},
  author = {Liu, Hong and HaoChen, Jeff Z. and Gaidon, Adrien and Ma, Tengyu},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05025 [cs, stat]},
  eprint = {2110.05025},
  primaryclass = {cs, stat},
  urldate = {2022-02-16},
  abstract = {Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find out via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments and theoretical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/64JUHWXH/Liu et al. - 2021 - Self-supervised Learning is More Robust to Dataset.pdf;/Users/marinbenc/Zotero/storage/4KVWATH5/2110.html}
}

@misc{liuUnsupervisedDeepDomain2018,
  title = {Unsupervised {{Deep Domain Adaptation}} for {{Pedestrian Detection}}},
  author = {Liu, Lihang and Lin, Weiyao and Wu, Lisheng and Yu, Yong and Yang, Michael Ying},
  year = {2018},
  month = feb,
  number = {arXiv:1802.03269},
  eprint = {1802.03269},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {This paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes. First, we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain. Meanwhile, we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples. Second, based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise. More specifically, we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer, and add the unsupervised regularizer to further improve the domain adaptation accuracy. In experiments for pedestrian detection, the proposed method boosts the recall value by nearly 30\% while the precision stays almost the same. Furthermore, we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/P2YU3XCR/Liu et al_2018_Unsupervised Deep Domain Adaptation for Pedestrian Detection.pdf;/Users/marinbenc/Zotero/storage/VY7U6WWI/1802.html}
}

@inproceedings{long2015fully,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440}
}

@article{lotanMedicalImagingPrivacy2020,
  title = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}: {{Myth}}, {{Fallacy}}, and the {{Future}}},
  shorttitle = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}},
  author = {Lotan, Eyal and Tschider, Charlotte and Sodickson, Daniel K. and Caplan, Arthur L. and Bruno, Mary and Zhang, Ben and Lui, Yvonne W.},
  year = {2020},
  month = sep,
  journal = {Journal of the American College of Radiology},
  volume = {17},
  number = {9},
  pages = {1159--1162},
  issn = {15461440},
  doi = {10.1016/j.jacr.2020.04.007},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BFEQKKMA/Lotan et al_2020_Medical Imaging and Privacy in the Era of Artificial Intelligence.pdf}
}

@article{lutnickIntegratedIterativeAnnotation2019,
  title = {An Integrated Iterative Annotation Technique for Easing Neural Network Training in Medical Image Analysis},
  author = {Lutnick, Brendon and Ginley, Brandon and Govind, Darshana and McGarry, Sean D. and LaViolette, Peter S. and Yacoub, Rabi and Jain, Sanjay and Tomaszewski, John E. and Jen, Kuang-Yu and Sarder, Pinaki},
  year = {2019},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {2},
  pages = {112--119},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0018-3},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RU4JUGUR/Lutnick et al_2019_An integrated iterative annotation technique for easing neural network training.pdf}
}

@article{mahabadiCardiacComputedTomographyderived2017,
  title = {Cardiac Computed Tomography-Derived Epicardial Fat Volume and Attenuation Independently Distinguish Patients with and without Myocardial Infarction},
  author = {Mahabadi, Amir Abbas and Balcer, Bastian and Dykun, Iryna and Forsting, Michael and Schlosser, Thomas and Heusch, Gerd and Rassaf, Tienush},
  editor = {Merx, Marc W.},
  year = {2017},
  month = aug,
  journal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0183514},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183514},
  urldate = {2023-09-29},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XEEL69CF/Mahabadi et al_2017_Cardiac computed tomography-derived epicardial fat volume and attenuation.pdf}
}

@article{manet,
  title = {{{MA-Net}}: {{A}} Multi-Scale Attention Network for Liver and Tumor Segmentation},
  author = {Fan, Tongle and Wang, Guanglei and Li, Yan and Wang, Hongrui},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {179656--179665},
  doi = {10.1109/ACCESS.2020.3025372}
}

@misc{monaiconsortiumMONAIMedicalOpen2023,
  title = {{{MONAI}}: {{Medical Open Network}} for {{AI}}},
  shorttitle = {{{MONAI}}},
  author = {{MONAI Consortium}},
  year = {2023},
  month = oct,
  doi = {10.5281/ZENODO.4323058},
  urldate = {2023-11-28},
  abstract = {AI Toolkit for Healthcare Imaging},
  copyright = {Apache License 2.0, Open Access},
  howpublished = {Zenodo}
}

@misc{myronenkoAutomated3DSegmentation2023,
  title = {Automated {{3D Segmentation}} of {{Kidneys}} and {{Tumors}} in {{MICCAI KiTS}} 2023 {{Challenge}}},
  author = {Myronenko, Andriy and Yang, Dong and He, Yufan and Xu, Daguang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.04110},
  eprint = {2310.04110},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform for researchers to compare their solutions to segmentation from 3D CT. In this work, we describe our submission to the challenge using automated segmentation of Auto3DSeg available in MONAI. Our solution achieves the average dice of 0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023 challenge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/NM2RZWIK/Myronenko et al_2023_Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge.pdf;/Users/marinbenc/Zotero/storage/2K9HW7IB/2310.html}
}

@misc{nguyen2020vindrcxr,
  title = {{{VinDr-CXR}}: {{An}} Open Dataset of Chest {{X-rays}} with Radiologist's Annotations},
  author = {Nguyen, Ha Q. and Lam, Khanh and Le, Linh T. and Pham, Hieu H. and Tran, Dat Q. and Nguyen, Dung B. and Le, Dung D. and Pham, Chi M. and Tong, Hang T. T. and Dinh, Diep H. and Do, Cuong D. and Doan, Luu T. and Nguyen, Cuong N. and Nguyen, Binh T. and Nguyen, Que V. and Hoang, Au D. and Phan, Hien N. and Nguyen, Anh T. and Ho, Phuong H. and Ngo, Dat T. and Nguyen, Nghia T. and Nguyen, Nhan T. and Dao, Minh and Vu, Van},
  year = {2020},
  eprint = {2012.15029},
  primaryclass = {eess.IV},
  archiveprefix = {arxiv}
}

@incollection{norooziUnsupervisedLearningVisual2016,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2022-02-16},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XSBR7VU9/Noroozi and Favaro - 2016 - Unsupervised Learning of Visual Representations by.pdf}
}

@incollection{norooziUnsupervisedLearningVisual2016a,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2023-11-30},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/JHCT2EEB/Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf}
}

@article{radlAVTMulticenterAortic2022a,
  title = {{{AVT}}: {{Multicenter}} Aortic Vessel Tree {{CTA}} Dataset Collection with Ground Truth Segmentation Masks},
  shorttitle = {{{AVT}}},
  author = {Radl, Lukas and Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Egger, Jan},
  year = {2022},
  month = feb,
  journal = {Data in Brief},
  volume = {40},
  pages = {107801},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.107801},
  urldate = {2023-10-05},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/KI8HGT9Y/Radl et al_2022_AVT.pdf}
}

@article{regionGrowing,
  title = {Seeded Region Growing},
  author = {Adams, R. and Bischof, L.},
  year = {1994},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {6},
  pages = {641--647},
  doi = {10.1109/34.295913}
}

@incollection{Rohlfing2005,
  title = {Quo Vadis, Atlas-Based Segmentation?},
  booktitle = {Handbook of Biomedical Image Analysis},
  author = {Rohlfing, Torsten and Brandt, Robert and Menzel, Randolf and Russakoff, Daniel B. and Maurer, Calvin R.},
  year = {2005},
  pages = {435--486},
  publisher = {{Springer US}},
  doi = {10.1007/0-306-48608-3_11}
}

@misc{rombach2021highresolution,
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2021},
  eprint = {2112.10752},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@misc{ronnebergerUNetConvolutionalNetworks2015d,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-06},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/IYQVGL78/Ronneberger et al_2015_U-Net.pdf;/Users/marinbenc/Zotero/storage/6UK6QXVC/1505.html}
}

@article{rotembergPatientcentricDatasetImages2021,
  title = {A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author = {Rotemberg, Veronica and Kurtansky, Nicholas and {Betz-Stablein}, Brigid and Caffery, Liam and Chousakos, Emmanouil and Codella, Noel and Combalia, Marc and Dusza, Stephen and Guitera, Pascale and Gutman, David and Halpern, Allan and Helba, Brian and Kittler, Harald and Kose, Kivanc and Langer, Steve and Lioprys, Konstantinos and Malvehy, Josep and Musthaq, Shenara and Nanda, Jabpani and Reiter, Ofer and Shih, George and Stratigos, Alexander and Tschandl, Philipp and Weber, Jochen and Soyer, H. Peter},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {34},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00815-z},
  urldate = {2023-10-02},
  abstract = {Abstract             Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients (20.8\% with at least one melanoma, 79.2\% with zero melanomas) from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 (1.8\%) histopathologically confirmed melanomas compared with benign melanoma mimickers.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/P5YJS86H/Rotemberg et al_2021_A patient-centric dataset of images and metadata for identifying melanomas.pdf}
}

@article{selleAnalysisVasculatureLiver2002,
  title = {Analysis of Vasculature for Liver Surgical Planning},
  author = {Selle, D. and Preim, B. and Schenk, A. and Peitgen, H.-O.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {21},
  number = {11},
  pages = {1344--1357},
  issn = {0278-0062},
  doi = {10.1109/TMI.2002.801166},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{SETR,
  title = {Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers},
  booktitle = {{{CVPR}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H.S. and Zhang, Li},
  year = {2021}
}

@article{sinclairAtlasISTNJointSegmentation2022a,
  title = {Atlas-{{ISTN}}: {{Joint}} Segmentation, Registration and Atlas Construction with Image-and-Spatial Transformer Networks},
  shorttitle = {Atlas-{{ISTN}}},
  author = {Sinclair, Matthew and Schuh, Andreas and Hahn, Karl and Petersen, Kersten and Bai, Ying and Batten, James and Schaap, Michiel and Glocker, Ben},
  year = {2022},
  month = may,
  journal = {Medical Image Analysis},
  volume = {78},
  pages = {102383},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102383},
  urldate = {2023-10-03},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/Q3S597NR/Sinclair et al_2022_Atlas-ISTN.pdf}
}

@inproceedings{sobhaniniaFetalUltrasoundImage2019,
  title = {Fetal {{Ultrasound Image Segmentation}} for {{Measuring Biometric Parameters Using Multi-Task Deep Learning}}},
  booktitle = {2019 41st {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Sobhaninia, Zahra and Rafiei, Shima and Emami, Ali and Karimi, Nader and Najarian, Kayvan and Samavi, Shadrokh and Reza Soroushmehr, S. M.},
  year = {2019},
  month = jul,
  pages = {6545--6548},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856981},
  urldate = {2023-09-27},
  isbn = {978-1-5386-1311-5},
  file = {/Users/marinbenc/Zotero/storage/IAH26LKM/Sobhaninia et al_2019_Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters Using.pdf}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995347},
  urldate = {2023-11-28},
  isbn = {978-1-4577-0394-2},
  file = {/Users/marinbenc/Zotero/storage/PYHB5657/Torralba_Efros_2011_Unbiased look at dataset bias.pdf}
}

@article{wasserthalTotalSegmentatorRobustSegmentation2023,
  title = {{{TotalSegmentator}}: {{Robust Segmentation}} of 104 {{Anatomic Structures}} in {{CT Images}}},
  shorttitle = {{{TotalSegmentator}}},
  author = {Wasserthal, Jakob and Breit, Hanns-Christian and Meyer, Manfred T. and Pradella, Maurice and Hinck, Daniel and Sauter, Alexander W. and Heye, Tobias and Boll, Daniel T. and Cyriac, Joshy and Yang, Shan and Bach, Michael and Segeroth, Martin},
  year = {2023},
  month = sep,
  journal = {Radiology: Artificial Intelligence},
  volume = {5},
  number = {5},
  pages = {e230024},
  issn = {2638-6100},
  doi = {10.1148/ryai.230024},
  urldate = {2023-11-27},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RMTDDIKB/Wasserthal et al_2023_TotalSegmentator.pdf}
}

@inproceedings{zhao2017pspnet,
  title = {Pyramid Scene Parsing Network},
  booktitle = {{{CVPR}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017}
}

@article{zhou2019unetplusplus,
  title = {{{UNet}}++: {{Redesigning}} Skip Connections to Exploit Multiscale Features in Image Segmentation},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2019},
  journal = {IEEE Transactions on Medical Imaging},
  publisher = {{IEEE}}
}
