@article{antonelliMedicalSegmentationDecathlon2022,
  title = {The {{Medical Segmentation Decathlon}}},
  author = {Antonelli, Michela and Reinke, Annika and Bakas, Spyridon and Farahani, Keyvan and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Van Ginneken, Bram and Bilello, Michel and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc J. and Heckers, Stephan H. and Huisman, Henkjan and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Pernicka, Jennifer S. Golia and Rhode, Kawal and {Tobon-Gomez}, Catalina and Vorontsov, Eugene and Meakin, James A. and Ourselin, Sebastien and Wiesenfarth, Manuel and Arbel{\'a}ez, Pablo and Bae, Byeonguk and Chen, Sihong and Daza, Laura and Feng, Jianjiang and He, Baochun and Isensee, Fabian and Ji, Yuanfeng and Jia, Fucang and Kim, Ildoo and {Maier-Hein}, Klaus and Merhof, Dorit and Pai, Akshay and Park, Beomhee and Perslev, Mathias and Rezaiifar, Ramin and Rippel, Oliver and Sarasua, Ignacio and Shen, Wei and Son, Jaemin and Wachinger, Christian and Wang, Liansheng and Wang, Yan and Xia, Yingda and Xu, Daguang and Xu, Zhanwei and Zheng, Yefeng and Simpson, Amber L. and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2022},
  month = jul,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4128},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-30695-9},
  urldate = {2023-09-28},
  abstract = {Abstract             International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD){\textemdash}a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/7ARBRK4N/Antonelli et al_2022_The Medical Segmentation Decathlon.pdf}
}

@inproceedings{attnAllYouNeed,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{azadBiDirectionalConvLSTMUNet2019,
  title = {Bi-{{Directional ConvLSTM U-Net}} with {{Densley Connected Convolutions}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Azad, Reza and {Asadi-Aghbolaghi}, Maryam and Fathy, Mahmood and Escalera, Sergio},
  year = {2019},
  month = oct,
  pages = {406--415},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCVW.2019.00052},
  urldate = {2021-08-23},
  isbn = {978-1-72815-023-9},
  file = {/Users/marinbenc/Zotero/storage/WF8FX5RR/Azad et al. - 2019 - Bi-Directional ConvLSTM U-Net with Densley Connect.pdf}
}

@article{bastarrikaRelationshipCoronaryArtery2010,
  title = {Relationship {{Between Coronary Artery Disease}} and {{Epicardial Adipose Tissue Quantification}} at {{Cardiac CT}}},
  author = {Bastarrika, Gorka and Broncano, Jordi and Schoepf, U. Joseph and Schwarz, Florian and Lee, Yeong Shyan and Abro, Joseph A. and Costello, Philip and Zwerner, Peter L.},
  year = {2010},
  month = jun,
  journal = {Academic Radiology},
  volume = {17},
  number = {6},
  pages = {727--734},
  issn = {10766332},
  doi = {10.1016/j.acra.2010.01.015},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{bencevicEpicardialAdiposeTissue2021,
  title = {Epicardial {{Adipose Tissue Segmentation}} from {{CT Images}} with {{A Semi-3D Neural Network}}},
  booktitle = {2021 {{International Symposium ELMAR}}},
  author = {Ben{\v c}evi{\'c}, Marin and Habijan, Marija and Gali{\'c}, Irena},
  year = {2021},
  month = sep,
  pages = {87--90},
  publisher = {{IEEE}},
  address = {{Zadar, Croatia}},
  doi = {10.1109/ELMAR52657.2021.9550936},
  isbn = {978-1-66544-437-8},
  file = {/Users/marinbenc/Zotero/storage/7C89QBMY/Bencevic et al_2021_Epicardial Adipose Tissue Segmentation from CT Images with A Semi-3D Neural.pdf}
}

@article{bencevicRecentProgressEpicardial2022,
  title = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}: {{A Systematic Review}}},
  shorttitle = {Recent {{Progress}} in {{Epicardial}} and {{Pericardial Adipose Tissue Segmentation}} and {{Quantification Based}} on {{Deep Learning}}},
  author = {Ben{\v c}evi{\'c}, Marin and Gali{\'c}, Irena and Habijan, Marija and Pi{\v z}urica, Aleksandra},
  year = {2022},
  month = may,
  journal = {Applied Sciences},
  volume = {12},
  number = {10},
  pages = {5217},
  issn = {2076-3417},
  doi = {10.3390/app12105217},
  abstract = {Epicardial and pericardial adipose tissues (EAT and PAT), which are located around the heart, have been linked to coronary atherosclerosis, cardiomyopathy, coronary artery disease, and other cardiovascular diseases. Additionally, the volume and thickness of EAT are good predictors of CVD risk levels. Manual quantification of these tissues is a tedious and error-prone process. This paper presents a comprehensive and critical overview of research on the epicardial and pericardial adipose tissue segmentation and quantification methods, evaluates their effectiveness in terms of segmentation time and accuracy, provides a critical comparison of the methods, and presents ongoing and future challenges in the field. Described methods are classified into pericardial adipose tissue segmentation, direct epicardial adipose tissue segmentation, and epicardial adipose tissue segmentation via pericardium delineation. A comprehensive categorization of the underlying methods is conducted with insights into their evolution from traditional image processing methods to recent deep learning-based methods. The paper also provides an overview of the research on the clinical significance of epicardial and pericardial adipose tissues as well as the terminology and definitions used in the medical literature.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/IZLV7MLV/Benčević et al_2022_Recent Progress in Epicardial and Pericardial Adipose Tissue Segmentation and.pdf}
}

@article{bencevicSegmentthenSegmentContextPreservingCropBased2023a,
  title = {Segment-Then-{{Segment}}: {{Context-Preserving Crop-Based Segmentation}} for {{Large Biomedical Images}}},
  shorttitle = {Segment-Then-{{Segment}}},
  author = {Ben{\v c}evi{\'c}, Marin and Qiu, Yuming and Gali{\'c}, Irena and Pizurica, Aleksandra},
  year = {2023},
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {2},
  pages = {633},
  issn = {1424-8220},
  doi = {10.3390/s23020633},
  abstract = {Medical images are often of huge size, which presents a challenge in terms of memory requirements when training machine learning models. Commonly, the images are downsampled to overcome this challenge, but this leads to a loss of information. We present a general approach for training semantic segmentation neural networks on much smaller input sizes called Segment-then-Segment. To reduce the input size, we use image crops instead of downscaling. One neural network performs the initial segmentation on a downscaled image. This segmentation is then used to take the most salient crops of the full-resolution image with the surrounding context. Each crop is segmented using a second specially trained neural network. The segmentation masks of each crop are joined to form the final output image. We evaluate our approach on multiple medical image modalities (microscopy, colonoscopy, and CT) and show that this approach greatly improves segmentation performance with small network input sizes when compared to baseline models trained on downscaled images, especially in terms of pixel-wise recall.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/W7BVETBY/Benčević et al_2023_Segment-then-Segment.pdf}
}

@inproceedings{bencevicSelfsupervisedLearningMeans2022,
  title = {Self-Supervised {{Learning}} as a {{Means}} to {{Reduce}} the {{Need}} for {{Labeled Data}} in {{Medical Image Analysis}}},
  booktitle = {2022 30th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Ben{\v c}evi{\'c}, Marin and Habijan, Marija and Gali{\'c}, Irena and Pizurica, Aleksandra},
  year = {2022},
  month = aug,
  pages = {1328--1332},
  publisher = {{IEEE}},
  address = {{Belgrade, Serbia}},
  doi = {10.23919/EUSIPCO55093.2022.9909542},
  isbn = {978-90-827970-9-1},
  file = {/Users/marinbenc/Zotero/storage/7QN6HGZ7/Bencevic et al_2022_Self-supervised Learning as a Means to Reduce the Need for Labeled Data in.pdf}
}

@article{bencevicTrainingPolarImage2021,
  title = {Training on {{Polar Image Transformations Improves Biomedical Image Segmentation}}},
  author = {Ben{\v c}evi{\'c}, Marin and Gali{\'c}, Irena and Habijan, Marija and Babin, Danilo},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {133365--133375},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3116265},
  urldate = {2022-05-04},
  file = {/Users/marinbenc/Zotero/storage/R2AIT49X/Bencevic et al. - 2021 - Training on Polar Image Transformations Improves B.pdf}
}

@article{bencevicUnderstandingSkinColor2024,
  title = {Understanding Skin Color Bias in Deep Learning-Based Skin Lesion Segmentation},
  author = {Ben{\v c}evi{\'c}, Marin and Habijan, Marija and Gali{\'c}, Irena and Babin, Danilo and Pi{\v z}urica, Aleksandra},
  year = {2024},
  month = mar,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {245},
  pages = {108044},
  issn = {01692607},
  doi = {10.1016/j.cmpb.2024.108044},
  langid = {english}
}

@inproceedings{bencevicUsingPolarTransform2022,
  title = {Using the {{Polar Transform}} for {{Efficient Deep Learning-Based Aorta Segmentation}} in {{CTA Images}}},
  booktitle = {2022 {{International Symposium ELMAR}}},
  author = {Ben{\v c}evi{\'c}, Marin and Habijan, Marija and Gali{\'c}, Irena and Babin, Danilo},
  year = {2022},
  month = sep,
  pages = {191--194},
  issn = {1334-2630},
  doi = {10.1109/ELMAR55880.2022.9899786},
  abstract = {Medical image segmentation often requires segmenting multiple elliptical objects on a single image. This includes, among other tasks, segmenting vessels such as the aorta in axial CTA slices. In this paper, we present a general approach to improving the semantic segmentation performance of neural networks in these tasks and validate our approach on the task of aorta segmentation. We use a cascade of two neural networks, where one performs a rough segmentation based on the U-Net architecture and the other performs the final segmentation on polar image transformations of the input. Connected component analysis of the rough segmentation is used to construct the polar transformations, and predictions on multiple transformations of the same image are fused using hysteresis thresholding. We show that this method improves aorta segmentation performance without requiring complex neural network architectures. In addition, we show that our approach improves robustness and pixel-level recall while achieving segmentation performance in line with the state of the art.},
  keywords = {Biomedical image processing,Convolutional neural network,Convolutional neural networks,Image segmentation,medical image processing,medical image segmentation,Neural networks,Robustness,semantic segmentation,Thresholding (Imaging),Transforms},
  file = {/Users/marinbenc/Zotero/storage/FG2XNBL6/9899786.html}
}

@inproceedings{bencevicUsingPolarTransform2022a,
  title = {Using the {{Polar Transform}} for {{Efficient Deep Learning-Based Aorta Segmentation}} in {{CTA Images}}},
  booktitle = {2022 {{International Symposium ELMAR}}},
  author = {Ben{\v c}evi{\'c}, Marin and Habijan, Marija and Gali{\'c}, Irena and Babin, Danilo},
  year = {2022},
  month = sep,
  pages = {191--194},
  publisher = {{IEEE}},
  address = {{Zadar, Croatia}},
  doi = {10.1109/ELMAR55880.2022.9899786},
  isbn = {978-1-66547-003-2},
  file = {/Users/marinbenc/Zotero/storage/EK88MEYY/Bencevic et al_2022_Using the Polar Transform for Efficient Deep Learning-Based Aorta Segmentation.pdf}
}

@inproceedings{bermudez-chaconDomainadaptiveTwostreamUNet2018,
  title = {A Domain-Adaptive Two-Stream {{U-Net}} for Electron Microscopy Image Segmentation},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {{Bermudez-Chacon}, Roger and {Marquez-Neila}, Pablo and Salzmann, Mathieu and Fua, Pascal},
  year = {2018},
  month = apr,
  pages = {400--404},
  publisher = {{IEEE}},
  address = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363602},
  urldate = {2023-11-28},
  isbn = {978-1-5386-3636-7},
  file = {/Users/marinbenc/Zotero/storage/XQK4IGIP/Bermudez-Chacon et al_2018_A domain-adaptive two-stream U-Net for electron microscopy image segmentation.pdf}
}

@article{bernalWMDOVAMapsAccurate2015,
  title = {{{WM-DOVA}} Maps for Accurate Polyp Highlighting in Colonoscopy: {{Validation}} vs. Saliency Maps from Physicians},
  shorttitle = {{{WM-DOVA}} Maps for Accurate Polyp Highlighting in Colonoscopy},
  author = {Bernal, Jorge and S{\'a}nchez, F. Javier and {Fern{\'a}ndez-Esparrach}, Gloria and Gil, Debora and Rodr{\'i}guez, Cristina and Vilari{\~n}o, Fernando},
  year = {2015},
  month = jul,
  journal = {Computerized Medical Imaging and Graphics},
  volume = {43},
  pages = {99--111},
  issn = {08956111},
  doi = {10.1016/j.compmedimag.2015.02.007},
  urldate = {2021-04-13},
  langid = {english}
}

@article{bilicLiverTumorSegmentation2019,
  title = {The {{Liver Tumor Segmentation Benchmark}} ({{LiTS}})},
  author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"u}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`a}, Jana and Lowengrub, John and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"u}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"o}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and {van Ginneken}, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and {Ben-Cohen}, Avi and Klang, Eyal and Amitai, Marianne M. and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and {Lev-Cohain}, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.04056 [cs]},
  eprint = {1901.04056},
  primaryclass = {cs},
  urldate = {2021-04-13},
  abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/K3CADVKU/Bilic et al. - 2019 - The Liver Tumor Segmentation Benchmark (LiTS).pdf;/Users/marinbenc/Zotero/storage/BWEWN65C/1901.html}
}

@article{caicedoNucleusSegmentationImaging2019,
  title = {Nucleus Segmentation across Imaging Experiments: The 2018 {{Data Science Bowl}}},
  shorttitle = {Nucleus Segmentation across Imaging Experiments},
  author = {Caicedo, Juan C. and Goodman, Allen and Karhohs, Kyle W. and Cimini, Beth A. and Ackerman, Jeanelle and Haghighi, Marzieh and Heng, CherKeng and Becker, Tim and Doan, Minh and McQuin, Claire and Rohban, Mohammad and Singh, Shantanu and Carpenter, Anne E.},
  year = {2019},
  month = dec,
  journal = {Nature Methods},
  volume = {16},
  number = {12},
  pages = {1247--1253},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0612-7},
  urldate = {2022-09-15},
  abstract = {Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Image processing,Machine learning},
  file = {/Users/marinbenc/Zotero/storage/DLM7E8AT/Caicedo et al. - 2019 - Nucleus segmentation across imaging experiments t.pdf;/Users/marinbenc/Zotero/storage/CBFG6Z5X/s41592-019-0612-7.html}
}

@article{calabreseUniversityCaliforniaSan2022,
  title = {The {{University}} of {{California San Francisco Preoperative Diffuse Glioma MRI}} ({{UCSF-PDGM}}) {{Dataset}}},
  author = {Calabrese, Evan and {Villanueva-Meyer}, Javier E. and Rudie, Jeffrey D. and Rauschecker, Andreas M. and Baid, Ujjwal and Bakas, Spyridon and Cha, Soonmee and Mongan, John T. and Hess, Christopher P.},
  year = {2022},
  month = nov,
  journal = {Radiology: Artificial Intelligence},
  volume = {4},
  number = {6},
  eprint = {2109.00356},
  primaryclass = {cs, eess},
  pages = {e220058},
  issn = {2638-6100},
  doi = {10.1148/ryai.220058},
  urldate = {2023-10-05},
  abstract = {Here we present the University of California San Francisco Preoperative Diffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500 subjects with histopathologically-proven diffuse gliomas who were imaged with a standardized 3 Tesla preoperative brain tumor MRI protocol featuring predominantly 3D imaging, as well as advanced diffusion and perfusion imaging techniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation status for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor methylation status for World Health Organization (WHO) grade III and IV gliomas. The UCSF-PDGM has been made publicly available in the hopes that researchers around the world will use these data to continue to push the boundaries of AI applications for diffuse gliomas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/DKRMCWRA/Calabrese et al_2022_The University of California San Francisco Preoperative Diffuse Glioma MRI.pdf;/Users/marinbenc/Zotero/storage/UADUEREK/2109.html}
}

@misc{carr2021shuffle,
  title = {Shuffle to {{Learn}}: {{Self-supervised}} Learning from Permutations via Differentiable Ranking},
  author = {Carr, Andrew N and Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Zeghidour, Neil},
  year = {2021}
}

@inproceedings{chaoHarDNetLowMemory2019,
  title = {{{HarDNet}}: {{A Low Memory Traffic Network}}},
  shorttitle = {{{HarDNet}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Chao, Ping and Kao, Chao-Yang and Ruan, Yushan and Huang, Chien-Hsiang and Lin, Youn-Long},
  year = {2019},
  month = oct,
  pages = {3551--3560},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00365},
  urldate = {2021-08-23},
  isbn = {978-1-72814-803-8},
  file = {/Users/marinbenc/Zotero/storage/9EV3N48V/Chao et al. - 2019 - HarDNet A Low Memory Traffic Network.pdf}
}

@misc{chen2017rethinking,
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  eprint = {1706.05587},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@article{chen2022vitadapter,
  title = {Vision Transformer Adapter for Dense Predictions},
  author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.08534},
  eprint = {2205.08534},
  archiveprefix = {arxiv}
}

@incollection{chenEncoderDecoderAtrousSeparable2018,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2018},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11211},
  pages = {833--851},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_49},
  urldate = {2021-08-23},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  file = {/Users/marinbenc/Zotero/storage/7WE8QGJL/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf}
}

@incollection{chenEncoderDecoderAtrousSeparable2018b,
  title = {Encoder-{{Decoder}} with {{Atrous Separable Convolution}} for {{Semantic Image Segmentation}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2018},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11211},
  pages = {833--851},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01234-2_49},
  urldate = {2022-10-25},
  isbn = {978-3-030-01233-5 978-3-030-01234-2},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/53WXAWMA/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution .pdf}
}

@article{Cheng2010,
  title = {Pericardial Fat Burden on {{ECG-Gated}} Noncontrast {{CT}} in Asymptomatic Patients Who Subsequently Experience Adverse Cardiovascular Events},
  author = {Cheng, Victor Y. and Dey, Damini and Tamarappoo, Balaji and Nakazato, Ryo and Gransar, Heidi and {Miranda-Peats}, Romalisa and Ramesh, Amit and Wong, Nathan D. and Shaw, Leslee J. and Slomka, Piotr J. and Berman, Daniel S.},
  year = {2010},
  month = apr,
  journal = {JACC: Cardiovascular Imaging},
  volume = {3},
  number = {4},
  pages = {352--360},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jcmg.2009.12.013}
}

@article{chenMultistageLearningSegmentation2021,
  title = {Multi-Stage Learning for Segmentation of Aortic Dissections Using a Prior Aortic Anatomy Simplification},
  author = {Chen, Duanduan and Zhang, Xuyang and Mei, Yuqian and Liao, Fangzhou and Xu, Huanming and Li, Zhenfeng and Xiao, Qianjiang and Guo, Wei and Zhang, Hongkun and Yan, Tianyi and Xiong, Jiang and Ventikos, Yiannis},
  year = {2021},
  month = apr,
  journal = {Medical Image Analysis},
  volume = {69},
  pages = {101931},
  issn = {13618415},
  doi = {10.1016/j.media.2020.101931},
  urldate = {2022-05-05},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/SGA5TPQ8/Chen et al. - 2021 - Multi-stage learning for segmentation of aortic di.pdf}
}

@article{Chenn2009,
  title = {Correlation of Pericardial and Mediastinal Fat with Coronary Artery Disease, Metabolic Syndrome, and Cardiac Risk Factors},
  author = {Chenn, Onn and Ahmad, Ijaz and Hua, Betty and Sockolow, Joshua A and Klem, Igor and Sacchi, Terrence and Heitner, John F},
  year = {2009},
  month = jan,
  journal = {Journal of Cardiovascular Magnetic Resonance},
  volume = {11},
  number = {S1},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1186/1532-429x-11-s1-o16}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.05709 [cs, stat]},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  urldate = {2022-02-10},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/9LN634BB/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/Users/marinbenc/Zotero/storage/JSY5MWDX/2002.html}
}

@techreport{cheungComputationallyEfficientApproach2021,
  type = {Preprint},
  title = {A Computationally Efficient Approach to Segmentation of the Aorta and Coronary Arteries Using Deep Learning},
  author = {Cheung, Wing Keung and Bell, Robert and Nair, Arjun and Menezies, Leon and Patel, Riyaz and Wan, Simon and Chou, Kacy and Chen, Jiahang and Torii, Ryo and Davies, Rhodri H. and Moon, James C. and Alexander, Daniel C. and Jacob, Joseph},
  year = {2021},
  month = feb,
  institution = {{Cardiovascular Medicine}},
  doi = {10.1101/2021.02.18.21252005},
  urldate = {2022-05-09},
  abstract = {Abstract           A fully automatic two-dimensional Unet model is proposed to segment aorta and coronary arteries in computed tomography images. Two models are trained to segment two regions of interest, (1) the aorta and the coronary arteries or (2) the coronary arteries alone. Our method achieves 91.20\% and 88.80\% dice similarity coefficient accuracy on regions of interest 1 and 2 respectively. Compared with a semi-automatic segmentation method, our model performs better when segmenting the coronary arteries alone. The performance of the proposed method is comparable to existing published two-dimensional or three-dimensional deep learning models. Furthermore, the algorithmic and graphical processing unit memory efficiencies are maintained such that the model can be deployed within hospital computer networks where graphical processing units are typically not available.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/FWH7X8XS/Cheung et al. - 2021 - A computationally efficient approach to segmentati.pdf}
}

@misc{choHowMuchData2016,
  title = {How Much Data Is Needed to Train a Medical Image Deep Learning System to Achieve Necessary High Accuracy?},
  author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
  year = {2016},
  month = jan,
  number = {arXiv:1511.06348},
  eprint = {1511.06348},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/marinbenc/Zotero/storage/RWKU3KYG/Cho et al_2016_How much data is needed to train a medical image deep learning system to.pdf;/Users/marinbenc/Zotero/storage/VUF28Q6H/1511.html}
}

@inproceedings{codellaSkinLesionAnalysis2018,
  title = {Skin Lesion Analysis toward Melanoma Detection: {{A}} Challenge at the 2017 {{International}} Symposium on Biomedical Imaging ({{ISBI}}), Hosted by the International Skin Imaging Collaboration ({{ISIC}})},
  shorttitle = {Skin Lesion Analysis toward Melanoma Detection},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Codella, Noel C. F. and Gutman, David and Celebi, M. Emre and Helba, Brian and Marchetti, Michael A. and Dusza, Stephen W. and Kalloo, Aadi and Liopyris, Konstantinos and Mishra, Nabin and Kittler, Harald and Halpern, Allan},
  year = {2018},
  month = apr,
  pages = {168--172},
  publisher = {{IEEE}},
  address = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363547},
  urldate = {2021-08-23},
  isbn = {978-1-5386-3636-7},
  file = {/Users/marinbenc/Zotero/storage/YWYZW6YY/Codella et al. - 2018 - Skin lesion analysis toward melanoma detection A .pdf}
}

@misc{codellaSkinLesionAnalysis2019c,
  title = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018: {{A Challenge Hosted}} by the {{International Skin Imaging Collaboration}} ({{ISIC}})},
  shorttitle = {Skin {{Lesion Analysis Toward Melanoma Detection}} 2018},
  author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  year = {2019},
  month = mar,
  number = {arXiv:1902.03368},
  eprint = {1902.03368},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-28},
  abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10\% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/4XNGJ29P/Codella et al_2019_Skin Lesion Analysis Toward Melanoma Detection 2018.pdf;/Users/marinbenc/Zotero/storage/DXME4599/1902.html}
}

@article{Commandeur2018,
  title = {Deep Learning for Quantification of Epicardial and Thoracic Adipose Tissue from Non-Contrast {{CT}}},
  author = {Commandeur, Frederic and Goeller, Markus and Betancur, Julian and Cadet, Sebastien and Doris, Mhairi and Chen, Xi and Berman, Daniel S. and Slomka, Piotr J. and Tamarappoo, Balaji K. and Dey, Damini},
  year = {2018},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {37},
  number = {8},
  pages = {1835--1846},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tmi.2018.2804799}
}

@article{commandeurDeepLearningQuantification2018a,
  title = {Deep {{Learning}} for {{Quantification}} of {{Epicardial}} and {{Thoracic Adipose Tissue From Non-Contrast CT}}},
  author = {Commandeur, Frederic and Goeller, Markus and Betancur, Julian and Cadet, Sebastien and Doris, Mhairi and Chen, Xi and Berman, Daniel S. and Slomka, Piotr J. and Tamarappoo, Balaji K. and Dey, Damini},
  year = {2018},
  month = aug,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {37},
  number = {8},
  pages = {1835--1846},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2018.2804799},
  urldate = {2021-08-24},
  file = {/Users/marinbenc/Zotero/storage/2IG73BTZ/Commandeur et al. - 2018 - Deep Learning for Quantification of Epicardial and.pdf}
}

@article{Coppini2010,
  title = {Quantification of Epicardial Fat by Cardiac {{CT}} Imaging},
  author = {Coppini, Giuseppe},
  year = {2010},
  month = jul,
  journal = {The Open Medical Informatics Journal},
  volume = {4},
  number = {1},
  pages = {126--135},
  publisher = {{Bentham Science Publishers Ltd.}},
  doi = {10.2174/1874431101004010126}
}

@article{danduStorageMediaComputers2008,
  title = {Storage Media for Computers in Radiology},
  author = {Dandu, Ravi Varma},
  year = {2008},
  month = nov,
  journal = {The Indian Journal of Radiology \& Imaging},
  volume = {18},
  number = {4},
  pages = {287--289},
  issn = {0971-3026},
  doi = {10.4103/0971-3026.43838},
  abstract = {The introduction and wide acceptance of digital technology in medical imaging has resulted in an exponential increase in the amount of data produced by the radiology department. There is an insatiable need for storage space to archive this ever-growing volume of image data. Healthcare facilities should plan the type and size of the storage media that they needed, based not just on the volume of data but also on considerations such as the speed and ease of access, redundancy, security, costs, as well as the longevity of the archival technology. This article reviews the various digital storage media and compares their merits and demerits.},
  langid = {english},
  pmcid = {PMC2747448},
  pmid = {19774182},
  keywords = {computers,Storage media},
  file = {/Users/marinbenc/Zotero/storage/QU9GG9WH/Dandu - 2008 - Storage media for computers in radiology.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009a,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {{IEEE}},
  address = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2023-12-01},
  isbn = {978-1-4244-3992-8}
}

@misc{dermisDermIS,
  title = {{{DermIS}} {\textemdash} Dermis.Net, {{Dept}}. of {{Clinical Social Medicine}} ({{Univ}}. of {{Heidelberg}}) and the {{Dept}}. of {{Dermatology}} ({{Univ}}. of {{Erlangen}})},
  year = {2012},
  urldate = {2024-01-23},
  howpublished = {https://www.dermis.net/}
}

@article{devunooruDeepLearningNeural2021,
  title = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis: A Recent Review and Taxonomy},
  shorttitle = {Deep Learning Neural Networks for Medical Image Segmentation of Brain Tumours for Diagnosis},
  author = {Devunooru, Sindhu and Alsadoon, Abeer and Chandana, P. W. C. and Beg, Azam},
  year = {2021},
  month = jan,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {12},
  number = {1},
  pages = {455--483},
  issn = {1868-5137, 1868-5145},
  doi = {10.1007/s12652-020-01998-w},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{Ding2014,
  title = {Automated Epicardial Fat Volume Quantification from Non-Contrast {{CT}}},
  booktitle = {Medical Imaging 2014: {{Image}} Processing},
  author = {Ding, Xiaowei and Terzopoulos, Demetri and {Diaz-Zamudio}, Mariana and Berman, Daniel S. and Slomka, Piotr J. and Dey, Damini},
  editor = {Ourselin, Sebastien and Styner, Martin A.},
  year = {2014},
  month = mar,
  publisher = {{SPIE}},
  doi = {10.1117/12.2043326}
}

@article{dosovitskiy2020vit,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  journal = {ICLR}
}

@inproceedings{dosovitskiy2021an,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  booktitle = {International Conference on Learning Representations},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021}
}

@misc{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  number = {arXiv:1603.07285},
  eprint = {1603.07285},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-19},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/H4LV3GKY/Dumoulin_Visin_2018_A guide to convolution arithmetic for deep learning.pdf;/Users/marinbenc/Zotero/storage/E2J26Z3H/1603.html}
}

@article{edlundLIVECellLargescaleDataset2021,
  title = {{{LIVECell}}{\textemdash}{{A}} Large-Scale Dataset for Label-Free Live Cell Segmentation},
  author = {Edlund, Christoffer and Jackson, Timothy R. and Khalid, Nabeel and Bevan, Nicola and Dale, Timothy and Dengel, Andreas and Ahmed, Sheraz and Trygg, Johan and Sj{\"o}gren, Rickard},
  year = {2021},
  month = sep,
  journal = {Nature Methods},
  volume = {18},
  number = {9},
  pages = {1038--1045},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01249-6},
  urldate = {2023-09-28},
  abstract = {Abstract             Light microscopy combined with well-established protocols of two-dimensional cell culture facilitates high-throughput quantitative imaging to study biological phenomena. Accurate segmentation of individual cells in images enables exploration of complex biological questions, but can require sophisticated imaging processing pipelines in cases of low contrast and high object density. Deep learning-based methods are considered state-of-the-art for image segmentation but typically require vast amounts of annotated data, for which there is no suitable resource available in the field of label-free cellular imaging. Here, we present LIVECell, a large, high-quality, manually annotated and expert-validated dataset of phase-contrast images, consisting of over 1.6 million cells from a diverse set of cell morphologies and culture densities. To further demonstrate its use, we train convolutional neural network-based models using LIVECell and evaluate model segmentation accuracy with a proposed a suite of benchmarks.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/TUQV6DIR/Edlund et al_2021_LIVECell—A large-scale dataset for label-free live cell segmentation.pdf}
}

@article{estevesPolarTransformerNetworks2018,
  title = {Polar {{Transformer Networks}}},
  author = {Esteves, Carlos and {Allen-Blanchette}, Christine and Zhou, Xiaowei and Daniilidis, Kostas},
  year = {2018},
  month = feb,
  journal = {arXiv:1709.01889 [cs]},
  eprint = {1709.01889},
  primaryclass = {cs},
  urldate = {2022-05-11},
  abstract = {Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/CRVBT5TW/Esteves et al. - 2018 - Polar Transformer Networks.pdf;/Users/marinbenc/Zotero/storage/BCP3Q5RH/1709.html}
}

@article{estevesPolarTransformerNetworks2018a,
  title = {Polar {{Transformer Networks}}},
  author = {Esteves, Carlos and {Allen-Blanchette}, Christine and Zhou, Xiaowei and Daniilidis, Kostas},
  year = {2018},
  month = feb,
  journal = {arXiv:1709.01889 [cs]},
  eprint = {1709.01889},
  primaryclass = {cs},
  urldate = {2021-04-01},
  abstract = {Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/X7Q3L4HS/Esteves et al. - 2018 - Polar Transformer Networks.pdf;/Users/marinbenc/Zotero/storage/66HS8UMH/1709.html}
}

@incollection{fangSelectiveFeatureAggregation2019,
  title = {Selective {{Feature Aggregation Network}} with {{Area-Boundary Constraints}} for {{Polyp Segmentation}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} {\textendash} {{MICCAI}} 2019},
  author = {Fang, Yuqi and Chen, Cheng and Yuan, Yixuan and Tong, Kai-yu},
  editor = {Shen, Dinggang and Liu, Tianming and Peters, Terry M. and Staib, Lawrence H. and Essert, Caroline and Zhou, Sean and Yap, Pew-Thian and Khan, Ali},
  year = {2019},
  volume = {11764},
  pages = {302--310},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32239-7_34},
  urldate = {2021-04-19},
  isbn = {978-3-030-32238-0 978-3-030-32239-7},
  langid = {english}
}

@incollection{fanPraNetParallelReverse2020,
  title = {{{PraNet}}: {{Parallel Reverse Attention Network}} for {{Polyp Segmentation}}},
  shorttitle = {{{PraNet}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} {\textendash} {{MICCAI}} 2020},
  author = {Fan, Deng-Ping and Ji, Ge-Peng and Zhou, Tao and Chen, Geng and Fu, Huazhu and Shen, Jianbing and Shao, Ling},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  volume = {12266},
  pages = {263--273},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-59725-2_26},
  urldate = {2021-08-23},
  isbn = {978-3-030-59724-5 978-3-030-59725-2},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BWSUWGXP/Fan et al. - 2020 - PraNet Parallel Reverse Attention Network for Pol.pdf}
}

@article{fantazzini3DAutomaticSegmentation2020,
  title = {{{3D Automatic Segmentation}} of {{Aortic Computed Tomography Angiography Combining Multi-View 2D Convolutional Neural Networks}}},
  author = {Fantazzini, Alice and Esposito, Mario and Finotello, Alice and Auricchio, Ferdinando and Pane, Bianca and Basso, Curzio and Spinella, Giovanni and Conti, Michele},
  year = {2020},
  month = oct,
  journal = {Cardiovascular Engineering and Technology},
  volume = {11},
  number = {5},
  pages = {576--586},
  issn = {1869-408X, 1869-4098},
  doi = {10.1007/s13239-020-00481-z},
  urldate = {2022-05-05},
  abstract = {Abstract                            Purpose               The quantitative analysis of contrast-enhanced Computed Tomography Angiography (CTA) is essential to assess aortic anatomy, identify pathologies, and perform preoperative planning in vascular surgery. To overcome the limitations given by manual and semi-automatic segmentation tools, we apply a deep learning-based pipeline to automatically segment the CTA scans of the aortic lumen, from the ascending aorta to the iliac arteries, accounting for 3D spatial coherence.                                         Methods               A first convolutional neural network (CNN) is used to coarsely segment and locate the aorta in the whole sub-sampled CTA volume, then three single-view CNNs are used to effectively segment the aortic lumen from axial, sagittal, and coronal planes under higher resolution. Finally, the predictions of the three orthogonal networks are integrated to obtain a segmentation with spatial coherence.                                         Results               The coarse segmentation performed to identify the aortic lumen achieved a Dice coefficient (DSC) of 0.92\,{$\pm$}\,0.01. Single-view axial, sagittal, and coronal CNNs provided a DSC of 0.92\,{$\pm$}\,0.02, 0.92\,{$\pm$}\,0.04, and 0.91\,{$\pm$}\,0.02, respectively. Multi-view integration provided a DSC of 0.93\,{$\pm$}\,0.02 and an average surface distance of 0.80\,{$\pm$}\,0.26~mm on a test set of 10 CTA scans. The generation of the ground truth dataset took about 150 h and the overall training process took 18 h. In prediction phase, the adopted pipeline takes around 25\,{$\pm$}\,1 s to get the final segmentation.                                         Conclusion               The achieved results show that the proposed pipeline can effectively localize and segment the aortic lumen in subjects with aneurysm.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/7YSZ3M2Y/Fantazzini et al. - 2020 - 3D Automatic Segmentation of Aortic Computed Tomog.pdf}
}

@inproceedings{fasterRCNN,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {91--99},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\textsubscript{r}cnn.}
}

@inproceedings{forman05,
  title = {Counting Positives Accurately despite Inaccurate Classification},
  booktitle = {Machine Learning: {{ECML}} 2005},
  author = {Forman, George},
  editor = {Gama, Jo{\~a}o and Camacho, Rui and Brazdil, Pavel B. and Jorge, Al{\'i}pio M{\'a}rio and Torgo, Lu{\'i}s},
  year = {2005},
  pages = {564--575},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Most supervised machine learning research assumes the training set is a random sample from the target population, thus the class distribution is invariant. In real world situations, however, the class distribution changes, and is known to erode the effectiveness of classifiers and calibrated probability estimators. This paper focuses on the problem of accurately estimating the number of positives in the test set{\textemdash}quantification{\textemdash}as opposed to classifying individual cases accuratel y. It compares three methods: classify \& count, an adjusted variant, and a mixture model. An empirical evaluation on a text classification benchmark reveals that the simple method is consistently biased, and that the mixture model is surprisingly effective even when positives are very scarce in the training set{\textemdash}a common case in information retrieval.},
  isbn = {978-3-540-31692-3}
}

@book{fosbinder2011essentials,
  title = {Essentials of Radiologic Science},
  author = {Fosbinder, R. and Orth, D.},
  year = {2011},
  publisher = {{Wolters Kluwer Health/Lippincott Williams \& Wilkins}},
  isbn = {978-0-7817-7554-0},
  lccn = {2010030972}
}

@inproceedings{ganinDA2015,
  title = {Unsupervised Domain Adaptation by Backpropagation},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  year = {2015},
  series = {{{ICML}}'15},
  pages = {1180--1189},
  publisher = {{JMLR.org}},
  address = {{Lille, France}},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard back propagation.Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

@inproceedings{girshickRichFeatureHierarchies2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = jun,
  pages = {580--587},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.81},
  urldate = {2022-10-11},
  isbn = {978-1-4799-5118-5},
  file = {/Users/marinbenc/Zotero/storage/HDEVS8XL/Girshick et al. - 2014 - Rich Feature Hierarchies for Accurate Object Detec.pdf}
}

@article{Goeller2018,
  title = {Epicardial Adipose Tissue Density and Volume Are Related to Subclinical Atherosclerosis, Inflammation and Major Adverse Cardiac Events in Asymptomatic Subjects},
  author = {Goeller, Markus and Achenbach, Stephan and Marwan, Mohamed and Doris, Mhairi K. and Cadet, Sebastien and Commandeur, Frederic and Chen, Xi and Slomka, Piotr J. and Gransar, Heidi and Cao, J. Jane and Wong, Nathan D. and Albrecht, Moritz H. and Rozanski, Alan and Tamarappoo, Balaji K. and Berman, Daniel S. and Dey, Damini},
  year = {2018},
  month = jan,
  journal = {Journal of Cardiovascular Computed Tomography},
  volume = {12},
  number = {1},
  pages = {67--73},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jcct.2017.11.007}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016},
  keywords = {Machine learning}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-12-01},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/D9ICM9PZ/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/Users/marinbenc/Zotero/storage/DLDPDG3R/1406.html}
}

@article{Gorter2008,
  title = {Relation of Epicardial and Pericoronary Fat to Coronary Atherosclerosis and Coronary Artery Calcium in Patients Undergoing Coronary Angiography},
  author = {Gorter, Petra M. and {de Vos}, Alexander M. and {van der Graaf}, Yolanda and Stella, Pieter R. and Doevendans, Pieter A. and Meijs, Matthijs F.L. and Prokop, Mathias and Visseren, Frank L.J.},
  year = {2008},
  month = aug,
  journal = {The American Journal of Cardiology},
  volume = {102},
  number = {4},
  pages = {380--385},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.amjcard.2008.04.002}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  urldate = {2023-10-21},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  file = {/Users/marinbenc/Zotero/storage/MENBPI9N/Hastie et al_2009_The Elements of Statistical Learning.pdf}
}

@misc{he2015deep,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  eprint = {1512.03385},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@article{he2019moco,
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2019},
  journal = {arXiv preprint arXiv:1911.05722},
  eprint = {1911.05722},
  archiveprefix = {arxiv}
}

@inproceedings{he2020,
  title = {Automatic Epicardial Fat Segmentation in Cardiac {{CT}} Imaging Using {{3D}} Deep Attention {{U-Net}}},
  booktitle = {Medical Imaging 2020: {{Image}} Processing},
  author = {He, Xiuxiu and Guo, Bang Jun and Lei, Yang and Wang, Tonghe and Liu, Tian and Curran, Walter J. and Zhang, Long Jiang and Yang, Xiaofeng},
  editor = {I{\v s}gum, Ivana and Landman, Bennett A.},
  year = {2020},
  volume = {11313},
  pages = {589--595},
  publisher = {{SPIE / International Society for Optics and Photonics}},
  doi = {10.1117/12.2550383}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2021-08-23},
  isbn = {978-1-4673-8851-1},
  file = {/Users/marinbenc/Zotero/storage/4QM6QX46/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heMaskRCNN2017,
  title = {Mask {{R-CNN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2017},
  month = oct,
  pages = {2980--2988},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.322},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
  keywords = {Feature extraction,Image segmentation,Object detection,Quantization (signal),Robustness,Semantics},
  file = {/Users/marinbenc/Zotero/storage/9WHZCWBV/8237584.html}
}

@inproceedings{heMaskRCNN2017b,
  title = {Mask {{R-CNN}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = {2017},
  month = oct,
  pages = {2980--2988},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.322},
  urldate = {2023-10-09},
  isbn = {978-1-5386-1032-9}
}

@article{hiraharaEffectPixelInterpolation2021,
  title = {Effect of the {{Pixel Interpolation Method}} for {{Downsampling Medical Images}} on {{Deep Learning Accuracy}}},
  author = {Hirahara, Daisuke and Takaya, Eichi and Kadowaki, Mizuki and Kobayashi, Yasuyuki and Ueda, Takuya},
  year = {2021},
  month = nov,
  journal = {Journal of Computer and Communications},
  volume = {9},
  number = {11},
  pages = {150--156},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/jcc.2021.911010},
  urldate = {2022-09-16},
  abstract = {Background: High-resolution medical images often need to be downsampled because of the memory limitations of the hardware used for machine learning. Although various image interpolation methods are applicable to downsampling, the effect of data preprocessing on the learning performance of convolutional neural networks (CNNs) has not been fully investigated. Methods: In this study, five different pixel interpolation algorithms (nearest neighbor, bilinear, Hamming window, bicubic, and Lanczos interpolation) were used for image downsampling to investigate their effects on the prediction accuracy of a CNN. Chest X-ray images from the NIH public dataset were examined by downsampling 10 patterns. Results: The accuracy improved with a decreasing image size, and the best accuracy was achieved at 64 {\texttimes} 64 pixels. Among the interpolation methods, bicubic interpolation obtained the highest accuracy, followed by the Hamming window.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/MK5M5MM2/Hirahara et al. - 2021 - Effect of the Pixel Interpolation Method for Downs.pdf;/Users/marinbenc/Zotero/storage/Y7WQJARN/paperinformation.html}
}

@article{ho2020denoising,
  title = {Denoising Diffusion Probabilistic Models},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  journal = {arXiv preprint arxiv:2006.11239},
  eprint = {2006.11239},
  archiveprefix = {arxiv}
}

@inproceedings{houPatchBasedConvolutionalNeural2016,
  title = {Patch-{{Based Convolutional Neural Network}} for {{Whole Slide Tissue Image Classification}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hou, Le and Samaras, Dimitris and Kurc, Tahsin M. and Gao, Yi and Davis, James E. and Saltz, Joel H.},
  year = {2016},
  month = jun,
  pages = {2424--2433},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.266},
  abstract = {Convolutional Neural Networks (CNN) are state-of-theart models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.},
  keywords = {Cancer,Image resolution,Neural networks,Predictive models,Robustness,Training,Visualization},
  file = {/Users/marinbenc/Zotero/storage/HG3NQ9Y2/Hou et al. - 2016 - Patch-Based Convolutional Neural Network for Whole.pdf;/Users/marinbenc/Zotero/storage/5WH599LC/7780635.html}
}

@misc{https://doi.org/10.48550/arxiv.2205.09723,
  title = {Robust and Efficient Medical Imaging with Self-Supervision},
  author = {Azizi, Shekoofeh and Culp, Laura and Freyberg, Jan and Mustafa, Basil and Baur, Sebastien and Kornblith, Simon and Chen, Ting and MacWilliams, Patricia and Mahdavi, S. Sara and Wulczyn, Ellery and Babenko, Boris and Wilson, Megan and Loh, Aaron and Chen, Po-Hsuan Cameron and Liu, Yuan and Bavishi, Pinal and McKinney, Scott Mayer and Winkens, Jim and Roy, Abhijit Guha and Beaver, Zach and Ryan, Fiona and Krogue, Justin and Etemadi, Mozziyar and Telang, Umesh and Liu, Yun and Peng, Lily and Corrado, Greg S. and Webster, Dale R. and Fleet, David and Hinton, Geoffrey and Houlsby, Neil and Karthikesalingam, Alan and Norouzi, Mohammad and Natarajan, Vivek},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2205.09723},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{huangHarDNetMSEGSimpleEncoderDecoder2021,
  title = {{{HarDNet-MSEG}}: {{A Simple Encoder-Decoder Polyp Segmentation Neural Network}} That {{Achieves}} over 0.9 {{Mean Dice}} and 86 {{FPS}}},
  shorttitle = {{{HarDNet-MSEG}}},
  author = {Huang, Chien-Hsiang and Wu, Hung-Yu and Lin, Youn-Long},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.07172 [cs]},
  eprint = {2101.07172},
  primaryclass = {cs},
  urldate = {2021-04-19},
  abstract = {We propose a new convolution neural network called HarDNet-MSEG for polyp segmentation. It achieves SOTA in both accuracy and inference speed on five popular datasets. For Kvasir-SEG, HarDNet-MSEG delivers 0.904 mean Dice running at 86.7 FPS on a GeForce RTX 2080 Ti GPU. It consists of a backbone and a decoder. The backbone is a low memory traffic CNN called HarDNet68, which has been successfully applied to various CV tasks including image classification, object detection, multi-object tracking and semantic segmentation, etc. The decoder part is inspired by the Cascaded Partial Decoder, known for fast and accurate salient object detection. We have evaluated HarDNet-MSEG using those five popular datasets. The code and all experiment details are available at Github. https://github.com/james128333/HarDNet-MSEG},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/YRDJXWHI/Huang et al. - 2021 - HarDNet-MSEG A Simple Encoder-Decoder Polyp Segme.pdf;/Users/marinbenc/Zotero/storage/GUI4TD7L/2101.html}
}

@inproceedings{huSqueezeandExcitationNetworks2018,
  title = {Squeeze-and-{{Excitation Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hu, Jie and Shen, Li and Sun, Gang},
  year = {2018},
  month = jun,
  pages = {7132--7141},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00745},
  urldate = {2021-08-23},
  isbn = {978-1-5386-6420-9},
  file = {/Users/marinbenc/Zotero/storage/3NCD5N6N/Hu et al. - 2018 - Squeeze-and-Excitation Networks.pdf}
}

@article{iacobellis15,
  title = {Local and Systemic Effects of the Multifaceted Epicardial Adipose Tissue Depot},
  author = {Iacobellis, Gianluca},
  year = {2015},
  month = apr,
  journal = {Nature Reviews Endocrinology},
  volume = {11},
  number = {6},
  pages = {363--371},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/nrendo.2015.58}
}

@article{Iacobellis2009-2,
  title = {Epicardial and Pericardial Fat: {{Close}}, but Very Different},
  author = {Iacobellis, Gianluca},
  year = {2009},
  month = apr,
  journal = {Obesity},
  volume = {17},
  number = {4},
  pages = {625--625},
  publisher = {{Wiley}},
  doi = {10.1038/oby.2008.575}
}

@article{Iacobellis2011,
  title = {Epicardial Fat Thickness and Coronary Artery Disease Correlate Independently of Obesity},
  author = {Iacobellis, Gianluca and Lonn, Eva and Lamy, Andre and Singh, Navneet and Sharma, Arya M.},
  year = {2011},
  month = feb,
  journal = {International Journal of Cardiology},
  volume = {146},
  number = {3},
  pages = {452--454},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.ijcard.2010.10.117}
}

@article{ibtehazMultiResUNetRethinkingUNet2020,
  title = {{{MultiResUNet}} : {{Rethinking}} the {{U-Net}} Architecture for Multimodal Biomedical Image Segmentation},
  shorttitle = {{{MultiResUNet}}},
  author = {Ibtehaz, Nabil and Rahman, M. Sohel},
  year = {2020},
  month = jan,
  journal = {Neural Networks},
  volume = {121},
  pages = {74--87},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.08.025},
  urldate = {2021-08-23},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/SVH4JXCR/Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net architecture f.pdf}
}

@article{ILSVRC15,
  title = {{{ImageNet}} Large Scale Visual Recognition Challenge},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2015},
  journal = {International Journal of Computer Vision (IJCV)},
  volume = {115},
  number = {3},
  pages = {211--252},
  doi = {10.1007/s11263-015-0816-y}
}

@article{info11020125,
  title = {Albumentations: {{Fast}} and Flexible Image Augmentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  journal = {Information-an International Interdisciplinary Journal},
  volume = {11},
  number = {2},
  issn = {2078-2489},
  doi = {10.3390/info11020125},
  article-number = {125}
}

@article{info11020125,
  title = {Albumentations: {{Fast}} and Flexible Image Augmentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  journal = {Information-an International Interdisciplinary Journal},
  volume = {11},
  number = {2},
  issn = {2078-2489},
  doi = {10.3390/info11020125},
  article-number = {125}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {1901.07031},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-10-02},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/ELVXRVGZ/Irvin et al_2019_CheXpert.pdf;/Users/marinbenc/Zotero/storage/PRSVMGSL/1901.html}
}

@article{isenseeNnUNetSelfconfiguringMethod2021,
  title = {{{nnU-Net}}: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation},
  shorttitle = {{{nnU-Net}}},
  author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and {Maier-Hein}, Klaus H.},
  year = {2021},
  month = feb,
  journal = {Nature Methods},
  volume = {18},
  number = {2},
  pages = {203--211},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-020-01008-z},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/H5L3AVAQ/Isensee et al_2021_nnU-Net.pdf}
}

@article{jaderbergSpatialTransformerNetworks2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  journal = {arXiv:1506.02025 [cs]},
  eprint = {1506.02025},
  primaryclass = {cs},
  urldate = {2021-04-26},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/T69B79T5/Jaderberg et al. - 2016 - Spatial Transformer Networks.pdf;/Users/marinbenc/Zotero/storage/63BFW68K/1506.html}
}

@inproceedings{jha2020kvasir,
  title = {Kvasir-Seg: {{A}} Segmented Polyp Dataset},
  booktitle = {International Conference on Multimedia Modeling},
  author = {Jha, Debesh and Smedsrud, Pia H and Riegler, Michael A and Halvorsen, P{\aa}l and {de Lange}, Thomas and Johansen, Dag and Johansen, H{\aa}vard D},
  year = {2020},
  pages = {451--462},
  organization = {{Springer}}
}

@inproceedings{jhaDoubleUNetDeepConvolutional2020,
  title = {{{DoubleU-Net}}: {{A Deep Convolutional Neural Network}} for {{Medical Image Segmentation}}},
  shorttitle = {{{DoubleU-Net}}},
  booktitle = {2020 {{IEEE}} 33rd {{International Symposium}} on {{Computer-Based Medical Systems}} ({{CBMS}})},
  author = {Jha, Debesh and Riegler, Michael A. and Johansen, Dag and Halvorsen, Pal and Johansen, Havard D.},
  year = {2020},
  month = jul,
  pages = {558--564},
  publisher = {{IEEE}},
  address = {{Rochester, MN, USA}},
  doi = {10.1109/CBMS49503.2020.00111},
  urldate = {2021-08-23},
  isbn = {978-1-72819-429-5},
  file = {/Users/marinbenc/Zotero/storage/3JKSV2PH/Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf}
}

@article{jhaInstanceSegmentationWhole2021,
  title = {Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment},
  shorttitle = {Instance Segmentation for Whole Slide Imaging},
  author = {Jha, Aadarsh and Yang, Haichun and Deng, Ruining and Kapp, Meghan E. and Fogo, Agnes B. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {Journal of Medical Imaging},
  volume = {8},
  number = {1},
  pages = {014001},
  issn = {2329-4302},
  doi = {10.1117/1.JMI.8.1.014001},
  urldate = {2022-09-19},
  abstract = {Purpose: Automatic instance segmentation of glomeruli within kidney whole slide imaging (WSI) is essential for clinical research in renal pathology. In computer vision, the end-to-end instance segmentation methods (e.g., Mask-RCNN) have shown their advantages relative to detect-then-segment approaches by performing complementary detection and segmentation tasks simultaneously. As a result, the end-to-end Mask-RCNN approach has been the de facto standard method in recent glomerular segmentation studies, where downsampling and patch-based techniques are used to properly evaluate the high-resolution images from WSI (e.g., {$>$}10,000{\texttimes}10,000\,\,pixels on 40{\texttimes}). However, in high-resolution WSI, a single glomerulus itself can be more than 1000{\texttimes}1000\,\,pixels in original resolution which yields significant information loss when the corresponding features maps are downsampled to the 28{\texttimes}28 resolution via the end-to-end Mask-RCNN pipeline., Approach: We assess if the end-to-end instance segmentation framework is optimal for high-resolution WSI objects by comparing Mask-RCNN with our proposed detect-then-segment framework. Beyond such a comparison, we also comprehensively evaluate the performance of our detect-then-segment pipeline through: (1)~two of the most prevalent segmentation backbones (U-Net and DeepLab\_v3); (2)~six different image resolutions (512{\texttimes}512, 256{\texttimes}256, 128{\texttimes}128, 64{\texttimes}64, 32{\texttimes}32, and 28{\texttimes}28); and (3)~two different color spaces (RGB and LAB)., Results: Our detect-then-segment pipeline, with the DeepLab\_v3 segmentation framework operating on previously detected glomeruli of 512{\texttimes}512 resolution, achieved a 0.953 Dice similarity coefficient (DSC), compared with a 0.902 DSC from the end-to-end Mask-RCNN pipeline. Further, we found that neither RGB nor LAB color spaces yield better performance when compared against each other in the context of a detect-then-segment framework., Conclusions: The detect-then-segment pipeline achieved better segmentation performance compared with the end-to-end method. Our study provides an extensive quantitative reference for other researchers to select the optimized and most accurate segmentation approach for glomeruli, or other biological objects of similar character, on high-resolution WSI.},
  pmcid = {PMC7790159},
  pmid = {33426152},
  file = {/Users/marinbenc/Zotero/storage/5XADS7A7/Jha et al. - 2021 - Instance segmentation for whole slide imaging end.pdf}
}

@article{jhaInstanceSegmentationWhole2021a,
  title = {Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment},
  shorttitle = {Instance Segmentation for Whole Slide Imaging},
  author = {Jha, Aadarsh and Yang, Haichun and Deng, Ruining and Kapp, Meghan E. and Fogo, Agnes B. and Huo, Yuankai},
  year = {2021},
  month = jan,
  journal = {Journal of Medical Imaging},
  volume = {8},
  number = {01},
  issn = {2329-4302},
  doi = {10.1117/1.JMI.8.1.014001},
  urldate = {2023-10-02},
  file = {/Users/marinbenc/Zotero/storage/33U6NMXI/Jha et al_2021_Instance segmentation for whole slide imaging.pdf}
}

@inproceedings{jhaResUNetAdvancedArchitecture2019,
  title = {{{ResUNet}}++: {{An Advanced Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{ResUNet}}++},
  booktitle = {2019 {{IEEE International Symposium}} on {{Multimedia}} ({{ISM}})},
  author = {Jha, Debesh and Smedsrud, Pia H. and Riegler, Michael A. and Johansen, Dag and Lange, Thomas De and Halvorsen, Pal and D. Johansen, Havard},
  year = {2019},
  month = dec,
  pages = {225--2255},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/ISM46123.2019.00049},
  urldate = {2022-10-25},
  isbn = {978-1-72815-606-4},
  file = {/Users/marinbenc/Zotero/storage/62KPNB2S/Jha et al. - 2019 - ResUNet++ An Advanced Architecture for Medical Im.pdf}
}

@inproceedings{jin2022learning,
  title = {Learning to Downsample for Segmentation of Ultra-High Resolution Images},
  booktitle = {International Conference on Learning Representations},
  author = {Jin, Chen and Tanno, Ryutaro and Mertzanidou, Thomy and Panagiotaki, Eleftheria and Alexander, Daniel C.},
  year = {2022}
}

@article{jinAIbasedAorticVessel2021,
  title = {{{AI-based Aortic Vessel Tree Segmentation}} for {{Cardiovascular Diseases Treatment}}: {{Status Quo}}},
  shorttitle = {{{AI-based Aortic Vessel Tree Segmentation}} for {{Cardiovascular Diseases Treatment}}},
  author = {Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Kleesiek, Jens and Frangi, Alejandro F. and Egger, Jan},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.02998 [physics]},
  eprint = {2108.02998},
  primaryclass = {physics},
  urldate = {2022-05-04},
  abstract = {The aortic vessel tree is composed of the aorta and its branching arteries, and plays a key role in supplying the whole body with blood. Aortic diseases, like aneurysms or dissections, can lead to an aortic rupture, whose treatment with open surgery is highly risky. Therefore, patients commonly undergo drug treatment under constant monitoring, which requires regular inspections of the vessels through imaging. The standard imaging modality for diagnosis and monitoring is computed tomography (CT), which can provide a detailed picture of the aorta and its branching vessels if combined with a contrast agent, resulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree geometry from consecutive CTAs, are overlaid and compared. This allows to not only detect changes in the aorta, but also more peripheral vessel tree changes, caused by the primary pathology or newly developed. When performed manually, this reconstruction requires slice by slice contouring, which could easily take a whole day for a single aortic vessel tree and, hence, is not feasible in clinical practice. Automatic or semi-automatic vessel tree segmentation algorithms, on the other hand, can complete this task in a fraction of the manual execution time and run in parallel to the clinical routine of the clinicians. In this paper, we systematically review computing techniques for the automatic and semi-automatic segmentation of the aortic vessel tree. The review concludes with an in-depth discussion on how close these state-of-the-art approaches are to an application in clinical practice and how active this research field is, taking into account the number of publications, datasets and challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Physics - Medical Physics},
  file = {/Users/marinbenc/Zotero/storage/FT9MX2J3/Jin et al. - 2021 - AI-based Aortic Vessel Tree Segmentation for Cardi.pdf;/Users/marinbenc/Zotero/storage/SNBK5TL5/2108.html}
}

@incollection{kamalianComputedTomographyImaging2016,
  title = {Computed Tomography Imaging and Angiography {\textendash} Principles},
  booktitle = {Handbook of {{Clinical Neurology}}},
  author = {Kamalian, Shervin and Lev, Michael H. and Gupta, Rajiv},
  year = {2016},
  volume = {135},
  pages = {3--20},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53485-9.00001-5},
  urldate = {2023-09-29},
  isbn = {978-0-444-53485-9},
  langid = {english}
}

@article{Kass1988,
  title = {Snakes: {{Active}} Contour Models},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf00133570}
}

@article{khaderDenoisingDiffusionProbabilistic2023,
  title = {Denoising Diffusion Probabilistic Models for {{3D}} Medical Image Generation},
  author = {Khader, Firas and {M{\"u}ller-Franzes}, Gustav and Tayebi Arasteh, Soroosh and Han, Tianyu and Haarburger, Christoph and {Schulze-Hagen}, Maximilian and Schad, Philipp and Engelhardt, Sandy and Bae{\ss}ler, Bettina and Foersch, Sebastian and Stegmaier, Johannes and Kuhl, Christiane and Nebelung, Sven and Kather, Jakob Nikolas and Truhn, Daniel},
  year = {2023},
  month = may,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {7303},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-34341-2},
  urldate = {2023-12-01},
  abstract = {Abstract             Recent advances in computer vision have shown promising results in image generation. Diffusion probabilistic models have generated realistic images from textual input, as demonstrated by DALL-E 2, Imagen, and Stable Diffusion. However, their use in medicine, where imaging data typically comprises three-dimensional volumes, has not been systematically evaluated. Synthetic images may play a crucial role in privacy-preserving artificial intelligence and can also be used to augment small datasets. We show that diffusion probabilistic models can synthesize high-quality medical data for magnetic resonance imaging (MRI) and computed tomography (CT). For quantitative evaluation, two radiologists rated the quality of the synthesized images regarding "realistic image appearance", "anatomical correctness", and "consistency between slices". Furthermore, we demonstrate that synthetic images can be used in self-supervised pre-training and improve the performance of breast segmentation models when data is scarce (Dice scores, 0.91 [without synthetic data], 0.95 [with synthetic data]).},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/2LD9NW58/Khader et al_2023_Denoising diffusion probabilistic models for 3D medical image generation.pdf}
}

@article{khanAutoCellSegRobustAutomatic2018,
  title = {{{AutoCellSeg}}: Robust Automatic Colony Forming Unit ({{CFU}})/Cell Analysis Using Adaptive Image Segmentation and Easy-to-Use Post-Editing Techniques},
  shorttitle = {{{AutoCellSeg}}},
  author = {Khan, Arif Ul Maula and Torelli, Angelo and Wolf, Ivo and Gretz, Norbert},
  year = {2018},
  month = may,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {7302},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24916-9},
  urldate = {2023-10-16},
  abstract = {Abstract             In biological assays, automated cell/colony segmentation and counting is imperative owing to huge image sets. Problems occurring due to drifting image acquisition conditions, background noise and high variation in colony features in experiments demand a user-friendly, adaptive and robust image processing/analysis method. We present AutoCellSeg (based on MATLAB) that implements a supervised automatic and robust image segmentation method. AutoCellSeg utilizes multi-thresholding aided by a feedback-based watershed algorithm taking segmentation plausibility criteria into account. It is usable in different operation modes and intuitively enables the user to select object features interactively for supervised image segmentation method. It allows the user to correct results with a graphical interface. This publicly available tool outperforms tools like OpenCFU and CellProfiler in terms of accuracy and provides many additional useful features for end-users.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/3UFPAMEA/Khan et al_2018_AutoCellSeg.pdf}
}

@article{kimCNNBasedUGS2018,
  title = {{{CNN}}-based {{UGS}} Method Using {{Cartesian}}-to-polar Coordinate Transformation},
  author = {Kim, B.-S. and Sun, J.-Y. and Kim, S.-W. and Kang, M.-C. and Ko, S.-J.},
  year = {2018},
  month = nov,
  journal = {Electronics Letters},
  volume = {54},
  number = {23},
  pages = {1321--1322},
  issn = {0013-5194, 1350-911X},
  doi = {10.1049/el.2018.5051},
  urldate = {2021-04-01},
  langid = {english}
}

@article{kimCyCNNRotationInvariant2020,
  title = {{{CyCNN}}: {{A Rotation Invariant CNN}} Using {{Polar Mapping}} and {{Cylindrical Convolution Layers}}},
  shorttitle = {{{CyCNN}}},
  author = {Kim, Jinpyo and Jung, Wooekun and Kim, Hyungmo and Lee, Jaejin},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.10588 [cs, eess]},
  eprint = {2007.10588},
  primaryclass = {cs, eess},
  urldate = {2022-05-10},
  abstract = {Deep Convolutional Neural Networks (CNNs) are empirically known to be invariant to moderate translation but not to rotation in image classification. This paper proposes a deep CNN model, called CyCNN, which exploits polar mapping of input images to convert rotation to translation. To deal with the cylindrical property of the polar coordinates, we replace convolution layers in conventional CNNs to cylindrical convolutional (CyConv) layers. A CyConv layer exploits the cylindrically sliding windows (CSW) mechanism that vertically extends the input-image receptive fields of boundary units in a convolutional layer. We evaluate CyCNN and conventional CNN models for classification tasks on rotated MNIST, CIFAR-10, and SVHN datasets. We show that if there is no data augmentation during training, CyCNN significantly improves classification accuracies when compared to conventional CNN models. Our implementation of CyCNN is publicly available on https://github.com/mcrl/CyCNN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/2CDN5SKY/Kim et al. - 2020 - CyCNN A Rotation Invariant CNN using Polar Mappin.pdf;/Users/marinbenc/Zotero/storage/5YC9ERD9/2007.html}
}

@article{kimCyCNNRotationInvariant2020a,
  title = {{{CyCNN}}: {{A Rotation Invariant CNN}} Using {{Polar Mapping}} and {{Cylindrical Convolution Layers}}},
  shorttitle = {{{CyCNN}}},
  author = {Kim, Jinpyo and Jung, Wooekun and Kim, Hyungmo and Lee, Jaejin},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.10588 [cs, eess]},
  eprint = {2007.10588},
  primaryclass = {cs, eess},
  urldate = {2021-04-08},
  abstract = {Deep Convolutional Neural Networks (CNNs) are empirically known to be invariant to moderate translation but not to rotation in image classification. This paper proposes a deep CNN model, called CyCNN, which exploits polar mapping of input images to convert rotation to translation. To deal with the cylindrical property of the polar coordinates, we replace convolution layers in conventional CNNs to cylindrical convolutional (CyConv) layers. A CyConv layer exploits the cylindrically sliding windows (CSW) mechanism that vertically extends the input-image receptive fields of boundary units in a convolutional layer. We evaluate CyCNN and conventional CNN models for classification tasks on rotated MNIST, CIFAR-10, and SVHN datasets. We show that if there is no data augmentation during training, CyCNN significantly improves classification accuracies when compared to conventional CNN models. Our implementation of CyCNN is publicly available on https://github.com/mcrl/CyCNN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/CTW38JJ9/Kim et al. - 2020 - CyCNN A Rotation Invariant CNN using Polar Mappin.pdf;/Users/marinbenc/Zotero/storage/MQWMRPVX/2007.html}
}

@inproceedings{kurugolAortaSegmentation3D2012,
  title = {Aorta Segmentation with a {{3D}} Level Set Approach and Quantification of Aortic Calcifications in Non-Contrast Chest {{CT}}},
  booktitle = {2012 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Kurugol, S. and San Jose Estepar, Raul and Ross, J. and Washko, G. R.},
  year = {2012},
  month = aug,
  pages = {2343--2346},
  publisher = {{IEEE}},
  address = {{San Diego, CA}},
  doi = {10.1109/EMBC.2012.6346433},
  urldate = {2022-05-04},
  isbn = {978-1-4577-1787-1 978-1-4244-4119-8},
  file = {/Users/marinbenc/Zotero/storage/7U6PGIIL/Kurugol et al. - 2012 - Aorta segmentation with a 3D level set approach an.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  urldate = {2023-10-05}
}

@inproceedings{Li2019,
  title = {A Neural Network-Based Method for Automatic Pericardium Segmentation},
  booktitle = {Proceedings of the 2nd International Conference on Computer Science and Software Engineering},
  author = {Li, Zhiquan and Zou, Lin and Yang, Ran},
  year = {2019},
  series = {{{CSSE}} 2019},
  pages = {45--49},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3339363.3339372},
  isbn = {978-1-4503-7172-8},
  keywords = {Neural Network,Pericardium Segmentation}
}

@article{liu2021Swin,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  journal = {arXiv preprint arXiv:2103.14030},
  eprint = {2103.14030},
  archiveprefix = {arxiv}
}

@inproceedings{liu2021Swin,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021}
}

@article{liuCurrentRoleImage2017,
  title = {The {{Current Role}} of {{Image Compression Standards}} in {{Medical Imaging}}},
  author = {Liu, Feng and {Hern{\'a}ndez-Cabronero}, Miguel and Sanchez, Victor and Marcellin, Michael and Bilgin, Ali},
  year = {2017},
  month = oct,
  journal = {Information},
  volume = {8},
  pages = {131},
  doi = {10.3390/info8040131},
  abstract = {With the increasing utilization of medical imaging in clinical practice and the growing dimensions of data volumes generated by various medical imaging modalities, the distribution, storage, and management of digital medical image data sets requires data compression. Over the past few decades, several image compression standards have been proposed by international standardization organizations. This paper discusses the current status of these image compression standards in medical imaging applications together with some of the legal and regulatory issues surrounding the use of compression in medical settings.},
  file = {/Users/marinbenc/Zotero/storage/XT3LAGTM/Liu et al. - 2017 - The Current Role of Image Compression Standards in.pdf}
}

@article{liuDDNetCartesianpolarDualdomain2019,
  title = {{{DDNet}}: {{Cartesian-polar Dual-domain Network}} for the {{Joint Optic Disc}} and {{Cup Segmentation}}},
  shorttitle = {{{DDNet}}},
  author = {Liu, Qing and Hong, Xiaopeng and Ke, Wei and Chen, Zailiang and Zou, Beiji},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1904.08773},
  urldate = {2022-05-10},
  abstract = {Existing joint optic disc and cup segmentation approaches are developed either in Cartesian or polar coordinate system. However, due to the subtle optic cup, the contextual information exploited from the single domain even by the prevailing CNNs is still insufficient. In this paper, we propose a novel segmentation approach, named Cartesian-polar dual-domain network (DDNet), which for the first time considers the complementary of the Cartesian domain and the polar domain. We propose a two-branch of domain feature encoder and learn translation equivariant representations on rectilinear grid from Cartesian domain and rotation equivariant representations on polar grid from polar domain parallelly. To fuse the features on two different grids, we propose a dual-domain fusion module. This module builds the correspondence between two grids by the differentiable polar transform layer and learns the feature importance across two domains in element-wise to enhance the expressive capability. Finally, the decoder aggregates the fused features from low-level to high-level and makes dense predictions. We validate the state-of-the-art segmentation performances of our DDNet on the public dataset ORIGA. According to the segmentation masks, we estimate the commonly used clinical measure for glaucoma, i.e., the vertical cup-to-disc ratio. The low cup-to-disc ratio estimation error demonstrates the potential application in glaucoma screening.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences}
}

@article{liuDDNetCartesianpolarDualdomain2019a,
  title = {{{DDNet}}: {{Cartesian-polar Dual-domain Network}} for the {{Joint Optic Disc}} and {{Cup Segmentation}}},
  shorttitle = {{{DDNet}}},
  author = {Liu, Qing and Hong, Xiaopeng and Ke, Wei and Chen, Zailiang and Zou, Beiji},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.08773 [cs]},
  eprint = {1904.08773},
  primaryclass = {cs},
  urldate = {2021-04-01},
  abstract = {Existing joint optic disc and cup segmentation approaches are developed either in Cartesian or polar coordinate system. However, due to the subtle optic cup, the contextual information exploited from the single domain even by the prevailing CNNs is still insufficient. In this paper, we propose a novel segmentation approach, named Cartesian-polar dual-domain network (DDNet), which for the first time considers the complementary of the Cartesian domain and the polar domain. We propose a two-branch of domain feature encoder and learn translation equivariant representations on rectilinear grid from Cartesian domain and rotation equivariant representations on polar grid from polar domain parallelly. To fuse the features on two different grids, we propose a dual-domain fusion module. This module builds the correspondence between two grids by the differentiable polar transform layer and learns the feature importance across two domains in element-wise to enhance the expressive capability. Finally, the decoder aggregates the fused features from low-level to high-level and makes dense predictions. We validate the state-of-the-art segmentation performances of our DDNet on the public dataset ORIGA. According to the segmentation masks, we estimate the commonly used clinical measure for glaucoma, i.e., the vertical cup-to-disc ratio. The low cup-to-disc ratio estimation error demonstrates the potential application in glaucoma screening.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/W36E24YP/Liu et al. - 2019 - DDNet Cartesian-polar Dual-domain Network for the.pdf;/Users/marinbenc/Zotero/storage/3V8FQANE/1904.html}
}

@article{liuSelfsupervisedLearningMore2021,
  title = {Self-Supervised {{Learning}} Is {{More Robust}} to {{Dataset Imbalance}}},
  author = {Liu, Hong and HaoChen, Jeff Z. and Gaidon, Adrien and Ma, Tengyu},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.05025 [cs, stat]},
  eprint = {2110.05025},
  primaryclass = {cs, stat},
  urldate = {2022-02-16},
  abstract = {Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find out via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments and theoretical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/64JUHWXH/Liu et al. - 2021 - Self-supervised Learning is More Robust to Dataset.pdf;/Users/marinbenc/Zotero/storage/4KVWATH5/2110.html}
}

@misc{liuUnsupervisedDeepDomain2018,
  title = {Unsupervised {{Deep Domain Adaptation}} for {{Pedestrian Detection}}},
  author = {Liu, Lihang and Lin, Weiyao and Wu, Lisheng and Yu, Yong and Yang, Michael Ying},
  year = {2018},
  month = feb,
  number = {arXiv:1802.03269},
  eprint = {1802.03269},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {This paper addresses the problem of unsupervised domain adaptation on the task of pedestrian detection in crowded scenes. First, we utilize an iterative algorithm to iteratively select and auto-annotate positive pedestrian samples with high confidence as the training samples for the target domain. Meanwhile, we also reuse negative samples from the source domain to compensate for the imbalance between the amount of positive samples and negative samples. Second, based on the deep network we also design an unsupervised regularizer to mitigate influence from data noise. More specifically, we transform the last fully connected layer into two sub-layers - an element-wise multiply layer and a sum layer, and add the unsupervised regularizer to further improve the domain adaptation accuracy. In experiments for pedestrian detection, the proposed method boosts the recall value by nearly 30\% while the precision stays almost the same. Furthermore, we perform our method on standard domain adaptation benchmarks on both supervised and unsupervised settings and also achieve state-of-the-art results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/P2YU3XCR/Liu et al_2018_Unsupervised Deep Domain Adaptation for Pedestrian Detection.pdf;/Users/marinbenc/Zotero/storage/VY7U6WWI/1802.html}
}

@inproceedings{long2015fully,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440}
}

@article{lotanMedicalImagingPrivacy2020,
  title = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}: {{Myth}}, {{Fallacy}}, and the {{Future}}},
  shorttitle = {Medical {{Imaging}} and {{Privacy}} in the {{Era}} of {{Artificial Intelligence}}},
  author = {Lotan, Eyal and Tschider, Charlotte and Sodickson, Daniel K. and Caplan, Arthur L. and Bruno, Mary and Zhang, Ben and Lui, Yvonne W.},
  year = {2020},
  month = sep,
  journal = {Journal of the American College of Radiology},
  volume = {17},
  number = {9},
  pages = {1159--1162},
  issn = {15461440},
  doi = {10.1016/j.jacr.2020.04.007},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/BFEQKKMA/Lotan et al_2020_Medical Imaging and Privacy in the Era of Artificial Intelligence.pdf}
}

@article{lutnickIntegratedIterativeAnnotation2019,
  title = {An Integrated Iterative Annotation Technique for Easing Neural Network Training in Medical Image Analysis},
  author = {Lutnick, Brendon and Ginley, Brandon and Govind, Darshana and McGarry, Sean D. and LaViolette, Peter S. and Yacoub, Rabi and Jain, Sanjay and Tomaszewski, John E. and Jen, Kuang-Yu and Sarder, Pinaki},
  year = {2019},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {2},
  pages = {112--119},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0018-3},
  urldate = {2023-09-28},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RU4JUGUR/Lutnick et al_2019_An integrated iterative annotation technique for easing neural network training.pdf}
}

@article{Mahabadi2013,
  title = {Association of Epicardial Fat with Cardiovascular Risk Factors and Incident Myocardial Infarction in the General Population},
  author = {Mahabadi, Amir A. and Berg, Marie H. and Lehmann, Nils and K{\"a}lsch, Hagen and Bauer, Marcus and Kara, Kaffer and Dragano, Nico and Moebus, Susanne and J{\"o}ckel, Karl-Heinz and Erbel, Raimund and M{\"o}hlenkamp, Stefan},
  year = {2013},
  month = apr,
  journal = {Journal of the American College of Cardiology},
  volume = {61},
  number = {13},
  pages = {1388--1395},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jacc.2012.11.062}
}

@article{Mahabadi2014,
  title = {Association of Epicardial Adipose Tissue with Progression of Coronary Artery Calcification Is More Pronounced in the Early Phase of Atherosclerosis},
  author = {Mahabadi, Amir A. and Lehmann, Nils and K{\"a}lsch, Hagen and Robens, Tim and Bauer, Marcus and Dykun, Iryna and Budde, Thomas and Moebus, Susanne and J{\"o}ckel, Karl-Heinz and Erbel, Raimund and M{\"o}hlenkamp, Stefan},
  year = {2014},
  month = sep,
  journal = {JACC: Cardiovascular Imaging},
  volume = {7},
  number = {9},
  pages = {909--916},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jcmg.2014.07.002}
}

@article{mahabadiCardiacComputedTomographyderived2017,
  title = {Cardiac Computed Tomography-Derived Epicardial Fat Volume and Attenuation Independently Distinguish Patients with and without Myocardial Infarction},
  author = {Mahabadi, Amir Abbas and Balcer, Bastian and Dykun, Iryna and Forsting, Michael and Schlosser, Thomas and Heusch, Gerd and Rassaf, Tienush},
  editor = {Merx, Marc W.},
  year = {2017},
  month = aug,
  journal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0183514},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183514},
  urldate = {2023-09-29},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XEEL69CF/Mahabadi et al_2017_Cardiac computed tomography-derived epicardial fat volume and attenuation.pdf}
}

@article{manet,
  title = {{{MA-Net}}: {{A}} Multi-Scale Attention Network for Liver and Tumor Segmentation},
  author = {Fan, Tongle and Wang, Guanglei and Li, Yan and Wang, Hongrui},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {179656--179665},
  doi = {10.1109/ACCESS.2020.3025372}
}

@inproceedings{marinEfficientSegmentationLearning2019,
  title = {Efficient {{Segmentation}}: {{Learning Downsampling Near Semantic Boundaries}}},
  shorttitle = {Efficient {{Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Marin, Dmitrii and He, Zijian and Vajda, Peter and Chatterjee, Priyam and Tsai, Sam and Yang, Fei and Boykov, Yuri},
  year = {2019},
  month = oct,
  pages = {2131--2141},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00222},
  abstract = {Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.},
  keywords = {Adaptation models,Computational modeling,Image resolution,Image segmentation,Interpolation,Semantics,Tensile stress},
  file = {/Users/marinbenc/Zotero/storage/9ZNP7RNZ/Marin et al. - 2019 - Efficient Segmentation Learning Downsampling Near.pdf;/Users/marinbenc/Zotero/storage/PFU8RZ47/9008795.html}
}

@article{MARTINBLAND1986307,
  title = {Statistical Methods for Assessing Agreement between Two Methods of Clinical Measurement},
  author = {Martin Bland, J. and Altman, {\relax DouglasG}.},
  year = {1986},
  journal = {The Lancet},
  volume = {327},
  number = {8476},
  pages = {307--310},
  issn = {0140-6736},
  doi = {10.1016/S0140-6736(86)90837-8},
  abstract = {In clinical measurement comparison of a new measurement technique with an established one is often needed to see whether they agree sufficiently for the new to replace the old. Such investigations are often analysed inappropriately, notably by using correlation coefficients. The use of correlation is misleading. An alternative approach, based on graphical techniques and simple calculations, is described, together with the relation between this analysis and the assessment of repeatability.}
}

@article{Marwan2013,
  title = {Quantification of Epicardial Fat by Computed Tomography: {{Why}}, When and How?},
  author = {Marwan, Mohamed and Achenbach, Stephan},
  year = {2013},
  month = jan,
  journal = {Journal of Cardiovascular Computed Tomography},
  volume = {7},
  number = {1},
  pages = {3--10},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jcct.2013.01.002}
}

@article{Militello2019,
  title = {A Semi-Automatic Approach for Epicardial Adipose Tissue Segmentation and Quantification on Cardiac {{CT}} Scans},
  author = {Militello, Carmelo and Rundo, Leonardo and Toia, Patrizia and Conti, Vincenzo and Russo, Giorgio and Filorizzo, Clarissa and Maffei, Erica and Cademartiri, Filippo and Grutta, Ludovico La and Midiri, Massimo and Vitabile, Salvatore},
  year = {2019},
  month = nov,
  journal = {Computers in Biology and Medicine},
  volume = {114},
  pages = {103424},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.compbiomed.2019.103424}
}

@misc{monaiconsortiumMONAIMedicalOpen2023,
  title = {{{MONAI}}: {{Medical Open Network}} for {{AI}}},
  shorttitle = {{{MONAI}}},
  author = {{MONAI Consortium}},
  year = {2023},
  month = oct,
  doi = {10.5281/ZENODO.4323058},
  urldate = {2023-11-28},
  abstract = {AI Toolkit for Healthcare Imaging},
  copyright = {Apache License 2.0, Open Access},
  howpublished = {Zenodo}
}

@misc{myronenkoAutomated3DSegmentation2023,
  title = {Automated {{3D Segmentation}} of {{Kidneys}} and {{Tumors}} in {{MICCAI KiTS}} 2023 {{Challenge}}},
  author = {Myronenko, Andriy and Yang, Dong and He, Yufan and Xu, Daguang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.04110},
  eprint = {2310.04110},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-28},
  abstract = {Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform for researchers to compare their solutions to segmentation from 3D CT. In this work, we describe our submission to the challenge using automated segmentation of Auto3DSeg available in MONAI. Our solution achieves the average dice of 0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023 challenge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/NM2RZWIK/Myronenko et al_2023_Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge.pdf;/Users/marinbenc/Zotero/storage/2K9HW7IB/2310.html}
}

@inproceedings{nazeriTwoStageConvolutionalNeural2018,
  title = {Two-{{Stage Convolutional Neural Network}} for {{Breast Cancer Histology Image Classification}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Nazeri, Kamyar and Aminpour, Azad and Ebrahimi, Mehran},
  editor = {Campilho, Aur{\'e}lio and Karray, Fakhri and {ter Haar Romeny}, Bart},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {717--726},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93000-8_81},
  abstract = {This paper explores the problem of breast tissue classification of microscopy images. Based on the predominant cancer type the goal is to classify images into four categories of normal, benign, in situ carcinoma, and invasive carcinoma. Given a suitable training dataset, we utilize deep learning techniques to address the classification problem. Due to the large size of each image in the training dataset, we propose a patch-based technique which consists of two consecutive convolutional neural networks. The first ``patch-wise'' network acts as an auto-encoder that extracts the most salient features of image patches while the second ``image-wise'' network performs classification of the whole image. The first network is pre-trained and aimed at extracting local information while the second network obtains global information of an input image. We trained the networks using the ICIAR 2018 grand challenge on BreAst Cancer Histology (BACH) dataset. The proposed method yields \$\$95{\textbackslash}\%\$\$95\%accuracy on the validation set compared to previously reported \$\$77{\textbackslash}\%\$\$77\%accuracy rates in the literature. Our code is publicly available at https://github.com/ImagingLab/ICIAR2018.},
  isbn = {978-3-319-93000-8},
  langid = {english},
  keywords = {Breast cancer,Convolutional neural networks,Microscopy image classification,Patch-wise learning,Whole slide images},
  file = {/Users/marinbenc/Zotero/storage/JQA2P9S5/Nazeri et al. - 2018 - Two-Stage Convolutional Neural Network for Breast .pdf}
}

@incollection{newellStackedHourglassNetworks2016,
  title = {Stacked {{Hourglass Networks}} for {{Human Pose Estimation}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9912},
  pages = {483--499},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46484-8_29},
  urldate = {2021-08-23},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/ZXRR7XDV/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf}
}

@misc{nguyen2020vindrcxr,
  title = {{{VinDr-CXR}}: {{An}} Open Dataset of Chest {{X-rays}} with Radiologist's Annotations},
  author = {Nguyen, Ha Q. and Lam, Khanh and Le, Linh T. and Pham, Hieu H. and Tran, Dat Q. and Nguyen, Dung B. and Le, Dung D. and Pham, Chi M. and Tong, Hang T. T. and Dinh, Diep H. and Do, Cuong D. and Doan, Luu T. and Nguyen, Cuong N. and Nguyen, Binh T. and Nguyen, Que V. and Hoang, Au D. and Phan, Hien N. and Nguyen, Anh T. and Ho, Phuong H. and Ngo, Dat T. and Nguyen, Nghia T. and Nguyen, Nhan T. and Dao, Minh and Vu, Van},
  year = {2020},
  eprint = {2012.15029},
  primaryclass = {eess.IV},
  archiveprefix = {arxiv}
}

@incollection{norooziUnsupervisedLearningVisual2016,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2022-02-16},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/XSBR7VU9/Noroozi and Favaro - 2016 - Unsupervised Learning of Visual Representations by.pdf}
}

@incollection{norooziUnsupervisedLearningVisual2016a,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2016},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9910},
  pages = {69--84},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_5},
  urldate = {2023-11-30},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/JHCT2EEB/Noroozi_Favaro_2016_Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  year = {2000},
  journal = {Dr. Dobb's Journal of Software Tools},
  citeulike-article-id = {2236121},
  posted-at = {2008-01-15 19:21:54},
  priority = {4},
  keywords = {bibtex-import}
}

@inproceedings{pmlr-v28-li13b,
  title = {Fixed-Point Model for Structured Labeling},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  author = {Li, Quannan and Wang, Jingdong and Wipf, David and Tu, Zhuowen},
  editor = {Dasgupta, Sanjoy and McAllester, David},
  year = {2013},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {28},
  pages = {214--221},
  publisher = {{PMLR}},
  address = {{Atlanta, Georgia, USA}},
  abstract = {In this paper, we propose a simple but effective solution to the structured labeling problem: a fixed-point model. Recently, layered models with sequential classifiers/regressors have gained an increasing amount of interests for structural prediction. Here, we design an algorithm with a new perspective on layered models; we aim to find a fixed-point function with the structured labels being both the output and the input. Our approach alleviates the burden in learning multiple/different classifiers in different layers. We devise a training strategy for our method and provide justifications for the fixed-point function to be a contraction mapping. The learned function captures rich contextual information and is easy to train and test. On several widely used benchmark datasets, the proposed method observes significant improvement in both performance and efficiency over many state-of-the-art algorithms.},
  pdf = {http://proceedings.mlr.press/v28/li13b.pdf}
}

@inproceedings{Qiu2018,
  title = {Training {{FCNs}} Model with Lesion-Size-Unified Dermoscopy Images for Lesion Segmentation},
  booktitle = {2018 International Conference on Artificial Intelligence and Big Data ({{ICAIBD}})},
  author = {Qiu, Yuming and Qin, Xiaolin and Zhang, Ju},
  year = {2018},
  month = may,
  publisher = {{IEEE}},
  doi = {10.1109/icaibd.2018.8396187}
}

@article{radlAVTMulticenterAortic2022,
  title = {{{AVT}}: {{Multicenter}} Aortic Vessel Tree {{CTA}} Dataset Collection with Ground Truth Segmentation Masks},
  shorttitle = {{{AVT}}},
  author = {Radl, Lukas and Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Egger, Jan},
  year = {2022},
  month = feb,
  journal = {Data in Brief},
  volume = {40},
  pages = {107801},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.107801},
  urldate = {2022-05-04},
  langid = {english}
}

@article{radlAVTMulticenterAortic2022a,
  title = {{{AVT}}: {{Multicenter}} Aortic Vessel Tree {{CTA}} Dataset Collection with Ground Truth Segmentation Masks},
  shorttitle = {{{AVT}}},
  author = {Radl, Lukas and Jin, Yuan and Pepe, Antonio and Li, Jianning and Gsaxner, Christina and Zhao, Fen-hua and Egger, Jan},
  year = {2022},
  month = feb,
  journal = {Data in Brief},
  volume = {40},
  pages = {107801},
  issn = {23523409},
  doi = {10.1016/j.dib.2022.107801},
  urldate = {2023-10-05},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/KI8HGT9Y/Radl et al_2022_AVT.pdf}
}

@article{Raggi2013,
  title = {Epicardial Adipose Tissue: {{A}} Long-Overlooked Marker of Risk of Cardiovascular Disease},
  author = {Raggi, Paolo and Alakija, Pauline},
  year = {2013},
  month = jul,
  journal = {Atherosclerosis},
  volume = {229},
  number = {1},
  pages = {32--33},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.atherosclerosis.2013.02.030}
}

@inproceedings{recasens2018learning,
  title = {Learning to Zoom: A Saliency-Based Sampling Layer for Neural Networks},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Recasens, Adria and Kellnhofer, Petr and Stent, Simon and Matusik, Wojciech and Torralba, Antonio},
  year = {2018},
  pages = {51--66}
}

@article{regionGrowing,
  title = {Seeded Region Growing},
  author = {Adams, R. and Bischof, L.},
  year = {1994},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {6},
  pages = {641--647},
  doi = {10.1109/34.295913}
}

@article{Rodrigues2016,
  title = {A Novel Approach for the Automated Segmentation and Volume Quantification of Cardiac Fats on Computed Tomography},
  author = {Rodrigues, {\'E}.O. and Morais, F.F.C. and Morais, N.A.O.S. and Conci, L.S. and Neto, L.V. and Conci, A.},
  year = {2016},
  month = jan,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {123},
  pages = {109--128},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.cmpb.2015.09.017}
}

@article{Rodrigues2017,
  title = {Machine Learning in the Prediction of Cardiac Epicardial and Mediastinal Fat Volumes},
  author = {Rodrigues, {\'E}.O. and Pinheiro, V.H.A. and Liatsis, P. and Conci, A.},
  year = {2017},
  month = oct,
  journal = {Computers in Biology and Medicine},
  volume = {89},
  pages = {520--529},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.compbiomed.2017.02.010}
}

@article{rodriguesNovelApproachAutomated2016,
  title = {A Novel Approach for the Automated Segmentation and Volume Quantification of Cardiac Fats on Computed Tomography},
  author = {Rodrigues, {\'E}.O. and Morais, F.F.C. and Morais, N.A.O.S. and Conci, L.S. and Neto, L.V. and Conci, A.},
  year = {2016},
  month = jan,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {123},
  pages = {109--128},
  issn = {01692607},
  doi = {10.1016/j.cmpb.2015.09.017},
  urldate = {2021-04-13},
  langid = {english}
}

@incollection{Rohlfing2005,
  title = {Quo Vadis, Atlas-Based Segmentation?},
  booktitle = {Handbook of Biomedical Image Analysis},
  author = {Rohlfing, Torsten and Brandt, Robert and Menzel, Randolf and Russakoff, Daniel B. and Maurer, Calvin R.},
  year = {2005},
  pages = {435--486},
  publisher = {{Springer US}},
  doi = {10.1007/0-306-48608-3_11}
}

@misc{rombach2021highresolution,
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2021},
  eprint = {2112.10752},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@misc{ronneberger2015unet,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  eprint = {1505.04597},
  primaryclass = {cs.CV},
  archiveprefix = {arxiv}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} {\textendash} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  urldate = {2021-08-23},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/PR8AHFUU/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015a,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} {\textendash} {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  urldate = {2022-05-06},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/TWH66NBH/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015d,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-06},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marinbenc/Zotero/storage/IYQVGL78/Ronneberger et al_2015_U-Net.pdf;/Users/marinbenc/Zotero/storage/6UK6QXVC/1505.html}
}

@article{rotembergPatientcentricDatasetImages2021,
  title = {A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author = {Rotemberg, Veronica and Kurtansky, Nicholas and {Betz-Stablein}, Brigid and Caffery, Liam and Chousakos, Emmanouil and Codella, Noel and Combalia, Marc and Dusza, Stephen and Guitera, Pascale and Gutman, David and Halpern, Allan and Helba, Brian and Kittler, Harald and Kose, Kivanc and Langer, Steve and Lioprys, Konstantinos and Malvehy, Josep and Musthaq, Shenara and Nanda, Jabpani and Reiter, Ofer and Shih, George and Stratigos, Alexander and Tschandl, Philipp and Weber, Jochen and Soyer, H. Peter},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {34},
  issn = {2052-4463},
  doi = {10.1038/s41597-021-00815-z},
  urldate = {2023-10-02},
  abstract = {Abstract             Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients (20.8\% with at least one melanoma, 79.2\% with zero melanomas) from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 (1.8\%) histopathologically confirmed melanomas compared with benign melanoma mimickers.},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/P5YJS86H/Rotemberg et al_2021_A patient-centric dataset of images and metadata for identifying melanomas.pdf}
}

@article{sabottkeEffectImageResolution2020,
  title = {The {{Effect}} of {{Image Resolution}} on {{Deep Learning}} in                     {{Radiography}}},
  author = {Sabottke, Carl F. and Spieler, Bradley M.},
  year = {2020},
  month = jan,
  journal = {Radiology: Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {e190015},
  publisher = {{Radiological Society of North America}},
  doi = {10.1148/ryai.2019190015},
  urldate = {2022-09-16},
  abstract = {Purpose To examine variations of convolutional neural network (CNN) performance for multiple chest radiograph diagnoses and image resolutions. Materials and Methods This retrospective study examined CNN performance using the publicly available National Institutes of Health chest radiograph dataset comprising 112 120 chest radiographic images from 30 805 patients. The network architectures examined included ResNet34 and DenseNet121. Image resolutions ranging from 32 {\texttimes} 32 to 600 {\texttimes} 600 pixels were investigated. Network training paradigms used 80\% of samples for training and 20\% for validation. CNN performance was evaluated based on area under the receiver operating characteristic curve (AUC) and label accuracy. Binary output networks were trained separately for each label or diagnosis under consideration. Results Maximum AUCs were achieved at image resolutions between 256 {\texttimes} 256 and 448 {\texttimes} 448 pixels for binary decision networks targeting emphysema, cardiomegaly, hernias, edema, effusions, atelectasis, masses, and nodules. When comparing performance between networks that utilize lower resolution (64 {\texttimes} 64 pixels) versus higher (320 {\texttimes} 320 pixels) resolution inputs, emphysema, cardiomegaly, hernia, and pulmonary nodule detection had the highest fractional improvements in AUC at higher image resolutions. Specifically, pulmonary nodule detection had an AUC performance ratio of 80.7\% {$\pm$} 1.5 (standard deviation) (0.689 of 0.854) whereas thoracic mass detection had an AUC ratio of 86.7\% {$\pm$} 1.2 (0.767 of 0.886) for these image resolutions. Conclusion Increasing image resolution for CNN training often has a trade-off with the maximum possible batch size, yet optimal selection of image resolution has the potential for further increasing neural network performance for various radiology-based machine learning tasks. Furthermore, identifying diagnosis-specific tasks that require relatively higher image resolution can potentially provide insight into the relative difficulty of identifying different radiology findings. Supplemental material is available for this article. Keywords: Computer Aided Diagnosis (CAD), Conventional Radiography, Convolutional Neural Network (CNN), Neural Networks, Thorax {\textcopyright} RSNA, 2020 See also the commentary by Lakhani in this issue.},
  file = {/Users/marinbenc/Zotero/storage/HNTCBCWG/Sabottke and Spieler - 2020 - The Effect of Image Resolution on Deep Learning in.pdf}
}

@article{sabottkeEffectImageResolution2020a,
  title = {The {{Effect}} of {{Image Resolution}} on {{Deep Learning}} in {{Radiography}}},
  author = {Sabottke, Carl F. and Spieler, Bradley M.},
  year = {2020},
  month = jan,
  journal = {Radiology: Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {e190015},
  issn = {2638-6100},
  doi = {10.1148/ryai.2019190015},
  urldate = {2024-01-30},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/NBVK28JA/Sabottke_Spieler_2020_The Effect of Image Resolution on Deep Learning in Radiography.pdf}
}

@article{Sacks2007,
  title = {Human Epicardial Adipose Tissue: {{A}} Review},
  author = {Sacks, Harold S. and Fain, John N.},
  year = {2007},
  month = jun,
  journal = {American Heart Journal},
  volume = {153},
  number = {6},
  pages = {907--917},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.ahj.2007.03.019}
}

@misc{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2205.11487},
  eprint = {2205.11487},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-30},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/MEH4TB3W/Saharia et al_2022_Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.pdf;/Users/marinbenc/Zotero/storage/AIYAYTZM/2205.html}
}

@inproceedings{salehinejadImageAugmentationUsing2018,
  title = {Image {{Augmentation Using Radial Transform}} for {{Training Deep Neural Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Salehinejad, Hojjat and Valaee, Shahrokh and Dowdell, Tim and Barfett, Joseph},
  year = {2018},
  month = apr,
  pages = {3016--3020},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8462241},
  urldate = {2021-08-23},
  isbn = {978-1-5386-4658-8}
}

@inproceedings{salehinejadImageAugmentationUsing2018a,
  title = {Image {{Augmentation Using Radial Transform}} for {{Training Deep Neural Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Salehinejad, Hojjat and Valaee, Shahrokh and Dowdell, Tim and Barfett, Joseph},
  year = {2018},
  month = apr,
  pages = {3016--3020},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8462241},
  urldate = {2022-05-11},
  isbn = {978-1-5386-4658-8}
}

@article{selleAnalysisVasculatureLiver2002,
  title = {Analysis of Vasculature for Liver Surgical Planning},
  author = {Selle, D. and Preim, B. and Schenk, A. and Peitgen, H.-O.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {21},
  number = {11},
  pages = {1344--1357},
  issn = {0278-0062},
  doi = {10.1109/TMI.2002.801166},
  urldate = {2023-09-27},
  langid = {english}
}

@inproceedings{SETR,
  title = {Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers},
  booktitle = {{{CVPR}}},
  author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H.S. and Zhang, Li},
  year = {2021}
}

@article{Shahzad2013,
  title = {Automatic Quantification of Epicardial Fat Volume on Non-Enhanced Cardiac {{CT}} Scans Using a Multi-Atlas Segmentation Approach},
  author = {Shahzad, Rahil and Bos, Daniel and Metz, Coert and Rossi, Alexia and Kiri{\c s}li, Hortense and {van der Lugt}, Aad and Klein, Stefan and Witteman, Jacqueline and {de Feyter}, Pim and Niessen, Wiro and {van Vliet}, Lucas and {van Walsum}, Theo},
  year = {2013},
  month = aug,
  journal = {Medical Physics},
  volume = {40},
  number = {9},
  pages = {091910},
  publisher = {{Wiley}},
  doi = {10.1118/1.4817577}
}

@book{shalev-shwartzUnderstandingMachineLearning2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  month = may,
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781107298019},
  urldate = {2023-12-01},
  abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for advanced undergraduates or beginning graduates, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics and engineering.},
  isbn = {978-1-107-05713-5 978-1-107-29801-9}
}

@incollection{shinMedicalImageSynthesis2018,
  title = {Medical {{Image Synthesis}} for {{Data Augmentation}} and {{Anonymization Using Generative Adversarial Networks}}},
  booktitle = {Simulation and {{Synthesis}} in {{Medical Imaging}}},
  author = {Shin, Hoo-Chang and Tenenholtz, Neil A. and Rogers, Jameson K. and Schwarz, Christopher G. and Senjem, Matthew L. and Gunter, Jeffrey L. and Andriole, Katherine P. and Michalski, Mark},
  editor = {Gooya, Ali and Goksel, Orcun and Oguz, Ipek and Burgos, Ninon},
  year = {2018},
  volume = {11037},
  pages = {1--11},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00536-8_1},
  urldate = {2023-12-01},
  isbn = {978-3-030-00535-1 978-3-030-00536-8},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/WDXVQG8D/Shin et al_2018_Medical Image Synthesis for Data Augmentation and Anonymization Using.pdf}
}

@article{sinclairAtlasISTNJointSegmentation2022a,
  title = {Atlas-{{ISTN}}: {{Joint}} Segmentation, Registration and Atlas Construction with Image-and-Spatial Transformer Networks},
  shorttitle = {Atlas-{{ISTN}}},
  author = {Sinclair, Matthew and Schuh, Andreas and Hahn, Karl and Petersen, Kersten and Bai, Ying and Batten, James and Schaap, Michiel and Glocker, Ben},
  year = {2022},
  month = may,
  journal = {Medical Image Analysis},
  volume = {78},
  pages = {102383},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102383},
  urldate = {2023-10-03},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/Q3S597NR/Sinclair et al_2022_Atlas-ISTN.pdf}
}

@inproceedings{sobhaniniaFetalUltrasoundImage2019,
  title = {Fetal {{Ultrasound Image Segmentation}} for {{Measuring Biometric Parameters Using Multi-Task Deep Learning}}},
  booktitle = {2019 41st {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Sobhaninia, Zahra and Rafiei, Shima and Emami, Ali and Karimi, Nader and Najarian, Kayvan and Samavi, Shadrokh and Reza Soroushmehr, S. M.},
  year = {2019},
  month = jul,
  pages = {6545--6548},
  publisher = {{IEEE}},
  address = {{Berlin, Germany}},
  doi = {10.1109/EMBC.2019.8856981},
  urldate = {2023-09-27},
  isbn = {978-1-5386-1311-5},
  file = {/Users/marinbenc/Zotero/storage/IAH26LKM/Sobhaninia et al_2019_Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters Using.pdf}
}

@article{tahaMetricsEvaluating3D2015,
  title = {Metrics for Evaluating {{3D}} Medical Image Segmentation: Analysis, Selection, and Tool},
  shorttitle = {Metrics for Evaluating {{3D}} Medical Image Segmentation},
  author = {Taha, Abdel Aziz and Hanbury, Allan},
  year = {2015},
  month = dec,
  journal = {BMC Medical Imaging},
  volume = {15},
  number = {1},
  pages = {29},
  issn = {1471-2342},
  doi = {10.1186/s12880-015-0068-x},
  urldate = {2022-05-10},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RHMR4LZM/Taha and Hanbury - 2015 - Metrics for evaluating 3D medical image segmentati.pdf}
}

@misc{tanEfficientNetRethinkingModel2020,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2020},
  month = sep,
  number = {arXiv:1905.11946},
  eprint = {1905.11946},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  urldate = {2022-09-16},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/marinbenc/Zotero/storage/S3VL5C2V/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf;/Users/marinbenc/Zotero/storage/L3HP5VWE/1905.html}
}

@article{tomarFANetFeedbackAttention2021a,
  title = {{{FANet}}: {{A Feedback Attention Network}} for {{Improved Biomedical Image Segmentation}}},
  shorttitle = {{{FANet}}},
  author = {Tomar, Nikhil Kumar and Jha, Debesh and Riegler, Michael A. and Johansen, H{\aa}vard D. and Johansen, Dag and Rittscher, Jens and Halvorsen, P{\aa}l and Ali, Sharib},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.17235 [cs, eess]},
  eprint = {2103.17235},
  primaryclass = {cs, eess},
  urldate = {2021-04-19},
  abstract = {With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/GV642GF8/Tomar et al. - 2021 - FANet A Feedback Attention Network for Improved B.pdf;/Users/marinbenc/Zotero/storage/QL674DQ4/2103.html}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995347},
  urldate = {2023-11-28},
  isbn = {978-1-4577-0394-2},
  file = {/Users/marinbenc/Zotero/storage/PYHB5657/Torralba_Efros_2011_Unbiased look at dataset bias.pdf}
}

@article{tschandlHAM10000DatasetLarge2018,
  title = {The {{HAM10000}} Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions},
  author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
  year = {2018},
  month = dec,
  journal = {Scientific Data},
  volume = {5},
  number = {1},
  pages = {180161},
  issn = {2052-4463},
  doi = {10.1038/sdata.2018.161},
  urldate = {2021-04-13},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/84M39I28/Tschandl et al. - 2018 - The HAM10000 dataset, a large collection of multi-.pdf}
}

@misc{uwaterlooSkinCancer,
  title = {Skin {{Cancer Detection}} | {{Vision}} and {{Image Processing Lab}} {\textemdash} Uwaterloo.ca, {{University}} of {{Waterloo}}},
  year = {2012},
  journal = {Skin Cancer Detection | Vision and Image Processing Lab {\textemdash} uwaterloo.ca, University of Waterloo},
  urldate = {2024-01-23},
  howpublished = {https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection}
}

@article{valanarasuKiUNetOvercompleteConvolutional2020a,
  title = {{{KiU-Net}}: {{Overcomplete Convolutional Architectures}} for {{Biomedical Image}} and {{Volumetric Segmentation}}},
  shorttitle = {{{KiU-Net}}},
  author = {Valanarasu, Jeya Maria Jose and Sindagi, Vishwanath A. and Hacihaliloglu, Ilker and Patel, Vishal M.},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.01663 [cs, eess]},
  eprint = {2010.01663},
  primaryclass = {cs, eess},
  urldate = {2021-04-22},
  abstract = {Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation- KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: https://github.com/jeya-maria-jose/KiU-Net-pytorch},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/marinbenc/Zotero/storage/L3XWJEEK/Valanarasu et al. - 2020 - KiU-Net Overcomplete Convolutional Architectures .pdf;/Users/marinbenc/Zotero/storage/Z4MWI9FD/2010.html}
}

@inproceedings{verchevalCounterfactualFunctionalConnectomes2023,
  title = {Counterfactual {{Functional Connectomes}} for {{Neurological Classifier Selection}}},
  booktitle = {2023 31st {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Vercheval, Nicolas and Ben{\v c}evi{\'c}, Marin and Mu{\v z}evi{\'c}, Dario and Gali{\'c}, Irena and Pi{\v z}urica, Aleksandra},
  year = {2023},
  month = sep,
  pages = {1050--1054},
  publisher = {{IEEE}},
  address = {{Helsinki, Finland}},
  doi = {10.23919/EUSIPCO58844.2023.10289859},
  isbn = {978-94-6459-360-0},
  file = {/Users/marinbenc/Zotero/storage/SDLQX28V/Vercheval et al_2023_Counterfactual Functional Connectomes for Neurological Classifier Selection.pdf}
}

@article{wasserthalTotalSegmentatorRobustSegmentation2023,
  title = {{{TotalSegmentator}}: {{Robust Segmentation}} of 104 {{Anatomic Structures}} in {{CT Images}}},
  shorttitle = {{{TotalSegmentator}}},
  author = {Wasserthal, Jakob and Breit, Hanns-Christian and Meyer, Manfred T. and Pradella, Maurice and Hinck, Daniel and Sauter, Alexander W. and Heye, Tobias and Boll, Daniel T. and Cyriac, Joshy and Yang, Shan and Bach, Michael and Segeroth, Martin},
  year = {2023},
  month = sep,
  journal = {Radiology: Artificial Intelligence},
  volume = {5},
  number = {5},
  pages = {e230024},
  issn = {2638-6100},
  doi = {10.1148/ryai.230024},
  urldate = {2023-11-27},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/RMTDDIKB/Wasserthal et al_2023_TotalSegmentator.pdf}
}

@article{waterloo,
  title = {Automatic Segmentation of Skin Lesions from Dermatological Photographs Using a Joint Probabilistic Texture Distinctiveness Approach},
  author = {Glaister, J and Wong, A and Clausi, D A.},
  year = {2014},
  journal = {IEEE Transactions on Biomedical Engineering}
}

@article{wenConvolutionalNeuralNetworks2020,
  title = {Convolutional Neural Networks for Classification of {{Alzheimer}}'s Disease: {{Overview}} and Reproducible Evaluation},
  shorttitle = {Convolutional Neural Networks for Classification of {{Alzheimer}}'s Disease},
  author = {Wen, Junhao and {Thibeau-Sutre}, Elina and {Diaz-Melo}, Mauricio and {Samper-Gonz{\'a}lez}, Jorge and Routier, Alexandre and Bottani, Simona and Dormont, Didier and Durrleman, Stanley and Burgos, Ninon and Colliot, Olivier},
  year = {2020},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {63},
  pages = {101694},
  issn = {13618415},
  doi = {10.1016/j.media.2020.101694},
  urldate = {2024-01-30},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/4MII4DW8/Wen et al_2020_Convolutional neural networks for classification of Alzheimer's disease.pdf}
}

@inproceedings{xie2021segformer,
  title = {{{SegFormer}}: {{Simple}} and Efficient Design for Semantic Segmentation with Transformers},
  booktitle = {Neural Information Processing Systems ({{NeurIPS}})},
  author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  year = {2021}
}

@article{yuThreeDimensionalDeepConvolutional2021,
  title = {A {{Three-Dimensional Deep Convolutional Neural Network}} for {{Automatic Segmentation}} and {{Diameter Measurement}} of {{Type B Aortic Dissection}}},
  author = {Yu, Yitong and Gao, Yang and Wei, Jianyong and Liao, Fangzhou and Xiao, Qianjiang and Zhang, Jie and Yin, Weihua and Lu, Bin},
  year = {2021},
  journal = {Korean Journal of Radiology},
  volume = {22},
  number = {2},
  pages = {168},
  issn = {1229-6929, 2005-8330},
  doi = {10.3348/kjr.2020.0313},
  urldate = {2022-05-05},
  langid = {english}
}

@article{Zhang2020,
  title = {Automatic Epicardial Fat Segmentation and Quantification of {{CT}} Scans Using Dual {{U-nets}} with a Morphological Processing Layer},
  author = {Zhang, Qi and Zhou, Jianhang and Zhang, Bob and Jia, Weijia and Wu, Enhua},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {128032--128041},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/access.2020.3008190}
}

@article{zhangAutomaticEpicardialFat2020a,
  title = {Automatic {{Epicardial Fat Segmentation}} and {{Quantification}} of {{CT Scans Using Dual U-Nets With}} a {{Morphological Processing Layer}}},
  author = {Zhang, Qi and Zhou, Jianhang and Zhang, Bob and Jia, Weijia and Wu, Enhua},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {128032--128041},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3008190},
  urldate = {2021-04-22},
  file = {/Users/marinbenc/Zotero/storage/JV42BARX/Zhang et al. - 2020 - Automatic Epicardial Fat Segmentation and Quantifi.pdf}
}

@inproceedings{zhao2017pspnet,
  title = {Pyramid Scene Parsing Network},
  booktitle = {{{CVPR}}},
  author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  year = {2017}
}

@article{zhou2019unetplusplus,
  title = {{{UNet}}++: {{Redesigning}} Skip Connections to Exploit Multiscale Features in Image Segmentation},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2019},
  journal = {IEEE Transactions on Medical Imaging},
  publisher = {{IEEE}}
}

@incollection{zhouFixedPointModelPancreas2017,
  title = {A {{Fixed-Point Model}} for {{Pancreas Segmentation}} in {{Abdominal CT Scans}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} - {{MICCAI}} 2017},
  author = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K. and Yuille, Alan L.},
  editor = {Descoteaux, Maxime and {Maier-Hein}, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
  year = {2017},
  volume = {10433},
  pages = {693--701},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-66182-7_79},
  urldate = {2022-12-19},
  isbn = {978-3-319-66181-0 978-3-319-66182-7},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/B7EKTDXI/Zhou et al. - 2017 - A Fixed-Point Model for Pancreas Segmentation in A.pdf}
}

@incollection{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and {Syeda-Mahmood}, Tanveer and Martel, Anne and {Maier-Hein}, Lena and Tavares, Jo{\~a}o Manuel R.S. and Bradley, Andrew and Papa, Jo{\~a}o Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
  year = {2018},
  volume = {11045},
  pages = {3--11},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00889-5_1},
  urldate = {2021-08-23},
  isbn = {978-3-030-00888-8 978-3-030-00889-5},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/FG69V4EC/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf}
}

@incollection{zhouUNetNestedUNet2018b,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Zhou, Zongwei and Rahman Siddiquee, Md Mahfuzur and Tajbakhsh, Nima and Liang, Jianming},
  editor = {Stoyanov, Danail and Taylor, Zeike and Carneiro, Gustavo and {Syeda-Mahmood}, Tanveer and Martel, Anne and {Maier-Hein}, Lena and Tavares, Jo{\~a}o Manuel R.S. and Bradley, Andrew and Papa, Jo{\~a}o Paulo and Belagiannis, Vasileios and Nascimento, Jacinto C. and Lu, Zhi and Conjeti, Sailesh and Moradi, Mehdi and Greenspan, Hayit and Madabhushi, Anant},
  year = {2018},
  volume = {11045},
  pages = {3--11},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-00889-5_1},
  urldate = {2022-10-25},
  isbn = {978-3-030-00888-8 978-3-030-00889-5},
  langid = {english},
  file = {/Users/marinbenc/Zotero/storage/QR98YF2W/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf}
}

@inproceedings{zhu3DCoarsetoFineFramework2018,
  title = {A {{3D Coarse-to-Fine Framework}} for {{Volumetric Medical Image Segmentation}}},
  booktitle = {2018 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Zhu, Zhuotun and Xia, Yingda and Shen, Wei and Fishman, Elliot and Yuille, Alan},
  year = {2018},
  month = sep,
  pages = {682--690},
  publisher = {{IEEE}},
  address = {{Verona}},
  doi = {10.1109/3DV.2018.00083},
  urldate = {2022-12-19},
  isbn = {978-1-5386-8425-2},
  file = {/Users/marinbenc/Zotero/storage/PYT5T9QR/Zhu et al. - 2018 - A 3D Coarse-to-Fine Framework for Volumetric Medic.pdf}
}
