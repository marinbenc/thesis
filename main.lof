\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A cardiac CTA in its full range (left) and windowed (right). \blx@tocontentsinit {2}\cite {radlAVTMulticenterAortic2022a}\relax }}{6}{figure.caption.23}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces An example T1-weighted MRI (left) and a T2-weighted MRI (right) showing a diffuse glioma. \blx@tocontentsinit {2}\cite {calabreseUniversityCaliforniaSan2022}\relax }}{7}{figure.caption.28}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces A demonstration of region growing for delineating the internal and external areas of the pericardium on a CT slice, set to the adipose tissue intensity range. The left image is the original input, and the right image depicts the segmented outcome with the heart exterior in red and the interior in blue. The green dots represent the manually chosen seed points initiating the region-growing technique. \blx@tocontentsinit {2}\cite {bencevicRecentProgressEpicardial2022}\relax }}{11}{figure.caption.38}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces A demonstration of employing active contours to complete the absent segments of the pericardium line, displayed in white. The contour, illustrated in blue, starts as a complete circle surrounding the image. With every iteration, the contour adapts more closely to the image's shape. \blx@tocontentsinit {2}\cite {bencevicRecentProgressEpicardial2022}\relax }}{12}{figure.caption.40}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic representation of the registration procedure. Initially, input and target images are chosen. Throughout the registration phase, the input image (shown in green) undergoes deformation to align with the fixed target image (shown in red). \blx@tocontentsinit {2}\cite {bencevicRecentProgressEpicardial2022}\relax }}{13}{figure.caption.42}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces A schematic of a supervised linear classifier in a machine learning workflow. The upper section illustrates the training phase. Here, features are color-coded according to their known class from training data, depicted in red and blue. The parameters of the decision boundary, which demarcates the zones of the two classes (highlighted in light red and grey), are determined during training. The lower part of the diagram depicts the inference stage. In this phase, features are extracted from new images, and the trained model is employed to classify each pixel in the image. \blx@tocontentsinit {2}\cite {bencevicRecentProgressEpicardial2022}\relax }}{14}{figure.caption.45}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces A small neural network with three layers, wherein each layer is connected to every neuron in the next layer. \blx@tocontentsinit {2}\cite {hastieElementsStatisticalLearning2009}\relax }}{15}{figure.caption.47}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces A visual depiction of a convolution procedure step-by-step. In each step, the kernel slides over the image (shown in blue). The overlapping elements between the kernel and the image are multiplied and then summed to produce a value in the resultant image (shown in green). \blx@tocontentsinit {2}\cite {dumoulinGuideConvolutionArithmetic2018}\relax }}{17}{figure.caption.56}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces An example of an input image (left) convolved with the Prewitt operator (right). Note that vertical edges are accentuated in the convolution result.\relax }}{18}{figure.caption.59}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces A view of one step of a single convolution operation inside a \ac {cnn} layer. The layer performs multiple convolutions, each with a different kernel that has an equal number of channels as the input image. In each step, the whole kernel slides over the width and height of the image, and the overlapping channels are multiplied together and summed to produce a single output value. The output of the convolution is one channel of a $n$-channel image where $n$ is the number of different kernels in the layer.\relax }}{20}{figure.caption.61}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces A typical architecture of a \ac {cnn} encoder. The encoder consists of consecutive convolutional and pooling layers that gradually increase the feature map depth and decrease its width and height. The result is a map of features that tells the decoder what features are on the image but does not provide much spatial information about the location of those features.Â \blx@tocontentsinit {2}\cite {lecunGradientbasedLearningApplied1998}\relax }}{20}{figure.caption.63}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces A diagram of how FCN forms predictions based on the output of different encoder layers. Encoder layers are shown on the left and the grid represents the coarseness of the feature map. The maps are combined at three different levels to produce three predictions. Each prediction is compared with the ground truth during training, but for inference only the 8x upsampled prediction is used. \blx@tocontentsinit {2}\cite {long2015fully}\relax }}{22}{figure.caption.66}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces A diagram of the U-Net model. The output of each layer of the encoder is concatenated to the input of its corresponding layer in the decoder. \blx@tocontentsinit {2}\cite {ronnebergerUNetConvolutionalNetworks2015d}\relax }}{23}{figure.caption.68}%
\contentsline {figure}{\numberline {2.14}{\ignorespaces A diagram of the nnU-Net procedure of creating a training configuration. \blx@tocontentsinit {2}\cite {isenseeNnUNetSelfconfiguringMethod2021}\relax }}{24}{figure.caption.70}%
\contentsline {figure}{\numberline {2.15}{\ignorespaces A comparison between U-Net (left) and U-Net++ (right). Each node in the graph represents a convolutional layer. The dashed arrows represent skip connections, while full arrows are downsampling or upsampling operations. \blx@tocontentsinit {2}\cite {zhou2019unetplusplus}\relax }}{25}{figure.caption.72}%
\contentsline {figure}{\numberline {2.16}{\ignorespaces A diagram of the Mask R-CNN architecture. Two parallel decoder branches are used to achieve segmentation and object detection simultaneously. \blx@tocontentsinit {2}\cite {heMaskRCNN2017b}\relax }}{26}{figure.caption.74}%
\contentsline {figure}{\numberline {2.17}{\ignorespaces A visualization of a single encoder layer in a transformer network. This shows the encoding process for one element of the input sequence.\relax }}{28}{figure.caption.82}%
\contentsline {figure}{\numberline {2.18}{\ignorespaces The ViT architecture for image classification. \blx@tocontentsinit {2}\cite {dosovitskiy2021an}\relax }}{30}{figure.caption.86}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A simulated example of heart disease risk prediction using simple linear regression. The plots on the right show how the approximated function depends on the sample size.\relax }}{32}{figure.caption.91}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Fitting a polygonal function of various degrees on three different sample sizes.\relax }}{33}{figure.caption.95}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces An overview of the simple transfer learning procedure. First, a model is pretrained on some related segmentation task. Then, part of the trained model's weights are copied to a new model that is then fine-tuned on the target segmentation task.\relax }}{35}{figure.caption.100}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces A diagram of the domain adaptation approach in \citet {ganinDA2015}. The gradients of the domain classification head which are applied to the encoder are reversed during backpropagation.\relax }}{36}{figure.caption.102}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces A self-supervised learning approach using shuffling image tiles as a pretext task presented by \citet {carr2021shuffle}. The trained feature encoder learns to extract relevant features and its parameters are transferred to a model trained to perform the downstream task.\relax }}{37}{figure.caption.104}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces An example of how a 2-class classification neural network bends an input space (left) such that it is linearly separable with a hyperplane.\relax }}{40}{figure.caption.112}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces The decision boundary (shown as the background color) according to a fully connected neural network with three layers where the second layer has one, two, or three neurons.\relax }}{40}{figure.caption.113}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces An example circular dataset transformed using the polar transform. The background color shows the decision boundary of a fully connected network with one neuron in the second layer.\relax }}{41}{figure.caption.115}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces The polar transformation of the simulated data using different polar origins ($C$).\relax }}{41}{figure.caption.116}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces A visual summary of the model-driven preprocessing approach. First, the transformation parameter predictor network predicts a segmentation-like map, from which the transformation parameters are calculated using a transformation parameter function. Then, the input image is transformed and the transformed representation is segmented by a segmentation neural network. The final segmentation map is obtained by inverting the transformation.\relax }}{42}{figure.caption.118}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces An example image and label from the lesion dataset and their corresponding polar transformation. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{46}{figure.caption.133}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces A diagram of the approach of predicting polar origins from a Cartesian network. The first network performs an initial segmentation, which is then used to extract a polar origin for the polar transformation. The method does not rely on any specific neural network architecture. The Polar and Cartesian network can be any neural network that takes an input image and produces a binary segmentation mask as output. The red point shows the extracted polar origin. The Polar network is trained on polar image transformations. The polar transformation is not part of the network itself, but happens as a preprocessing step for the Polar network. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{48}{figure.caption.141}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces A diagram of the approach of using a centerpoint prediction network. The first network can be any neural network that predicts a heatmap from an input image, which is then used to extract the polar origin, shown as a red point. The Polar network can be any semantic segmentation neural network that produces a binary mask output from an input image. The Polar network is trained on polar image transformations. The polar transformation is not part of the network itself, but happens as a preprocessing step for the Polar network. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{49}{figure.caption.143}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Examples of heatmaps generated from the ground truth data. The heatmap is a Gaussian centered on the center of mass of the ground-truth label, shown as a blue point on the input images. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{49}{figure.caption.144}%
\contentsline {figure}{\numberline {4.10}{\ignorespaces Example input images and ground-truth labels for each dataset used in our experiments. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{51}{figure.caption.151}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Polyp dataset}}}{51}{subfigure.10.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Lesion dataset}}}{51}{subfigure.10.2}%
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Liver dataset}}}{51}{subfigure.10.3}%
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {EAT dataset}}}{51}{subfigure.10.4}%
\contentsline {figure}{\numberline {4.11}{\ignorespaces The training and validation Dice coefficient (DSC) of the polar and Cartesian U-Net models during training. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{56}{figure.caption.162}%
\contentsline {figure}{\numberline {4.12}{\ignorespaces The relationship between mean squared errors of the centers used for the polar transformation and segmentation performance of the polar network on the lesion dataset. The mean squared errors are calculated compared to the ground-truth centers. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{57}{figure.caption.163}%
\contentsline {figure}{\numberline {4.13}{\ignorespaces The best Dice coefficient by epoch 50 for models trained on subsets of the lesion training dataset. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{57}{figure.caption.164}%
\contentsline {figure}{\numberline {4.14}{\ignorespaces A random sampling of inverse polar transformed predictions from the polar network with the polar origins predicted from the centerpoint predictor for various datasets. The prediction is shown in green and overlaid on top of the original input image. EAT predictions (d) are cropped and zoomed to better show the predictions. \blx@tocontentsinit {4}\cite {bencevicTrainingPolarImage2021}\relax }}{58}{figure.caption.166}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Polyp predictions}}}{58}{subfigure.14.1}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Lesion predictions}}}{58}{subfigure.14.2}%
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Liver predictions}}}{58}{subfigure.14.3}%
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {EAT predictions}}}{58}{subfigure.14.4}%
\contentsline {figure}{\numberline {4.15}{\ignorespaces A visual explanation of hysteresis thresholding. Kept regions are marked with green checkmarks, while deleted regions are marked with red crosses. The pixel intensity scale is shown on the right, where $t_1$ is marked in yellow and $t_2$ is marked in blue. All regions below $t_1$ are removed, while regions above $t_1$ are kept if they are connected to at least one pixel with intensity larger than $t_2$.\relax }}{61}{figure.caption.178}%
\contentsline {figure}{\numberline {4.16}{\ignorespaces A summary of our approach. An input image is first segmented using a U-Net network. For each connected component in the segmentation, the input image is transformed to polar coordinates using the centroid of the connected component as the origin. These images are then fed into a U-Net trained on polar images, and the predictions for each object are fused, hysteresis thresholded, and transformed back to cartesian coordinates. Note how one of the false positive connected components in the initial segmentation was removed during hysteresis thresholding since the component was only predicted in one of the three polar predictions. \blx@tocontentsinit {4}\cite {bencevicUsingPolarTransform2022a}\relax }}{62}{figure.caption.181}%
\contentsline {figure}{\numberline {4.17}{\ignorespaces Hystersis-thresholded segmentation output. For each polar prediction, the component that contains the origin of the transform gets a weight of 2 assigned, while all other components get a weight of 1. This left-most image is the result of summing the predictions of 3 polar transformations of the original image (one for each connected component), converted to cartesian coordinates. Note how the thresholding removes the false positive object on the left of the image while keeping the true positive objects intact. \blx@tocontentsinit {4}\cite {bencevicUsingPolarTransform2022a}\relax }}{63}{figure.caption.182}%
\contentsline {figure}{\numberline {4.18}{\ignorespaces Random examples of predictions. Columns from left to right show: the input image, the initial prediction from the non-polar network, the final fused polar prediction, and the ground truth segmentation label. \blx@tocontentsinit {4}\cite {bencevicUsingPolarTransform2022a}\relax }}{65}{figure.caption.186}%
\contentsline {figure}{\numberline {4.19}{\ignorespaces A box plot of the per-scan Dice coefficients of our experiments. \textit {Non-polar} are the results of the U-Net trained using cartesian images. \textit {Polar + GT centers} are the results of the U-Net trained on polar images, using ground-truth connected component centers during inference. \textit {Polar + NP centers} are the results when running inference on the polar model using center points obtained from the non-polar model predictions. \blx@tocontentsinit {4}\cite {bencevicUsingPolarTransform2022a}\relax }}{66}{figure.caption.187}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces A visual summary of our approach. (1) An image is uniformly downsampled from its original resolution. (2) A rough segmentation is predicted by a neural network, and the bounding box of each connected component is calculated. (3) The bounding boxes are scaled to the original image space and crops of the input image are taken in the original resolution and scaled to a common input size. (4) Each crop is segmented separately by a second neural network specifically trained on cropped images. These crops are fused to form a final segmentation in the original high resolution. \blx@tocontentsinit {5}\cite {bencevicSegmentthenSegmentContextPreservingCropBased2023a}\relax }}{69}{figure.caption.194}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces The relationship between input dimensions and the mean Dice Score Coefficient (DSC) and recall for different datasets. The points are measured values from our experiments.\relax }}{77}{figure.caption.212}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Violin plots of Dice Score Coefficients of our approach compared to the baseline models at various input dimensions. The dashed lines represent quartiles of the distributions.\relax }}{78}{figure.caption.213}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Example outputs from the models for the cells dataset at various input sizes. \blx@tocontentsinit {5}\cite {bencevicSegmentthenSegmentContextPreservingCropBased2023a}\relax }}{79}{figure.caption.215}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Example outputs from the models for the polyp dataset at various input sizes. \blx@tocontentsinit {5}\cite {bencevicSegmentthenSegmentContextPreservingCropBased2023a}\relax }}{80}{figure.caption.216}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Example outputs from the models for the aorta dataset at various input sizes. \blx@tocontentsinit {5}\cite {bencevicSegmentthenSegmentContextPreservingCropBased2023a}\relax }}{80}{figure.caption.217}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Mean per-input inference time across different input sizes for the U-Net-based models. \blx@tocontentsinit {5}\cite {bencevicSegmentthenSegmentContextPreservingCropBased2023a}\relax }}{81}{figure.caption.220}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces An illustration of the end-to-end model-driven cropping architecture, comprising two interconnected modules: a coarse and a fine segmentation module, linked via a cropping layer. Both modules are designed to handle images of small, fixed input sizes. The first module processes a downscaled version of the input image. Its segmentation output determines the region of interest in the high-resolution image, which, along with the cropped initial segmentation, is fed into the fine segmentation module. Before being fine-tuned together, both networks are pre-trained as standard segmentation networks.\relax }}{83}{figure.caption.224}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Randomly chosen examples of out-of-sample segmentation results. The columns, from left to right, show the input image with the ground truth segmentation mask, the final output segmentation mask of our approach, the rough segmentation of our approach, and the output of a baseline U-Net model. The images are zoomed in on the lesion region.\relax }}{86}{figure.caption.227}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces A box plot of Jaccard indices above or equal to 0.65 (above) and the number of Jaccard indices below 0.65 (below). The first two columns show in-sample results, while the last two columns show out-of-sample results for models trained on DermIS and tested on DermQuest and vice-versa.\relax }}{88}{figure.caption.229}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces An example of the original image label and our relabeled image. The EAT label is shown in red, while the pericardium label is shown in white. \blx@tocontentsinit {6}\cite {bencevicEpicardialAdiposeTissue2021}\relax }}{93}{figure.caption.237}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces A sample of the inputs to the neural network from a single patient, sorted by slice depth. The first channel (CT adipose tissue) is shown in full white, while the second channel (the slice depth) is shown from black (highest z-axis) to green (lowest z-axis). \blx@tocontentsinit {6}\cite {bencevicEpicardialAdiposeTissue2021}\relax }}{93}{figure.caption.239}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Different random augmentation examples of the same input image. \blx@tocontentsinit {6}\cite {bencevicEpicardialAdiposeTissue2021}\relax }}{94}{figure.caption.242}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Examples of EAT predictions compared to the ground truth images. \blx@tocontentsinit {6}\cite {bencevicEpicardialAdiposeTissue2021}\relax }}{95}{figure.caption.247}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces The Bland-Altman analysis of the number of pixels predicted as EAT on each slice of the test dataset for each fold. The dashed lines indicate a 95\% confidence interval. \blx@tocontentsinit {6}\cite {bencevicEpicardialAdiposeTissue2021}\relax }}{96}{figure.caption.248}%
