%----------------------------------------------------------------------------------------
%  ABSTRACT PAGE
%----------------------------------------------------------------------------------------
\chapter{Abstract}

Medical image segmentation plays a pivotal role in various medical applications from surgical planning to diagnosis and research. Typically, medical image segmentation heavily relies on neural networks to achieve accurate segmentation across different imaging modalities such as CT, MRI, microscopy, dermatoscopy, and others. However, the effectiveness of neural networks is significantly influenced by the availability and quality of training data, which is often challenging to acquire due to the high time and financial cost of image acquisition, invasiveness of some imaging procedures, large file sizes, and regulatory challenges. The labor-intensive and expert-driven nature of annotating medical images for segmentation further compounds these challenges, making the assembly of large, high-quality datasets for training medical image segmentation models particularly difficult.

Statistical learning theory principles indicate that more complex medical image segmentation tasks require neural networks with a greater number of parameters for effective segmentation. This requirement for more parameters, in turn, demands larger sample sizes to avoid overfitting. In medical imaging, however, there is a lack of datasets with large sample sizes. Therefore, there is a large need for data-efficient segmentation methods that can deliver reliable results with limited training samples. 

In this thesis, we present an overview of medical image segmentation methods and existing strategies for improving their data efficiency. In addition, we propose various new methods that simplify the segmentation task, allowing convolutional neural networks to perform accurate segmentation with fewer parameters and, by extension, smaller sample sizes. Our methods center on transforming the segmentation boundary into a representation that can modeled with fewer parameters. We do so by leveraging domain knowledge and traditional image-processing techniques to identify beneficial image transformations. The parameters of the image transformations are dynamically selected for each image using neural networks, breaking down the segmentation task into two more manageable stages: an initial rough localization of the target object followed by the segmentation of a simplified representation of the image.

In addressing the challenge of data efficiency in medical image segmentation, this thesis introduces the following original scientific contributions:

\begin{enumerate}
	\item \textbf{A new biomedical image segmentation method based on polar transform preprocessing with a learned center point.} We introduce a novel preprocessing technique for medical images, particularly those with elliptically shaped objects. By applying a polar transform to the image, we simplify circular decision boundaries into linear ones, making segmentation more straightforward. A key contribution is developing a neural network to identify the optimal origin for the polar transformation, enhancing segmentation performance and allowing the use of less complex networks.
	\item \textbf{An improved method of reducing the input image size in neural networks using salient image crops.} Building on the insights of model-driven polar transformations, we propose a model-driven cropping technique to minimize neural network input sizes without sacrificing fine details. By localizing the target object in a downsampled image and extracting identified regions of interest from a high-resolution image, we maintain precise segmentation with smaller network input sizes. Since model complexity increases with image size, this reduction in input sizes improves data efficiency.
	\item \textbf{A new neural network architecture for high-resolution image segmentation that combines object detection in low-resolution images and segmentation in high-resolution images.} We extend our preprocessing methods to create an end-to-end trainable network that combines low-resolution object localization with high-resolution image segmentation. We allow network convergence in two key ways. First, we use a shared architecture between the localization and segmentation subnetworks which allows transfer learning. Secondly, we ensure gradient flow throughout the network by passing information from one subnetwork to the other. We show that training this end-to-end network increases the robustness of both the localization and segmentation stages.
	\item \textbf{A new method of embedding depth information in two-dimensional convolutional neural network input data.} Acknowledging the data efficiency limitations of 3D networks for volumetric data like CT scans, we develop a method to embed depth information into 2D slices by adding a normalized z-coordinate channel to each slice. We show that this allows effective segmentation of CT images with slice-based 2D networks.
\end{enumerate}

The effectiveness of these methods is validated across various medical imaging modalities, including CT scans, microscopy, dermatoscopy, and colonoscopy, showing not only enhanced data efficiency but also achieving state-of-the-art segmentation results in certain tasks. The techniques introduced are versatile, suitable for a broad spectrum of medical imaging fields, and can serve as general preprocessing steps for any convolutional neural network-based segmentation architecture.

The research presented in this thesis has resulted in the publication of four research papers in scientific journals (all as first author) and four papers presented at international scientific conferences (of which three as first author). 